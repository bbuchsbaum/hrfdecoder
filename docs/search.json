[{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":null,"dir":"","previous_headings":"","what":"AR Prewhitening Implementation Summary","title":"AR Prewhitening Implementation Summary","text":"Date: 2025-11-09 Status: âœ… Implementation Complete","code":""},{"path":[]},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_1-package-dependencies","dir":"","previous_headings":"Changes Made","what":"1. Package Dependencies","title":"AR Prewhitening Implementation Summary","text":"Files Modified: - DESCRIPTION - Added fmriAR (>= 0.3.0) Imports Remotes - NAMESPACE - Added imports fmriAR::fit_noise fmriAR::whiten_apply","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_2-core-fitting-function","dir":"","previous_headings":"Changes Made","what":"2. Core Fitting Function","title":"AR Prewhitening Implementation Summary","text":"File: R/fit.R New Parameters: - ar_order - AR order prewhitening (default: NULL AR). Set 1 AR(1), 2 AR(2), \"auto\" automatic BIC-based selection - ar_method - AR estimation method: \"ar\" (Yule-Walker) \"arma\" (Hannan-Rissanen). Default: \"ar\" - ar_pooling - Spatial pooling: \"global\" (one AR model) \"run\" (per-run AR). Default: \"run\" Implementation Details: - AR prewhitening inserted baseline residualization (line 52) standardization (line 81) - AR plan estimated residuals using fmriAR::fit_noise() - Whitening applied via fmriAR::whiten_apply() - AR plan stored fit$preproc$ar_plan test application - AR settings stored fit$settings (ar_order, ar_method, ar_pooling) Code Flow:","code":"1. Baseline residualization  â†’ Remove nuisance signals 2. AR prewhitening (NEW)     â†’ Remove temporal autocorrelation 3. Standardization           â†’ Z-score normalization 4. Decoder preparation       â†’ Build priors, Laplacian 5. ALS solver                â†’ Fit W, P, HRF"},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_3-prediction-function","dir":"","previous_headings":"Changes Made","what":"3. Prediction Function","title":"AR Prewhitening Implementation Summary","text":"File: R/predict.R Changes: - AR prewhitening applied test data standardization (lines 24-33) - Uses stored fit$preproc$ar_plan training - New helper function .get_run_ids_from_test_data() (lines 107-122) extracts run IDs test data : 1. ev_model_test provided 2. Training run_ids test length matches 3. Default single run otherwise Preprocessing Order Predict:","code":"1. AR prewhitening  â†’ Apply stored ar_plan from training 2. Standardization  â†’ Apply stored center/scale from training 3. Prediction       â†’ Compute soft labels"},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_4-rmvpa-integration","dir":"","previous_headings":"Changes Made","what":"4. rMVPA Integration","title":"AR Prewhitening Implementation Summary","text":"File: R/hrfdecoder_model.R Changes: - Added AR parameters hrfdecoder_model() signature (lines 16-18) - Default: ar_order = 1 (enables AR(1) default rMVPA) - AR parameters passed fit_hrfdecoder() train_model.hrfdecoder_model() (lines 75-77) Usage:","code":"spec <- hrfdecoder_model(   dataset = dset,   design = mvdes,   ar_order = 1,        # NEW   ar_method = \"ar\",    # NEW   ar_pooling = \"run\",  # NEW   lambda_W = 10,   ... )"},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_5-comprehensive-tests","dir":"","previous_headings":"Changes Made","what":"5. Comprehensive Tests","title":"AR Prewhitening Implementation Summary","text":"File: tests/testthat/test-ar-prewhitening.R Test Coverage: 1. âœ… AR(1) prewhitening reduces autocorrelation 2. âœ… Run-specific AR parameters estimated applied 3. âœ… AR parameters applied consistently test data 4. âœ… Backward compatibility: ar_order=NULL reproduces old behavior 5. âœ… Auto AR order selection works 6. âœ… rMVPA integration: AR parameters passed model spec","code":""},{"path":[]},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"standalone-usage","dir":"","previous_headings":"Usage Examples","what":"Standalone Usage","title":"AR Prewhitening Implementation Summary","text":"","code":"library(hrfdecoder) library(fmridesign) library(fmrihrf)  # Create event model ev_model <- event_model(   onsets ~ hrf(condition, basis = \"spmg3\"),   data = events_df,   block = ~ run,   sampling_frame = sampling_frame(blocklens = c(200, 200), TR = 2) )  # Create baseline model base_model <- baseline_model(   ~ poly(run, degree = 3) + motion,   data = nuisance_df,   block = ~ run,   sampling_frame = sampling_frame(blocklens = c(200, 200), TR = 2) )  # Fit decoder WITH AR(1) prewhitening fit_ar <- fit_hrfdecoder(   Y = roi_data,              # (400 TRs Ã— 500 voxels)   ev_model = ev_model,   base_model = base_model,   ar_order = 1,              # Enable AR(1) prewhitening   ar_method = \"ar\",          # Yule-Walker estimation   ar_pooling = \"run\",        # Run-specific AR parameters   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5,   max_iter = 20,   verbose = 1 )  # Fit decoder WITHOUT AR (backward compatible) fit_no_ar <- fit_hrfdecoder(   Y = roi_data,   ev_model = ev_model,   base_model = base_model,   ar_order = NULL,           # Disable AR prewhitening   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5,   max_iter = 20,   verbose = 1 )  # Predict on test data (AR automatically applied if used in training) preds_tr <- predict_hrfdecoder(fit_ar, Y_test, mode = \"tr\") preds_trial <- predict_hrfdecoder(fit_ar, Y_test,                                   ev_model_test = ev_model_test,                                   mode = \"trial\")  # Inspect AR parameters fit_ar$preproc$ar_plan        # fmriAR plan object fit_ar$settings$ar_order       # AR order used (1) fit_ar$settings$ar_pooling     # Pooling method (\"run\")"},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"rmvpa-usage","dir":"","previous_headings":"Usage Examples","what":"rMVPA Usage","title":"AR Prewhitening Implementation Summary","text":"","code":"library(rMVPA) library(hrfdecoder)  # Create design mvdes <- continuous_mvpa_design(   event_model = ev_model,   block_var = run_ids,   design_df_events = trials_df )  # Specify model WITH AR prewhitening spec <- hrfdecoder_model(   dataset = as_mvpa_dataset(fmri_dataset),   design = mvdes,   basis = fmrihrf::spmg3(),   ar_order = 1,              # AR(1) prewhitening   ar_method = \"ar\",          # Yule-Walker   ar_pooling = \"run\",        # Run-specific   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5 )  # Run searchlight with AR prewhitening results <- run_searchlight(spec, radius = 8, method = \"randomized\", niter = 4)  # Specify model WITHOUT AR spec_no_ar <- hrfdecoder_model(   dataset = as_mvpa_dataset(fmri_dataset),   design = mvdes,   ar_order = NULL,           # Disable AR   lambda_W = 10,   ... )"},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"auto-ar-order-selection","dir":"","previous_headings":"Usage Examples","what":"Auto AR Order Selection","title":"AR Prewhitening Implementation Summary","text":"","code":"# Let fmriAR automatically select AR order via BIC fit_auto <- fit_hrfdecoder(   Y = roi_data,   ev_model = ev_model,   ar_order = \"auto\",         # Automatic order selection   ar_pooling = \"global\",     # Global AR for speed   lambda_W = 10,   max_iter = 20 )  # Check selected order fit_auto$preproc$ar_plan$order[\"p\"]  # Selected AR order (e.g., 2)"},{"path":[]},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_1-preprocessing-order","dir":"","previous_headings":"Key Design Decisions","what":"1. Preprocessing Order","title":"AR Prewhitening Implementation Summary","text":"Chosen: Baseline â†’ AR â†’ Standardization Rationale: - Baseline removal first eliminates structured nuisance AR estimation - AR operates residuals (nuisance removal) - Standardization applied whitened residuals consistent scaling","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_2-ar-parameter-storage","dir":"","previous_headings":"Key Design Decisions","what":"2. AR Parameter Storage","title":"AR Prewhitening Implementation Summary","text":"Location: fit$preproc$ar_plan Rationale: - Consistent existing preprocessing storage (center, scale) - Complete fmriAR plan object enables identical test application - information loss (full AR state preserved)","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_3-run-specific-vs-global-ar","dir":"","previous_headings":"Key Design Decisions","what":"3. Run-Specific vs.Â Global AR","title":"AR Prewhitening Implementation Summary","text":"Default: ar_pooling = \"run\" Rationale: - fMRI noise often varies across runs (scanner drift, subject state) - Run-specific AR respects heterogeneity - Laplacian already blocks smoothing across runs (consistent design)","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_4-rmvpa-default","dir":"","previous_headings":"Key Design Decisions","what":"4. rMVPA Default","title":"AR Prewhitening Implementation Summary","text":"Default: ar_order = 1 hrfdecoder_model() Rationale: - fMRI data exhibits AR(1) autocorrelation - Enables prewhitening default rMVPA users - Can disabled ar_order = NULL desired","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"id_5-backward-compatibility","dir":"","previous_headings":"Key Design Decisions","what":"5. Backward Compatibility","title":"AR Prewhitening Implementation Summary","text":"Mechanism: ar_order = NULL (default standalone fit_hrfdecoder()) Guarantees: - Existing code without AR parameters runs unchanged - ar_order = NULL ar_order = 0 produces identical results pre-AR code - breaking changes API","code":""},{"path":[]},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"computational-cost","dir":"","previous_headings":"Performance Characteristics","what":"Computational Cost","title":"AR Prewhitening Implementation Summary","text":"AR Estimation: - Algorithm: Yule-Walker (default) Hannan-Rissanen - Complexity: O(pÂ² T V) p AR order, T TRs, V voxels - Typical: ~0.1-0.5 seconds 200 TRs Ã— 1000 voxels Whitening: - Algorithm: Recursive filtering via C++/RcppArmadillo - Complexity: O(p T V) - Typical: ~0.05-0.2 seconds data - Parallelization: OpenMP across voxels (enabled fmriAR) Overhead: - Searchlight impact: +5-10% total runtime - Benefit: efficient decoder (better GLS weights)","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"memory-usage","dir":"","previous_headings":"Performance Characteristics","what":"Memory Usage","title":"AR Prewhitening Implementation Summary","text":"AR Plan Storage: - Global pooling: O(p) parameters - Run pooling: O(R Ã— p) R = number runs - Typical footprint: <1 MB","code":""},{"path":[]},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"unit-tests","dir":"","previous_headings":"Testing & Validation","what":"Unit Tests","title":"AR Prewhitening Implementation Summary","text":"tests tests/testthat/test-ar-prewhitening.R: âœ… Decorrelation verification âœ… Multi-run handling âœ… Train/test consistency âœ… Backward compatibility âœ… Auto order selection âœ… rMVPA integration","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"recommended-validation","dir":"","previous_headings":"Testing & Validation","what":"Recommended Validation","title":"AR Prewhitening Implementation Summary","text":"Check residual whiteness: Compare /without AR: Verify CV safety:","code":"# After fitting with AR fit <- fit_hrfdecoder(..., ar_order = 1, ...) # Manually compute residuals and check ACF fit_ar <- fit_hrfdecoder(..., ar_order = 1, ...) fit_no_ar <- fit_hrfdecoder(..., ar_order = NULL, ...) # Compare decoder accuracy on held-out data # In rMVPA: AR estimated from training fold only # Each fold uses its own AR parameters"},{"path":[]},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"potential-additions","dir":"","previous_headings":"Future Enhancements","what":"Potential Additions","title":"AR Prewhitening Implementation Summary","text":"ARMA Models: Parcel-Based Pooling: AR Diagnostics: Prewhitened Prior:","code":"fit <- fit_hrfdecoder(..., ar_order = 1, ar_method = \"arma\", q = 1, ...) fit <- fit_hrfdecoder(..., ar_pooling = \"parcel\", parcels = parcel_labels, ...) fit$diagnostics$ar_acf      # Post-whitening ACF fit$diagnostics$ar_bic      # BIC per AR order # Currently: only Y is whitened # Future: whiten DBbeta prior as well"},{"path":[]},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"updated-files","dir":"","previous_headings":"Documentation","what":"Updated Files","title":"AR Prewhitening Implementation Summary","text":"R/fit.R - Roxygen docs AR parameters (lines 16-22) R/hrfdecoder_model.R - Roxygen docs rMVPA AR params (lines 2-4)","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"additional-documentation-needed","dir":"","previous_headings":"Documentation","what":"Additional Documentation Needed","title":"AR Prewhitening Implementation Summary","text":"use AR prewhitening Impact decoder performance Choosing AR order Computational cost Add AR prewhitening feature list Example /without AR","code":""},{"path":[]},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"related-documentation","dir":"","previous_headings":"References","what":"Related Documentation","title":"AR Prewhitening Implementation Summary","text":"Integration Plan: AR_PREWHITENING_INTEGRATION_PLAN.md fmriAR Package: /Users/bbuchsbaum/code/fmriAR Design Notes: notes/hrf_weakly_supervised_decoder.md Section 4","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"key-papers","dir":"","previous_headings":"References","what":"Key Papers","title":"AR Prewhitening Implementation Summary","text":"Worsley et al.Â (2002). â€œgeneral statistical analysis fMRI data.â€ NeuroImage. Purdon & Weisskoff (1998). â€œEffect temporal autocorrelation due physiological noise stimulus paradigm voxel-level false-positive rates fMRI.â€ Human Brain Mapping.","code":""},{"path":"/AR_IMPLEMENTATION_SUMMARY.html","id":"summary-checklist","dir":"","previous_headings":"","what":"Summary Checklist","title":"AR Prewhitening Implementation Summary","text":"âœ… fmriAR dependency added DESCRIPTION NAMESPACE âœ… AR prewhitening integrated fit_hrfdecoder() preprocessing pipeline âœ… AR plan stored applied test data predict_hrfdecoder() âœ… rMVPA integration: AR parameters passed model spec âœ… Comprehensive unit tests created âœ… Roxygen documentation updated âœ… Backward compatibility maintained (ar_order = NULL) âœ… Implementation follows integration plan Status: Ready testing deployment! ğŸ‰","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":null,"dir":"","previous_headings":"","what":"AR Prewhitening Integration Plan for hrfdecoder","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Date: 2025-11-09 Authors: Analysis 3 specialized sub-agents (fmriAR, rMVPA, hrfdecoder) Status: Design Complete, Ready Implementation","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"executive-summary","dir":"","previous_headings":"","what":"Executive Summary","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"document provides comprehensive plan integrating AR(1)/ARMA prewhitening hrfdecoder package using fmriAR, full consideration rMVPA plugin architecture. integration addresses autocorrelated noise fMRI data maintaining clean separation concerns across preprocessing pipeline.","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"key-findings","dir":"","previous_headings":"Executive Summary","what":"Key Findings","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Current State: hrfdecoder handles nuisance signals via pre-projection address autocorrelated noise Optimal Solution: Use fmriAR run-specific AR estimation whitening Integration Point: Insert baseline residualization standardization rMVPA Impact: Minimal changes required; AR parameters passed model spec","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"table-of-contents","dir":"","previous_headings":"","what":"Table of Contents","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Current Noise Handling Analysis fmriAR Capabilities Overview rMVPA Architecture & Constraints Integration Architecture Implementation Plan Testing Strategy Performance Considerations","code":""},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_11-autocorrelated-noise-not-handled","dir":"","previous_headings":"1. Current Noise Handling Analysis","what":"1.1 Autocorrelated Noise: NOT Handled","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Evidence hrfdecoder: File: src/softlabels_als.cpp:39 Objective function: matters: fMRI noise exhibits temporal autocorrelation (typically AR(1) Ï â‰ˆ 0.3-0.5): Inflated degrees freedom â†’ Liberal statistical tests Suboptimal decoder weights â†’ using GLS-optimal weights Biased variance estimates â†’ Confidence intervals narrow Reduced efficiency â†’ data needed power Mentioned design notes (Section 4): > â€œAR(1) prewhitening per run prior fittingâ€ listed optional enhancement (implemented)","code":"recon = arma::accu(arma::square(S - P));  // Assumes i.i.d. noise L = ||XW - P||Â²_F              (reconstruction - assumes white noise)   + Î»_W ||W||Â²_F                (weight regularization)   + Î»_HRF ||P - DBÎ²||Â²_F        (prior adherence)   + Î»_smooth P^T L P            (temporal smoothness)"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_12-nuisance-signals-handled-via-pre-projection-","dir":"","previous_headings":"1. Current Noise Handling Analysis","what":"1.2 Nuisance Signals: Handled via Pre-Projection âœ“","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Evidence hrfdecoder: File: R/fit.R:40 Implementation: R/interop_fmri.R:44-46 gets removed: Low-frequency drift (polynomial trends, high-pass filtering) Motion artifacts (6 24 motion regressors) Physiological noise (cardiac, respiratory modeled) nuisance regressors specified baseline_model Approach: Orthogonal Projection Conclusion: âœ“ Nuisance removal properly implemented via pre-projection. changes needed.","code":"Y <- residualize_baseline(Y, base_model)  # BEFORE any fitting residualize_baseline <- function(Y, base_model = NULL) {   if (is.null(base_model)) return(Y)   fmridesign::residualize(base_model, Y)  # Delegate to fmridesign } Y_clean = Y - X_baseline (X_baseline^T X_baseline)^(-1) X_baseline^T Y"},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_21-core-functionality","dir":"","previous_headings":"2. fmriAR Capabilities Overview","what":"2.1 Core Functionality","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Package: fmriAR v0.3.0 Location: /Users/bbuchsbaum/code/fmriAR Provides: AR/ARMA parameter estimation residuals (Yule-Walker, Hannan-Rissanen) Run-aware whitening automatic boundary resets Censor-aware whitening excluding bad TRs Multi-scale spatial pooling across parcel hierarchies AFNI-compatible restricted AR models Fast C++ implementation OpenMP parallelization","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_22-api-design","dir":"","previous_headings":"2. fmriAR Capabilities Overview","what":"2.2 API Design","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Primary workflow: Returned objects: phi: AR coefficients (per-run pooled) theta: MA coefficients order: (p, q) runs: Run labels method: â€œarâ€, â€œarmaâ€, â€œafniâ€ X: Whitened design matrix Y: Whitened data matrix","code":"# 1. Estimate AR/ARMA noise model from residuals plan <- fit_noise(   resid,                  # (T x V) residual matrix   runs = run_ids,         # Run identifiers (respects boundaries)   method = \"ar\",          # or \"arma\"   p = \"auto\",             # Auto-select order via BIC (or specify integer)   q = 0,                  # MA order (0 for pure AR)   exact_first = \"ar1\"     # Exact AR(1) scaling at segment starts )  # 2. Apply whitening to design and data matrices whitened <- whiten_apply(   plan,   X, Y,   runs = run_ids,         # Same run structure as estimation   censor = NULL           # Optional: indices of bad TRs to skip )  # 3. Use whitened data X_white <- whitened$X Y_white <- whitened$Y"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_23-run-specific-ar-handling","dir":"","previous_headings":"2. fmriAR Capabilities Overview","what":"2.3 Run-Specific AR Handling","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Key feature: Estimates separate AR parameters per run resets whitening buffers run boundaries. C++ implementation: arma_whiten_inplace() fmriAR/src/fmriAR_whiten.cpp","code":"runs <- rep(1:3, each=100)  # 3 runs of 100 TRs each  plan <- fit_noise(resid, runs = runs, pooling = \"run\", p = 1) # plan$phi = list(Ï†â‚, Ï†â‚‚, Ï†â‚ƒ)  # One per run  whitened <- whiten_apply(plan, X, Y, runs = runs) # Applies run-specific Ï†áµ£ and resets AR state at run boundaries // Pseudocode: For each time t and voxel j: y_t[j] = y_t[j] - sum_{k=1}^p Ï†_k * y_{t-k}[j]  // AR filter  // At run boundaries: // - Reset buffers (no cross-run AR dependency) // - Apply exact_first_ar1 scaling if p=1: y_t[j] *= sqrt(1 - Ï†â‚Â²)"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_24-traintest-workflow","dir":"","previous_headings":"2. fmriAR Capabilities Overview","what":"2.4 Train/Test Workflow","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Critical design pattern cross-validation: Key principle: AR parameters learned training data , frozen test application.","code":"# TRAINING PHASE # Step 1: Fit AR model on training residuals train_resid <- Y_train - X_train %*% solve(X_train, Y_train) plan <- fit_noise(train_resid, runs = train_runs, method = \"ar\", p = 2)  # Step 2: Whiten training data train_white <- whiten_apply(plan, X_train, Y_train, runs = train_runs)  # TESTING PHASE # Step 3: Apply SAME plan to test data (no re-estimation!) test_white <- whiten_apply(plan, X_test, Y_test, runs = test_runs)"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_25-integration-strengths","dir":"","previous_headings":"2. fmriAR Capabilities Overview","what":"2.5 Integration Strengths","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"âœ“ Run-aware: Proper multi-run fMRI handling âœ“ Train/test separation: Plan estimated , applied âœ“ Fast: C++ + OpenMP parallelization âœ“ Flexible pooling: Global, run-specific, parcel-based AR âœ“ Censoring support: Can exclude bad TRs âœ“ Production-ready: Mature package comprehensive tests","code":""},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_31-model-plugin-system","dir":"","previous_headings":"3. rMVPA Architecture & Constraints","what":"3.1 Model Plugin System","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Package: rMVPA Location: /Users/bbuchsbaum/code/rMVPA S3 Method Dispatch Pattern: hrfdecoder integration: File: R/hrfdecoder_model.R Already implements required S3 methods: - train_model.hrfdecoder_model() (lines 48-71) - format_result.hrfdecoder_model() (lines 74-105) - merge_results.hrfdecoder_model() (lines 108-126)","code":"# Core generics for any model plugin: train_model(obj, train_dat, y, sl_info, cv_spec, indices, ...) predict_model(obj, new_data, ...) format_result(obj, result, error_message, context, ...) merge_results(obj, result_set, indices, id, ...) compute_performance(obj, result, ...)"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_32-data-flow-in-cross-validation","dir":"","previous_headings":"3. rMVPA Architecture & Constraints","what":"3.2 Data Flow in Cross-Validation","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"","code":"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 1. ROI/Searchlight Extraction                              â”‚ â”‚    mvpa_iterate() extracts TÃ—V matrix per ROI              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚                  â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 2. Cross-Validation Fold Creation                          â”‚ â”‚    Based on block_var (run IDs) or custom CV scheme        â”‚ â”‚    â†’ train_indices, test_indices                           â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚                  â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 3. PER FOLD:                                                â”‚ â”‚    a. train_model() called with train_dat                  â”‚ â”‚       â†’ Fits decoder on training subset                    â”‚ â”‚    b. format_result() called with test data                â”‚ â”‚       â†’ Predicts on test subset                            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚                  â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 4. merge_results() across folds                            â”‚ â”‚    â†’ Combined classification_result                        â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_33-preprocessing-integration-points","dir":"","previous_headings":"3. rMVPA Architecture & Constraints","what":"3.3 Preprocessing Integration Points","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Two architectural options identified:","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"option-a-per-fold-preprocessing-recommended","dir":"","previous_headings":"3. rMVPA Architecture & Constraints > 3.3 Preprocessing Integration Points","what":"Option A: Per-Fold Preprocessing (RECOMMENDED)","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Pros: - âœ“ CV-safe: AR estimated training fold - âœ“ Minimal rMVPA changes: Contained model-specific methods - âœ“ Flexible: fold can different AR structure Cons: - âœ— Redundant computation: Re-estimates AR fold - âœ— complex: Logic split across train/predict","code":"train_model.hrfdecoder_model <- function(obj, train_dat, ...) {   X_train <- as.matrix(train_dat)    # ===== AR PREWHITENING HAPPENS HERE =====   # 1. Estimate AR from training residuals   # 2. Whiten training data   # 3. Store plan in fit object    fit <- fit_hrfdecoder(     Y = X_train_white,  # Whitened data     ev_model = obj$design$event_model,     ar_plan = plan,     # NEW: Store for test application     ...   )    return(fit) }  format_result.hrfdecoder_model <- function(obj, result, context, ...) {   X_test <- as.matrix(context$test)    # ===== APPLY SAME AR PLAN TO TEST DATA =====   X_test_white <- whiten_apply(result$fit$ar_plan, NULL, X_test)$Y    preds <- predict_hrfdecoder(result$fit, X_test_white, ...)   return(preds) }"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"option-b-dataset-level-preprocessing","dir":"","previous_headings":"3. rMVPA Architecture & Constraints > 3.3 Preprocessing Integration Points","what":"Option B: Dataset-Level Preprocessing","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Prewhiten entire dataset passing rMVPA. RECOMMENDED : - âœ— CV leakage: Test data influences AR estimation - âœ— Less flexible: Canâ€™t adapt AR per fold","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_34-key-constraints-from-rmvpa","dir":"","previous_headings":"3. rMVPA Architecture & Constraints","what":"3.4 Key Constraints from rMVPA","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Blocked CV standard: Uses block_var (run IDs) create folds Result format required: Must return tibble columns class, probs, y_true, test_ind, fit, error, error_message Event model required: trial-level evaluation (default mode) Standardization handling: rMVPA may pre-standardize; hrfdecoder currently calls standardize = FALSE rMVPA context","code":""},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_41-preprocessing-pipeline-order","dir":"","previous_headings":"4. Integration Architecture","what":"4.1 Preprocessing Pipeline Order","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"RECOMMENDED ORDER: Rationale: Baseline FIRST: Removes structured nuisance AR estimation AR SECOND: Estimates autocorrelation residuals (nuisance removal) Standardization THIRD: Z-scores applied whitened residuals ALS LAST: Operates fully preprocessed data","code":"Input Y (T Ã— V raw fMRI data)           â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 1. Baseline Residualization             â”‚  [R/fit.R:40] â”‚    Y â† residualize_baseline(Y, base_model) â”‚    Removes: drift, motion, nuisance    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 2. AR Prewhitening (NEW)                â”‚  [INSERTION POINT] â”‚    plan â† fit_noise(Y, runs, p)        â”‚ â”‚    Y â† whiten_apply(plan, NULL, Y)$Y   â”‚ â”‚    Removes: temporal autocorrelation   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 3. Standardization                      â”‚  [R/fit.R:43-50] â”‚    Y â† scale(Y)                        â”‚ â”‚    Z-scores: (Y - mean) / sd           â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 4. Decoder Input Preparation            â”‚  [R/prep.R:40-58] â”‚    X_list, DBbeta, P0 â† prepare_decoder_inputs() â”‚    L â† build_laplacian_from_runs()    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 5. ALS Solver                           â”‚  [src/softlabels_als.cpp:68] â”‚    W, P, b â† fit_softlabels_als()      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_42-exact-code-insertion-point","dir":"","previous_headings":"4. Integration Architecture","what":"4.2 Exact Code Insertion Point","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"File: /Users/bbuchsbaum/code/hrfdecoder/R/fit.R Location: lines 40 42","code":"fit_hrfdecoder <- function(   Y,   ev_model,   base_model = NULL,   hrf = NULL,   ar_order = 1,              # NEW PARAMETER   ar_method = \"ar\",          # NEW: \"ar\" or \"arma\"   ar_pooling = \"run\",        # NEW: \"global\" or \"run\"   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5,   theta_penalty = 0.01,   max_iter = 20,   tol = 1e-4,   nonneg = TRUE,   background = TRUE,   standardize = TRUE,   verbose = 1 ) {   stopifnot(is.matrix(Y))   stopifnot(inherits(ev_model, \"event_model\"))   Tn <- nrow(Y)    # ===== STEP 1: Baseline residualization =====   Y <- residualize_baseline(Y, base_model)    # ===== STEP 2: AR PREWHITENING (NEW) =====   ar_plan <- NULL   if (!is.null(ar_order) && ar_order > 0) {     # Need run_ids BEFORE prep     prep_temp <- prepare_decoder_inputs(ev_model, hrf = hrf, background = background)     run_ids <- prep_temp$run_ids      # Estimate AR from current Y (post-baseline, pre-standardization)     ar_plan <- fmriAR::fit_noise(       Y,       runs = run_ids,       method = ar_method,       p = ar_order,       pooling = ar_pooling,       exact_first = \"ar1\"     )      # Apply whitening     Y_white <- fmriAR::whiten_apply(ar_plan, X = NULL, Y = Y, runs = run_ids)     Y <- Y_white$Y      if (verbose) {       message(\"Applied AR(\", ar_order, \") prewhitening (pooling=\", ar_pooling, \")\")     }   }    # ===== STEP 3: Standardization =====   preproc <- list(     standardize = isTRUE(standardize),     center = NULL,     scale = NULL,     ar_plan = ar_plan  # NEW: Store for predict   )   if (isTRUE(standardize)) {     Ys <- scale(Y)     preproc$center <- attr(Ys, \"scaled:center\")     s <- attr(Ys, \"scaled:scale\")     s[is.na(s) | s == 0] <- 1     preproc$scale <- s     attr(Ys, \"scaled:center\") <- NULL     attr(Ys, \"scaled:scale\") <- NULL     Y <- Ys   }    # ===== STEP 4: Decoder preparation & fitting =====   prep <- prepare_decoder_inputs(ev_model, hrf = hrf, background = background)   # ... (rest unchanged)    fit <- list(     W = fit_cpp$W,     P = Z,     b = as.numeric(fit_cpp$b),     theta = theta,     hrf = hrf_est,     conditions = prep$conditions,     background = background,     converged = isTRUE(fit_cpp$converged),     iterations = fit_cpp$iterations,     settings = list(       lambda_W = lambda_W,       lambda_HRF = lambda_HRF,       lambda_smooth = lambda_smooth,       theta_penalty = theta_penalty,       max_iter = max_iter,       tol = tol,       nonneg = nonneg,       background = background,       TR = TR,       run_ids = prep$run_ids,       ar_order = ar_order,         # NEW       ar_method = ar_method,        # NEW       ar_pooling = ar_pooling       # NEW     ),     train = list(       P0 = prep$P0,       prior = prep$DBbeta,       events = events_tbl     ),     preproc = preproc,              # Contains ar_plan     diagnostics = list(       obj_trace = fit_cpp$obj_trace     )   )   class(fit) <- \"hrfdecoder_fit\"   fit }"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_43-prediction-update","dir":"","previous_headings":"4. Integration Architecture","what":"4.3 Prediction Update","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"File: /Users/bbuchsbaum/code/hrfdecoder/R/predict.R Location: Lines 21-27 (preprocessing section)","code":"predict_hrfdecoder <- function(   object,   Y_test,   ev_model_test = NULL,   mode = c(\"tr\", \"trial\"),   window = c(4, 8),   weights = c(\"hrf\", \"flat\") ) {   stopifnot(inherits(object, \"hrfdecoder_fit\"))   mode <- match.arg(mode)   weights <- match.arg(weights)    # ===== APPLY PREPROCESSING IN SAME ORDER AS TRAINING =====    # STEP 1: AR prewhitening (if was applied during training)   if (!is.null(object$preproc$ar_plan)) {     run_ids_test <- get_run_ids_from_test_data(object, Y_test, ev_model_test)     Y_white <- fmriAR::whiten_apply(       object$preproc$ar_plan,       X = NULL,       Y = Y_test,       runs = run_ids_test     )     Y_test <- Y_white$Y   }    # STEP 2: Standardization (if was applied during training)   if (!is.null(object$preproc) && isTRUE(object$preproc$standardize)) {     ctr <- object$preproc$center %||% rep(0, ncol(Y_test))     scl <- object$preproc$scale %||% rep(1, ncol(Y_test))     scl[is.na(scl) | scl == 0] <- 1     Y_test <- sweep(Y_test, 2, ctr, FUN = \"-\")     Y_test <- sweep(Y_test, 2, scl, FUN = \"/\")   }    # ===== COMPUTE PREDICTIONS =====   scores <- predict_softlabels(Y_test, object$W, object$b)   probs <- row_softmax(scores)    # ... (rest unchanged) }  # NEW HELPER FUNCTION get_run_ids_from_test_data <- function(fit_obj, Y_test, ev_model_test) {   # Extract run IDs for test data   # Option 1: From ev_model_test if provided   if (!is.null(ev_model_test)) {     prep <- prepare_decoder_inputs(ev_model_test, hrf = fit_obj$hrf,                                    background = fit_obj$background)     return(prep$run_ids)   }    # Option 2: Assume same run structure as training (if single-run test)   if (nrow(Y_test) == length(fit_obj$settings$run_ids)) {     return(fit_obj$settings$run_ids)   }    # Option 3: Default to single run   return(rep(1L, nrow(Y_test))) }"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_44-rmvpa-integration-updates","dir":"","previous_headings":"4. Integration Architecture","what":"4.4 rMVPA Integration Updates","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"File: /Users/bbuchsbaum/code/hrfdecoder/R/hrfdecoder_model.R Updates required:","code":"hrfdecoder_model <- function(   dataset,   design,   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5,   theta_penalty = 0.01,   basis = NULL,   window = c(4, 8),   nonneg = TRUE,   max_iter = 10,   tol = 1e-4,   ar_order = 1,           # NEW   ar_method = \"ar\",       # NEW   ar_pooling = \"run\",     # NEW   performance = NULL,   crossval = NULL,   return_predictions = TRUE,   return_fits = FALSE ) {   # ... auto-detect CV ...    rMVPA::create_model_spec(     \"hrfdecoder_model\",     dataset = dataset,     design = design,     lambda_W = lambda_W,     lambda_HRF = lambda_HRF,     lambda_smooth = lambda_smooth,     theta_penalty = theta_penalty,     basis = basis,     window = window,     nonneg = nonneg,     max_iter = max_iter,     tol = tol,     ar_order = ar_order,        # NEW     ar_method = ar_method,      # NEW     ar_pooling = ar_pooling,    # NEW     crossval = crossval,     performance = performance,     compute_performance = TRUE,     return_predictions = return_predictions,     return_fits = return_fits   ) }  # train_model.hrfdecoder_model UNCHANGED - just passes ar_order to fit_hrfdecoder train_model.hrfdecoder_model <- function(obj, train_dat, y, sl_info, cv_spec, indices, ...) {   X <- as.matrix(train_dat)   ev_model <- obj$design$event_model   base_model <- obj$design$baseline_model %||% NULL    fit <- fit_hrfdecoder(     Y = X,     ev_model = ev_model,     base_model = base_model,     hrf = obj$basis,     lambda_W = obj$lambda_W,     lambda_HRF = obj$lambda_HRF,     lambda_smooth = obj$lambda_smooth,     theta_penalty = obj$theta_penalty,     ar_order = obj$ar_order,       # NEW     ar_method = obj$ar_method,     # NEW     ar_pooling = obj$ar_pooling,   # NEW     max_iter = obj$max_iter,     tol = obj$tol,     nonneg = obj$nonneg,     standardize = FALSE,     verbose = 0   )    structure(     list(fit = fit, sl_info = sl_info, indices = indices),     class = \"hrfdecoder_fit_wrap\"   ) }  # format_result.hrfdecoder_model UNCHANGED - predict_hrfdecoder handles AR internally"},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_51-phase-1-core-ar-integration-week-1","dir":"","previous_headings":"5. Implementation Plan","what":"5.1 Phase 1: Core AR Integration (Week 1)","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Files modify: Add parameters: ar_order, ar_method, ar_pooling Insert AR prewhitening lines 40-42 Store ar_plan fit$preproc Store AR settings fit$settings Apply AR whitening standardization Add helper get_run_ids_from_test_data() Add AR parameters hrfdecoder_model() signature Pass fit_hrfdecoder() train_model() Add fmriAR (>= 0.3.0) Imports Add remote: bbuchsbaum/fmriAR Import: fmriAR::fit_noise, fmriAR::whiten_apply","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_52-phase-2-testing--validation-week-1-2","dir":"","previous_headings":"5. Implementation Plan","what":"5.2 Phase 2: Testing & Validation (Week 1-2)","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"New test file: tests/testthat/test-ar-prewhitening.R Test cases: Decorrelation test: Multi-run test: Train/test consistency: CV safety test: Backward compatibility:","code":"test_that(\"AR(1) prewhitening reduces autocorrelation in residuals\", {   # Generate AR(1) data with known phi   # Fit with ar_order=1   # Check: residual ACF near zero at lag 1 }) test_that(\"Run-specific AR parameters are estimated and applied\", {   # Simulate 3 runs with different AR parameters   # Fit with ar_pooling=\"run\"   # Check: fit$preproc$ar_plan has 3 different phi values }) test_that(\"AR plan from training is applied to test data\", {   # Train on runs 1-2, test on run 3   # Verify: test predictions use stored ar_plan   # Verify: manual application of ar_plan matches predict output }) test_that(\"AR estimation does not leak information across CV folds\", {   # Run blocked CV with ar_order=1   # Verify: each fold estimates AR from training data only   # Verify: test AR params != training AR params (different data) }) test_that(\"ar_order=NULL or 0 reproduces old behavior exactly\", {   # Fit with ar_order=0   # Fit without ar_order (old API)   # Check: W, P, b identical })"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_53-phase-3-documentation--examples-week-2","dir":"","previous_headings":"5. Implementation Plan","what":"5.3 Phase 3: Documentation & Examples (Week 2)","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Update roxygen docs: R/fit.R: Vignette: vignettes/ar-prewhitening.Rmd use AR prewhitening Impact decoder performance Choosing AR order (BIC, AIC, fixed) Computational cost considerations README update: Add AR prewhitening feature list Simple example /without AR","code":"#' @param ar_order AR order for prewhitening (default: 1). #'   Set to 0 or NULL to disable. #' @param ar_method AR estimation method: \"ar\" (Yule-Walker) or #'   \"arma\" (Hannan-Rissanen). Default: \"ar\". #' @param ar_pooling Spatial pooling for AR: \"global\" (one AR for all voxels), #'   \"run\" (separate AR per run). Default: \"run\"."},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_54-phase-4-performance-optimization-week-3","dir":"","previous_headings":"5. Implementation Plan","what":"5.4 Phase 4: Performance Optimization (Week 3)","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Potential optimizations: Parallel AR estimation across voxels: Cache AR plan repeated fits: fitting multiple ROIs run structure, reuse AR plan Auto AR order selection: Benchmark comparison: Compare runtime /without AR Measure impact searchlight analysis","code":"plan <- fmriAR::fit_noise(..., threads = TRUE)  # Enable OpenMP ar_order = \"auto\"  # Use BIC to select p âˆˆ {0, 1, 2, 3}"},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_61-unit-tests","dir":"","previous_headings":"6. Testing Strategy","what":"6.1 Unit Tests","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"File: tests/testthat/test-ar-prewhitening.R Coverage goals: âœ“ AR estimation reduces autocorrelation âœ“ Run-specific AR parameters âœ“ Train/test consistency âœ“ CV fold independence âœ“ Backward compatibility (ar_order=0) âœ“ Error handling (bad inputs)","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_62-integration-tests","dir":"","previous_headings":"6. Testing Strategy","what":"6.2 Integration Tests","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"File: tests/testthat/test-rmvpa-integration.R Coverage goals: âœ“ rMVPA model spec AR parameters âœ“ Searchlight AR prewhitening âœ“ Blocked CV AR âœ“ Performance metrics unchanged AR (improved)","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_63-simulation-studies","dir":"","previous_headings":"6. Testing Strategy","what":"6.3 Simulation Studies","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Validate AR improves decoder performance: Expected: AR prewhitening improves accuracy Expected: AR minimal impact Expected: Run-specific AR outperforms global","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_64-real-data-validation","dir":"","previous_headings":"6. Testing Strategy","what":"6.4 Real Data Validation","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Apply existing fMRI datasets: Check residual whiteness (ACF diagnostics) Compare decoder accuracy /without AR Measure computational overhead","code":""},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_71-computational-cost","dir":"","previous_headings":"7. Performance Considerations","what":"7.1 Computational Cost","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"AR estimation: - Per voxel: O(pÂ² T) Yule-Walker (fast) - Parallelized: OpenMP across voxels - Typical: ~0.1-0.5 seconds 100 TRs Ã— 1000 voxels Whitening: - Per voxel: O(p T) recursive filtering - Parallelized: OpenMP across voxels - Typical: ~0.05-0.2 seconds data Impact searchlight: - Overhead: +5-10% runtime (negligible vs.Â decoder fitting) - Benefit: efficient decoder (better GLS weights)","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_72-memory-usage","dir":"","previous_headings":"7. Performance Considerations","what":"7.2 Memory Usage","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"AR plan storage: - Global pooling: O(p) parameters - Run pooling: O(R Ã— p) R = number runs - Parcel pooling: O(P Ã— p) P = number parcels Typical footprint: <1 MB analyses","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_73-scaling-recommendations","dir":"","previous_headings":"7. Performance Considerations","what":"7.3 Scaling Recommendations","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"large searchlights (V > 10,000): - Use ar_pooling = \"global\" reduce AR estimation cost - Enable OpenMP parallelization fmriAR many runs (R > 10): - Use ar_pooling = \"run\" run-specific noise structure - Consider caching AR plans across ROIs run structure","code":""},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_81-open-questions","dir":"","previous_headings":"8. Open Questions & Future Enhancements","what":"8.1 Open Questions","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Pro: Automatic, data-driven Con: Adds complexity, computational cost Pro: flexible noise modeling Con: Slower estimation, parameters Current: Y whitened Alternative: Whiten Y DBbeta prior Trade-: Computational cost vs.Â theoretical correctness","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_82-future-enhancements","dir":"","previous_headings":"8. Open Questions & Future Enhancements","what":"8.2 Future Enhancements","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Parcel-based AR pooling: Multi-scale AR: AR diagnostics: Prewhitened prior:","code":"ar_pooling = \"parcel\", parcels = parcel_labels  # Spatial pooling of AR ar_pooling = \"multiscale\", parcel_sets = list(coarse, medium, fine) fit$diagnostics$ar_acf  # Post-whitening ACF fit$diagnostics$ar_bic  # BIC per AR order # Whiten DBbeta prior along with Y DBbeta_white <- whiten_apply(plan, NULL, DBbeta)$Y"},{"path":[]},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_92-implementation-checklist","dir":"","previous_headings":"9. Summary & Recommendations","what":"9.2 Implementation Checklist","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Modify R/fit.R (add AR prewhitening) Modify R/predict.R (apply AR test) Modify R/hrfdecoder_model.R (pass AR params) Update DESCRIPTION (add fmriAR dependency) Update NAMESPACE (import fmriAR functions) Unit tests AR estimation Integration tests rMVPA Simulation studies (varying Ï) Real data validation Roxygen docs new parameters Vignette AR prewhitening Update README examples Enable OpenMP parallelization Benchmark performance Profile memory usage","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"id_93-expected-benefits","dir":"","previous_headings":"9. Summary & Recommendations","what":"9.3 Expected Benefits","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"Statistical: - âœ“ Correct effective degrees freedom - âœ“ Valid confidence intervals - âœ“ Optimal GLS decoder weights Practical: - âœ“ Improved decoder efficiency (less data needed) - âœ“ Better generalization test data - âœ“ accurate performance estimates Computational: - âœ“ Minimal overhead (~5-10% runtime) - âœ“ Parallelized AR estimation - âœ“ Efficient recursive whitening","code":""},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"a1-complete-fit_hrfdecoder-with-ar","dir":"","previous_headings":"Appendix A: Code Snippets","what":"A.1 Complete fit_hrfdecoder() with AR","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"See Section 4.2 full implementation.","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"a2-complete-predict_hrfdecoder-with-ar","dir":"","previous_headings":"Appendix A: Code Snippets","what":"A.2 Complete predict_hrfdecoder() with AR","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"See Section 4.3 full implementation.","code":""},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"a3-example-usage","dir":"","previous_headings":"Appendix A: Code Snippets","what":"A.3 Example Usage","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"","code":"library(hrfdecoder) library(fmridesign) library(fmrihrf)  # 1. Create event model ev_model <- event_model(   onsets ~ hrf(condition, basis = \"spmg3\"),   data = events_df,   block = ~ run,   sampling_frame = sampling_frame(blocklens = c(200, 200, 200), TR = 2) )  # 2. Create baseline model (nuisance) base_model <- baseline_model(   ~ poly(run, degree = 3) + motion_1 + motion_2,   data = nuisance_df,   block = ~ run,   sampling_frame = sampling_frame(blocklens = c(200, 200, 200), TR = 2) )  # 3. Fit decoder with AR(1) prewhitening fit <- fit_hrfdecoder(   Y = roi_data,                 # (600 TRs Ã— 500 voxels)   ev_model = ev_model,   base_model = base_model,   hrf = fmrihrf::spmg3(),   ar_order = 1,                 # NEW: Enable AR(1) prewhitening   ar_method = \"ar\",             # Yule-Walker estimation   ar_pooling = \"run\",           # Run-specific AR parameters   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5,   max_iter = 20,   verbose = 1 )  # 4. Predict on test data preds_tr <- predict_hrfdecoder(fit, Y_test, mode = \"tr\") preds_trial <- predict_hrfdecoder(fit, Y_test, ev_model_test = ev_model_test,                                   mode = \"trial\")  # 5. Inspect AR parameters fit$preproc$ar_plan$phi    # List of AR coefficients per run fit$settings$ar_order       # AR order used"},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"a4-rmvpa-usage","dir":"","previous_headings":"Appendix A: Code Snippets","what":"A.4 rMVPA Usage","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"","code":"library(rMVPA) library(hrfdecoder)  # 1. Create design mvdes <- continuous_mvpa_design(   event_model = ev_model,   block_var = run_ids,   design_df_events = trials_df )  # 2. Specify model with AR spec <- hrfdecoder_model(   dataset = as_mvpa_dataset(fmri_dataset),   design = mvdes,   basis = fmrihrf::spmg3(),   ar_order = 1,              # NEW   ar_method = \"ar\",          # NEW   ar_pooling = \"run\",        # NEW   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5 )  # 3. Run searchlight with AR prewhitening results <- run_searchlight(spec, radius = 8, method = \"randomized\", niter = 4)"},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"b1-package-locations","dir":"","previous_headings":"Appendix B: References","what":"B.1 Package Locations","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"hrfdecoder: /Users/bbuchsbaum/code/hrfdecoder fmriAR: /Users/bbuchsbaum/code/fmriAR rMVPA: /Users/bbuchsbaum/code/rMVPA","code":""},{"path":[]},{"path":"/AR_PREWHITENING_INTEGRATION_PLAN.html","id":"b3-related-documentation","dir":"","previous_headings":"Appendix B: References","what":"B.3 Related Documentation","title":"AR Prewhitening Integration Plan for hrfdecoder","text":"fmriAR vignette: fmriAR-introduction.Rmd rMVPA model plugin guide: custom-models.Rmd hrfdecoder design notes: notes/hrf_weakly_supervised_decoder.md END INTEGRATION PLAN","code":""},{"path":"/CHANGELOG.html","id":null,"dir":"","previous_headings":"","what":"Changelog","title":"Changelog","text":"notable changes hrfdecoder documented file.","code":""},{"path":[]},{"path":"/CHANGELOG.html","id":"added","dir":"","previous_headings":"[0.2.1] - 2025-11-09","what":"Added","title":"Changelog","text":"New parameters fit_hrfdecoder(): ar_order, ar_method, ar_pooling Supports AR(p), ARMA(p,q), automatic order selection via BIC Run-specific global AR parameter pooling Seamless test data application (AR plan stored reused) rMVPA integration default ar_order = 1 hrfdecoder_model()","code":""},{"path":"/CHANGELOG.html","id":"changed","dir":"","previous_headings":"[0.2.1] - 2025-11-09","what":"Changed","title":"Changelog","text":"Preprocessing pipeline order now: Baseline â†’ AR Whitening â†’ Standardization â†’ ALS fit$preproc now includes ar_plan field AR enabled fit$settings expanded store ar_order, ar_method, ar_pooling","code":""},{"path":"/CHANGELOG.html","id":"technical-details","dir":"","previous_headings":"[0.2.1] - 2025-11-09","what":"Technical Details","title":"Changelog","text":"AR prewhitening occurs baseline residualization standardization AR parameters estimated residuals using fmriAR::fit_noise() Whitening applied via fmriAR::whiten_apply() run-aware boundary resets Test data automatically whitened using stored AR plan training Helper function .get_run_ids_from_test_data() extracts run structure test application","code":""},{"path":"/CHANGELOG.html","id":"documentation","dir":"","previous_headings":"[0.2.1] - 2025-11-09","what":"Documentation","title":"Changelog","text":"Comprehensive integration plan: AR_PREWHITENING_INTEGRATION_PLAN.md Implementation summary: AR_IMPLEMENTATION_SUMMARY.md Working examples: examples/ar_prewhitening_example.R Full test suite: tests/testthat/test-ar-prewhitening.R","code":""},{"path":"/CHANGELOG.html","id":"backward-compatibility","dir":"","previous_headings":"[0.2.1] - 2025-11-09","what":"Backward Compatibility","title":"Changelog","text":"Fully backward compatible: ar_order = NULL (default) disables AR prewhitening Existing code runs unchanged produces identical results breaking changes API","code":""},{"path":"/CHANGELOG.html","id":"performance","dir":"","previous_headings":"[0.2.1] - 2025-11-09","what":"Performance","title":"Changelog","text":"AR estimation overhead: ~5-10% total runtime Benefits: Correct GLS weights, valid statistical inference, improved generalization Parallelizable via fmriARâ€™s OpenMP support","code":""},{"path":[]},{"path":"/CHANGELOG.html","id":"added-1","dir":"","previous_headings":"[0.2.0] - Previous Release","what":"Added","title":"Changelog","text":"Integration fmridesign fmrihrf event modeling HRF basis HRF estimation basis space penalty matrices Baseline residualization via fmridesign::residualize() rMVPA plugin architecture hrfdecoder_model() Multi-run support block-diagonal Laplacian ALS solver convergence diagnostics","code":""},{"path":"/CHANGELOG.html","id":"features","dir":"","previous_headings":"[0.2.0] - Previous Release","what":"Features","title":"Changelog","text":"Joint estimation soft labels (P), decoder weights (W), HRF Temporal smoothness via second-difference Laplacian HRF-convolved design priors Trial-level TR-level prediction modes Cross-validation compatible","code":""},{"path":"/IMPLEMENTATION_STATUS.html","id":null,"dir":"","previous_headings":"","what":"Implementation Status: AR Prewhitening","title":"Implementation Status: AR Prewhitening","text":"Date: 2025-11-09 Status: âœ… FULLY IMPLEMENTED READY USE","code":""},{"path":"/IMPLEMENTATION_STATUS.html","id":"what-happened","dir":"","previous_headings":"","what":"What Happened","title":"Implementation Status: AR Prewhitening","text":"discussion referenced implementation tests created skip guards AR prewhitening functionality didnâ€™t exist yet. skip guards (skip_ar_prewhitening()) placeholders prevent test failures feature built. just completed full implementation, skip guards now removed tests active.","code":""},{"path":[]},{"path":"/IMPLEMENTATION_STATUS.html","id":"white_check_mark-implemented-features","dir":"","previous_headings":"Current State","what":"âœ… Implemented Features","title":"Implementation Status: AR Prewhitening","text":"Location: R/fit.R lines 48-73 Integrated baseline residualization standardization Uses fmriAR estimation whitening Location: R/predict.R lines 23-33 Stored AR plan automatically applied test data Helper function extracts run IDs test event models Location: R/hrfdecoder_model.R lines 16-18, 75-77 AR parameters passed model spec Default ar_order = 1 rMVPA usage fmriAR (>= 0.3.0) added DESCRIPTION NAMESPACE Functions imported: fit_noise, whiten_apply File: tests/testthat/test-ar-prewhitening.R 6 comprehensive test cases tests now run (skip guards) Roxygen docs updated modified files Integration plan: AR_PREWHITENING_INTEGRATION_PLAN.md Implementation summary: AR_IMPLEMENTATION_SUMMARY.md Working examples: examples/ar_prewhitening_example.R Changelog: CHANGELOG.md","code":""},{"path":"/IMPLEMENTATION_STATUS.html","id":"timeline","dir":"","previous_headings":"","what":"Timeline","title":"Implementation Status: AR Prewhitening","text":"Initial Analysis - 3 sub-agents examined fmriAR, rMVPA, hrfdecoder Integration Plan Created - 610-line comprehensive plan written DESCRIPTION/NAMESPACE updated R/fit.R modified (AR pipeline added) R/predict.R modified (test application) R/hrfdecoder_model.R modified (rMVPA integration) Tests Created - 6 test cases written skip guards Skip Guards Removed - Tests now active (just completed)","code":""},{"path":"/IMPLEMENTATION_STATUS.html","id":"what-changed-since-the-discussion","dir":"","previous_headings":"","what":"What Changed Since the Discussion","title":"Implementation Status: AR Prewhitening","text":"discussion referenced said: â€œRight now decoder doesnâ€™t AR prewhitening logicâ€¦ tests canâ€™t meaningfully runâ€”â€™d fail immediately APIs â€™re meant exercise simply donâ€™t exist.â€ true written, now: âœ… AR prewhitening logic exists fit_hrfdecoder() âœ… fmriAR APIs threaded codebase âœ… AR plan stored fit objects âœ… Prediction applies AR automatically âœ… Tests can run validate implementation","code":""},{"path":[]},{"path":"/IMPLEMENTATION_STATUS.html","id":"id_1-check-the-implementation","dir":"","previous_headings":"How to Verify","what":"1. Check the Implementation","title":"Implementation Status: AR Prewhitening","text":"","code":"# Look at the AR prewhitening code file.show(\"R/fit.R\")  # Lines 48-73 show AR integration  # Look at test data application file.show(\"R/predict.R\")  # Lines 23-33 show AR application  # Check tests are active (no skip guards) file.show(\"tests/testthat/test-ar-prewhitening.R\")  # No skip_ar_prewhitening() calls"},{"path":"/IMPLEMENTATION_STATUS.html","id":"id_2-run-a-quick-test","dir":"","previous_headings":"How to Verify","what":"2. Run a Quick Test","title":"Implementation Status: AR Prewhitening","text":"","code":"library(hrfdecoder) library(fmridesign) library(fmrihrf)  # Create minimal data set.seed(42) Y <- matrix(rnorm(200 * 50), 200, 50)  sf <- fmrihrf::sampling_frame(blocklens = 200, TR = 2) events <- data.frame(   onset = seq(10, 190, by = 20),   condition = rep(c(\"A\", \"B\"), 5),   run = 1 )  ev_model <- fmridesign::event_model(   onsets ~ hrf(condition, basis = \"spmg1\"),   data = events,   sampling_frame = sf,   block = ~ run )  # Fit WITH AR - should work! fit_ar <- fit_hrfdecoder(   Y = Y,   ev_model = ev_model,   ar_order = 1,          # AR enabled   verbose = 1 )  # Check AR plan exists stopifnot(!is.null(fit_ar$preproc$ar_plan)) stopifnot(fit_ar$settings$ar_order == 1)  print(\"âœ… AR prewhitening is WORKING!\")"},{"path":"/IMPLEMENTATION_STATUS.html","id":"id_3-run-the-full-test-suite","dir":"","previous_headings":"How to Verify","what":"3. Run the Full Test Suite","title":"Implementation Status: AR Prewhitening","text":"Expected output: tests pass (assuming fmriAR, fmridesign, fmrihrf installed)","code":"cd /Users/bbuchsbaum/code/hrfdecoder Rscript -e \"devtools::test(filter = 'ar-prewhitening')\""},{"path":[]},{"path":"/IMPLEMENTATION_STATUS.html","id":"what-you-can-do-now","dir":"","previous_headings":"","what":"What You Can Do Now","title":"Implementation Status: AR Prewhitening","text":"Use AR prewhitening analyses: Run tests verify everything works: Try examples: Read documentation: AR_PREWHITENING_INTEGRATION_PLAN.md - Design & rationale AR_IMPLEMENTATION_SUMMARY.md - Usage guide CHANGELOG.md - changed","code":"fit <- fit_hrfdecoder(..., ar_order = 1, ar_pooling = \"run\", ...) devtools::test() source(\"examples/ar_prewhitening_example.R\")"},{"path":"/IMPLEMENTATION_STATUS.html","id":"questions","dir":"","previous_headings":"","what":"Questions?","title":"Implementation Status: AR Prewhitening","text":"implementation complete functional. skip guards tests temporary placeholders now removed underlying functionality exists works. see issues, bugs fix, missing features! ğŸ‰","code":""},{"path":[]},{"path":"/README_RMVPA_INTEGRATION.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"directory contains detailed architectural analysis rMVPA package focus hrfdecoder continuous-time decoder model integrates framework.","code":""},{"path":"/README_RMVPA_INTEGRATION.html","id":"documents-in-this-analysis","dir":"","previous_headings":"","what":"Documents in This Analysis","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"Complete 9-section report covering aspects rMVPA architecture Detailed data flow diagrams Code signatures patterns Integration recommendations ~3,000 lines detailed documentation Quick file reference line numbers Critical integration points Data structure flows Implementation checklist Best quick lookups navigation","code":""},{"path":[]},{"path":"/README_RMVPA_INTEGRATION.html","id":"id_1-s3-plugin-system","dir":"","previous_headings":"Key Architectural Insights","what":"1. S3 Plugin System","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"rMVPA extends models via S3 method dispatch. Core operations: hrfdecoder: Implement 5 methods plug pipeline: - train_model.hrfdecoder_model() - trains solver, returns fit object - format_result.hrfdecoder_model() - predicts + aggregates events - merge_results.hrfdecoder_model() - combines fold results - compute_performance.hrfdecoder_model() - extracts metrics - y_train.hrfdecoder_model() - returns dummy sequence (real labels)","code":"train_model(model_spec, data, labels, ...)          # S3 generic format_result(model_spec, fit, error, context, ...) # S3 generic merge_results(model_spec, fold_results, ...)        # S3 generic compute_performance(model_spec, classification_result) # S3 generic"},{"path":"/README_RMVPA_INTEGRATION.html","id":"id_2-data-flow-strict-temporal-ordering","dir":"","previous_headings":"Key Architectural Insights","what":"2. Data Flow: Strict Temporal Ordering","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"Critical: Data extraction (step 1) occurs fold creation. determines preprocessing can happen.","code":"ROI Extracted (TR x features)     â†“ [PREPROCESSING OPPORTUNITY] â† AR whitening goes here     â†“ CV Folds Created (based on block_var/runs)     â†“ For Each Fold:   â”œâ”€ train_model() [train data only]   â”œâ”€ format_result() [test data, using train params]   â”œâ”€ Result: tibble(class, probs, y_true, test_ind)     â†“ merge_results() [combine folds, compute metrics]     â†“ Final: classification_result with event-level predictions"},{"path":"/README_RMVPA_INTEGRATION.html","id":"id_3-preprocessing-timing-two-options","dir":"","previous_headings":"Key Architectural Insights","what":"3. Preprocessing Timing (Two Options)","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"Option : Per-ROI, Folds (PREFERRED) Pros: Stable AR estimation, whole-ROI context Cons: Must apply params per-fold train_model Option B: Per-Fold, Inside train_model Pros: Strict train/test separation Cons: Slower (AR estimated per fold), harder parameter tuning","code":"Extract ROI â†’ AR estimation â†’ Prewhiten ROI â†’ Create folds â†’ Train/test Create folds â†’ For each fold: estimate AR, prewhiten, fit â†’ apply params to test"},{"path":"/README_RMVPA_INTEGRATION.html","id":"id_4-hrfdecoder-specific-design","dir":"","previous_headings":"Key Architectural Insights","what":"4. hrfdecoder-Specific Design","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"Special Extension: hrfdecoder_design subclasses mvpa_design - Stores event metadata: event_model, events - Dummy y_train (TR indices 1:T) fold construction - Actual targets come event table, y_train - Fold assignment via block_var (runs), y values - Ensures temporal structure preserved cross-validation Example Usage:","code":"design <- hrfdecoder_design(   event_model = evm,        # from fmridesign   events = events_df,       # with onset, condition   block_var = run_ids       # 1:3, determines folds )  mspec <- hrfdecoder_model(   dataset = dset,   design = design,   lambda_W = 10,   window = c(4, 8),   # ... other parameters )  results <- run_searchlight(mspec, radius=8)"},{"path":"/README_RMVPA_INTEGRATION.html","id":"id_5-where-ar-prewhitening-fits","dir":"","previous_headings":"Key Architectural Insights","what":"5. Where AR Prewhitening Fits","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"train_model.hrfdecoder_model(): 1. Receive training data matrix (ROI: obs x voxels) 2. Estimate AR model per voxel 3. Apply whitening training data 4. Fit decoder whitened training data 5. Store AR parameters fit object format_result.hrfdecoder_model(): 1. Receive test data matrix 2. Apply AR transformation (using training params) 3. Predict whitened test data 4. Aggregate TR-level predictions event level 5. Return classification_result Key: AR parameters estimated training data , applied identically test data.","code":""},{"path":"/README_RMVPA_INTEGRATION.html","id":"integration-checklist","dir":"","previous_headings":"","what":"Integration Checklist","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"implementing AR prewhitening hrfdecoder: Default: ar_order = 0 (whitening) Callable: hrfdecoder_model(..., ar_order = 2, ...) Per-voxel AR(p) estimation training data Store coefficients fit object Forward: residuals AR model (training data) Inverse: apply transformation test data Lines 175-210 /R/hrfdecoder_model.R Add AR estimation hrfdecoder_fit() Store ar_params returned fit object Lines 216-297 /R/hrfdecoder_model.R Apply AR transformation test data prediction Use AR params training fold Verify train/test independence (data leakage) Cross-validation /without AR Parameter sensitivity analysis","code":""},{"path":[]},{"path":[]},{"path":"/README_RMVPA_INTEGRATION.html","id":"train_model","dir":"","previous_headings":"Method Signatures Reference","what":"train_model","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"","code":"train_model.hrfdecoder_model <- function(   obj,          # hrfdecoder_model spec   train_dat,    # tibble/matrix: obs x features (already split by fold)   y,            # vector: dummy sequence (ignored by hrfdecoder)   sl_info,      # list: center_local_id, center_global_id (searchlight info)   cv_spec,      # cross_validation spec   indices,      # integer: global voxel indices   ...           # additional arguments ) # Returns: object of class \"hrfdecoder_fit_wrap\" with fit, ar_params, indices"},{"path":"/README_RMVPA_INTEGRATION.html","id":"format_result","dir":"","previous_headings":"Method Signatures Reference","what":"format_result","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"","code":"format_result.hrfdecoder_model <- function(   obj,              # hrfdecoder_model spec   result,           # fit object from train_model   error_message,    # NULL if no error   context,          # list with test, ytest, roi, ytrain, train, .id   ...               # additional arguments ) # Returns: tibble with class, probs, y_true, test_ind, fit, error, error_message"},{"path":"/README_RMVPA_INTEGRATION.html","id":"merge_results","dir":"","previous_headings":"Method Signatures Reference","what":"merge_results","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"","code":"merge_results.hrfdecoder_model <- function(   obj,          # hrfdecoder_model spec   result_set,   # tibble: results from all folds (from format_result)   indices,      # integer: global voxel indices for ROI   id,           # identifier for this ROI   ...           # additional arguments ) # Returns: tibble with result, indices, performance, id, error, error_message"},{"path":"/README_RMVPA_INTEGRATION.html","id":"file-organization","dir":"","previous_headings":"","what":"File Organization","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"","code":"rMVPA/R/ â”œâ”€â”€ allgeneric.R          [944 lines] Generic functions & S3 dispatch â”œâ”€â”€ mvpa_model.R          [387 lines] Standard MVPA model specs â”œâ”€â”€ hrfdecoder_model.R    [336 lines] hrfdecoder adapter â† MODIFY THIS â”œâ”€â”€ hrfdecoder_design.R   [174 lines] hrfdecoder design extension â”œâ”€â”€ mvpa_iterate.R        [585 lines] CV iteration & fold generation â”œâ”€â”€ crossval.R            [903 lines] CV specifications â”œâ”€â”€ common.R              [624 lines] Normalization utilities â”œâ”€â”€ dataset.R             [479 lines] Data structures â”œâ”€â”€ design.R              [344 lines] Design specifications â”œâ”€â”€ classifiers.R         [739 lines] Model registry â”œâ”€â”€ searchlight.R        [1130 lines] Searchlight execution â””â”€â”€ regional.R            [549 lines] Regional analysis"},{"path":[]},{"path":"/README_RMVPA_INTEGRATION.html","id":"current-bottlenecks","dir":"","previous_headings":"Performance Considerations","what":"Current Bottlenecks","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"Mitigation: Estimate per ROI, apply per-fold Current: Simple list assignment (fine) Mitigation: Use sparse matrices possible","code":""},{"path":"/README_RMVPA_INTEGRATION.html","id":"optimization-opportunities","dir":"","previous_headings":"Performance Considerations","what":"Optimization Opportunities","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"Cache AR estimates â€™re stable across folds Vectorize per-voxel AR estimation (use RcppArmadillo) Parallelize fold-level prewhitening","code":""},{"path":"/README_RMVPA_INTEGRATION.html","id":"testing-strategy","dir":"","previous_headings":"","what":"Testing Strategy","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"","code":"# 1. Unit tests for AR functions test_that(\"AR whitening preserves shape\", { ... }) test_that(\"AR parameters recoverable\", { ... })  # 2. Integration tests for train_model test_that(\"train_model returns ar_params\", { ... }) test_that(\"format_result applies ar_params correctly\", { ... })  # 3. Cross-validation tests test_that(\"train/test data independence\", { ... }) test_that(\"AR estimation only uses training data\", { ... })  # 4. End-to-end tests test_that(\"searchlight with AR produces valid results\", { ... }) test_that(\"performance metrics computed correctly\", { ... })"},{"path":"/README_RMVPA_INTEGRATION.html","id":"references","dir":"","previous_headings":"","what":"References","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"rMVPA Main Report: See rMVPA_ARCHITECTURE_REPORT.md comprehensive documentation Quick Reference: See RMVPA_KEY_FINDINGS.md file locations navigation hrfdecoder Package: Continuous-time decoding solver ALS optimization fmridesign Package: Event model construction HRF convolution neuroim2 Package: Neuroimaging data structures (NeuroVec, NeuroVol)","code":""},{"path":"/README_RMVPA_INTEGRATION.html","id":"questions--answers","dir":"","previous_headings":"","what":"Questions & Answers","title":"hrfdecoder Integration with rMVPA: Comprehensive Analysis","text":"Q: preprocessing happen pipeline? : Two options documented main report (Section 5.1). Either per-ROI fold creation (preferred) per-fold inside train_model() (stricter slower). Q: train/test splits handled? : block_var (run IDs) blocked_cross_validation(). run held completely, temporal mixing. AR parameters estimated training runs , applied held-runs. Q: Can preprocessing model-specific? : Yes! S3 method dispatch allows preprocess_roi.hrfdecoder_model() exist alongside preprocess_roi.mvpa_model(), etc. model can implement preprocessing. Q: ensure prewhitening happens train_model? : Modify process_roi.default() /R/allgeneric.R call roi <- preprocess_roi(mod_spec, roi) internal_crossval(). Report Generated: November 9, 2024 rMVPA Version Analyzed: Current (dev) Scope: Complete architectural analysis (7,000+ lines code reviewed)","code":""},{"path":[]},{"path":[]},{"path":"/RMVPA_KEY_FINDINGS.html","id":"core-plugin-architecture-files","dir":"","previous_headings":"Quick File Reference","what":"Core Plugin Architecture Files","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"Generic functions: train_model(), predict_model(), format_result(), merge_results(), compute_performance() Key class resolution dispatcher process_roi.default() entry point (line 206-222) - controls flow CV methods mvpa_model() (line 260-337) - creates standard model spec create_model_spec() (line 196-213) - internal spec factory S3 methods mvpa_model class Shows attach metadata model specs hrfdecoder_model() (line 97-146) - creates hrfdecoder spec train_model.hrfdecoder_model() (line 175-210) - trains decoder per fold format_result.hrfdecoder_model() (line 216-297) - predicts + aggregates events merge_results.hrfdecoder_model() (line 303-328) - folds â†’ classification_result compute_performance.hrfdecoder_model() (line 334-336) - delegates perf_fun hrfdecoder_design() (line 60-130) - extends mvpa_design event metadata Validates events, sampling_frame, TR alignment Stores event_model events training","code":""},{"path":"/RMVPA_KEY_FINDINGS.html","id":"data-flow-architecture","dir":"","previous_headings":"Quick File Reference","what":"Data Flow Architecture","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"Line 228-229: Generates folds via crossval_samples() Line 263-291: Per-fold training via train_model() + format_result() Line 295: Merges via merge_results() external_crossval() (line 89-175) - external test sets prewhitening must hook (inside train_model) blocked_cross_validation() (line 402-406) crossval_samples() generics (line 867) crossval_samples.blocked_cross_validation() (line 572-574) Implementation: crossv_block() (line 170-192)","code":""},{"path":"/RMVPA_KEY_FINDINGS.html","id":"preprocessing-architecture","dir":"","previous_headings":"Quick File Reference","what":"Preprocessing Architecture","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"normalize_image_samples() (line 52-60) - per-volume z-scoring standardize_vars() (line 66-77) - per-block z-scoring normalize_surface_samples() (line 82-89) IMPORTANT: dataset-level (fold-aware) AR/nuisance regression currently","code":""},{"path":"/RMVPA_KEY_FINDINGS.html","id":"dataset--design-structures","dir":"","previous_headings":"Quick File Reference","what":"Dataset & Design Structures","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"mvpa_dataset() (line 193+) - creates dataset spec gen_sample_dataset() (line 65-156) - test data generation Structure: dataset$train_data, dataset$test_data, dataset$mask mvpa_design() - creates design spec y_train(), y_test() generics (line 366-383)","code":""},{"path":"/RMVPA_KEY_FINDINGS.html","id":"model-registry","dir":"","previous_headings":"Quick File Reference","what":"Model Registry","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"MVPAModels environment - registry available models Example: MVPAModels$corclass (line 134-158) fit(), predict(), prob() methods","code":""},{"path":[]},{"path":"/RMVPA_KEY_FINDINGS.html","id":"where-preprocessing-can-hook-in","dir":"","previous_headings":"Critical Integration Points for AR Prewhitening","what":"Where Preprocessing Can Hook In","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"","code":"OPTION A: Per-ROI, Before Fold Creation (PREFERRED)   Location: process_roi() â†’ [preprocess_roi() HOOK] â†’ internal_crossval()    Pros:   âœ“ Whole-ROI AR estimation (more stable)   âœ“ Easier to manage parameters   âœ— Must handle fold-level AR application in train_model  OPTION B: Per-Fold, Inside train_model()   Location: train_model.hrfdecoder_model() â†’ [estimate AR] â†’ train/test    Pros:   âœ“ Fold-specific AR parameters (stricter separation)   âœ“ Simpler logic   âœ— Repeated AR estimation per fold (slower, harder tuning)"},{"path":"/RMVPA_KEY_FINDINGS.html","id":"key-code-locations-for-integration","dir":"","previous_headings":"Critical Integration Points for AR Prewhitening","what":"Key Code Locations for Integration","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"Entry point modification (add hook): AR whitening storage: Test data application:","code":"/R/allgeneric.R:206-222 (process_roi.default)   Add: roi <- preprocess_roi(mod_spec, roi) /R/hrfdecoder_model.R:209 (train_model returns fit object)   Store: AR params in fit object for test data application /R/hrfdecoder_model.R:237-243 (format_result.hrfdecoder_model)   Apply: Same AR params to test data before prediction"},{"path":"/RMVPA_KEY_FINDINGS.html","id":"data-structure-flow-for-hrfdecoder","dir":"","previous_headings":"","what":"Data Structure Flow for hrfdecoder","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"","code":"hrfdecoder_design()   â”œâ”€ train_design = data.frame(y=1:T, block=block_var)   â”œâ”€ y_train = ~ y  (dummy, just for length)   â”œâ”€ block_var = ~ block  (DETERMINES folds)   â”œâ”€ event_model  (from fmridesign)   â””â”€ events  (data.frame with onset, condition)  mvpa_dataset()   â”œâ”€ train_data = NeuroVec(T x V)  [TR-level]   â”œâ”€ test_data = NeuroVec(T_test x V)   â””â”€ mask = NeuroVol  hrfdecoder_model()   â”œâ”€ lambda_W, lambda_HRF, lambda_smooth   â”œâ”€ basis = HRF (fmrihrf)   â”œâ”€ window = event aggregation   â””â”€ crossval = blocked_cross_validation(block_var)  run_searchlight(spec) â†’ process_roi() â†’ internal_crossval()   â”œâ”€ [EXTRACT ROI: TR x voxels]   â”œâ”€ [PREPROCESS: AR whitening???]   â”œâ”€ [CREATE FOLDS by block_var]   â”œâ”€ For each fold:   â”‚  â”œâ”€ train_model() â†’ hrfdecoder_fit(X_train, event_model)   â”‚  â””â”€ format_result() â†’ predict(X_test) â†’ aggregate_events()   â””â”€ merge_results() â†’ classification_result with event-level probs"},{"path":[]},{"path":"/RMVPA_KEY_FINDINGS.html","id":"for-implementing-train_model-with-preprocessing","dir":"","previous_headings":"Key Method Signatures","what":"For implementing train_model with preprocessing:","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"","code":"train_model.hrfdecoder_model <- function(obj, train_dat, y, sl_info, cv_spec, indices, ...) {   X <- as.matrix(train_dat)      # PREWHITEN training data   X_pw <- apply_ar(X, obj$ar_order)  # Returns: (X_pw, ar_params)   ar_params <- attr(X_pw, \"ar_params\")      # FIT on prewhitened data   fit <- hrfdecoder::hrfdecoder_fit(X_pw, event_model=obj$design$event_model, ...)      # RETURN with AR params for test data   structure(     list(fit=fit, ar_params=ar_params, sl_info=sl_info, indices=indices),     class=\"hrfdecoder_fit_wrap\"   ) }  format_result.hrfdecoder_model <- function(obj, result, error_message=NULL, context, ...) {   Xtest <- as.matrix(context$test)      # APPLY SAME AR to test data using training fold's params   Xtest_pw <- apply_ar_inverse(Xtest, result$ar_params)      # PREDICT on prewhitened test data   Ptest <- predict(result$fit, Xtest_pw)      # AGGREGATE to event level   agg <- hrfdecoder::aggregate_events(Ptest, obj$design$events, window=obj$window)      # RETURN results tibble   tibble::tibble(     class = list(pred_class),     probs = list(probs),     y_true = list(observed),     test_ind = list(as.integer(context$test)),     fit = list(if (obj$return_fits) result$fit else NULL),     error = FALSE,     error_message = \"~\"   ) }"},{"path":[]},{"path":"/RMVPA_KEY_FINDINGS.html","id":"already-implemented-","dir":"","previous_headings":"Summary: Whatâ€™s Already in Place vs.Â Whatâ€™s Needed","what":"Already Implemented âœ“","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"hrfdecoder_model() spec creation train_model.hrfdecoder_model() solver integration format_result.hrfdecoder_model() event aggregation merge_results.hrfdecoder_model() fold merging hrfdecoder_design() metadata Blocked cross-validation (respects TR structure) S3 method dispatch system","code":""},{"path":"/RMVPA_KEY_FINDINGS.html","id":"needs-implementation-for-ar-prewhitening-","dir":"","previous_headings":"Summary: Whatâ€™s Already in Place vs.Â Whatâ€™s Needed","what":"Needs Implementation for AR Prewhitening âœ—","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"AR model fitting (per-voxel global) AR whitening application (forward) AR inverse application (test data) Storage fit object (ar_params) Integration point process_roi() train_model() Parameter control (ar_order) hrfdecoder_model() Validation tests (train/test separation)","code":""},{"path":"/RMVPA_KEY_FINDINGS.html","id":"files-used-in-this-analysis","dir":"","previous_headings":"","what":"Files Used in This Analysis","title":"rMVPA Architecture: Key Findings for hrfdecoder Integration","text":"Absolute paths rMVPA package: /Users/bbuchsbaum/code/rMVPA/R/allgeneric.R - 944 lines /Users/bbuchsbaum/code/rMVPA/R/mvpa_model.R - 387 lines /Users/bbuchsbaum/code/rMVPA/R/hrfdecoder_model.R - 336 lines /Users/bbuchsbaum/code/rMVPA/R/hrfdecoder_design.R - 174 lines /Users/bbuchsbaum/code/rMVPA/R/mvpa_iterate.R - 585 lines /Users/bbuchsbaum/code/rMVPA/R/crossval.R - 903 lines /Users/bbuchsbaum/code/rMVPA/R/common.R - 624 lines /Users/bbuchsbaum/code/rMVPA/R/dataset.R - 479 lines /Users/bbuchsbaum/code/rMVPA/R/design.R - 344 lines /Users/bbuchsbaum/code/rMVPA/R/classifiers.R - 739 lines /Users/bbuchsbaum/code/rMVPA/R/searchlight.R - 1130 lines /Users/bbuchsbaum/code/rMVPA/R/regional.R - 549 lines Total analyzed: ~7,000 lines rMVPA code focused architecture","code":""},{"path":"/articles/01-getting-started.html","id":"goal","dir":"Articles","previous_headings":"","what":"Goal","title":"Getting started with hrfdecoder","text":"Learn fit HRF-aware weakly supervised decoder fMRI time series data make predictions new data.","code":""},{"path":"/articles/01-getting-started.html","id":"tldr","dir":"Articles","previous_headings":"","what":"TL;DR","title":"Getting started with hrfdecoder","text":"","code":"library(hrfdecode) library(fmridesign)  # Fit decoder from fMRI timeseries and event onsets fit <- fit_hrfdecoder(   Y = fmri_data,           # T Ã— V matrix (time Ã— voxels)   event_model = ev_model,  # event_model from fmridesign   baseline_model = bl_model )  # Predict on new data preds <- predict_hrfdecoder(fit, Y_test = test_data, mode = \"trial\")"},{"path":"/articles/01-getting-started.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting started with hrfdecoder","text":"Traditional MVPA requires trial-averaged data explicit trial labels. hrfdecode package takes different approach: learns directly continuous fMRI time series event timing information. â€œweakly supervisedâ€ approach jointly estimates: Soft labels â€” continuous trial-level predictions HRF parameters â€” subject-specific hemodynamic response Decoder weights â€” multivariate classification function three components optimized together using alternating least squares.","code":""},{"path":"/articles/01-getting-started.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Getting started with hrfdecoder","text":"tutorial, â€™ll create synthetic data mimics simple two-class decoding experiment. Next, define experimental design using fmridesign package. Now simulate fMRI data signal subset voxels.","code":"library(hrfdecode) library(fmridesign) # Simulation parameters n_trs <- 200        # Number of time points n_voxels <- 50      # Number of voxels n_trials <- 40      # Number of trials tr <- 2             # TR in seconds  # Create event table (onset times and conditions) onsets <- seq(10, n_trs * tr - 20, length.out = n_trials) conditions <- rep(c(\"A\", \"B\"), each = n_trials / 2)  event_table <- data.frame(   onset = onsets,   condition = conditions,   duration = 1 )  head(event_table) #>      onset condition duration #> 1 10.00000         A        1 #> 2 19.48718         A        1 #> 3 28.97436         A        1 #> 4 38.46154         A        1 #> 5 47.94872         A        1 #> 6 57.43590         A        1 # Create event model from event table   ev_model <- event_model(   onset ~ hrf(condition, basis = \"spmg1\"),   data = event_table,   block = ~ 1,   sampling_frame = fmrihrf::sampling_frame(TR = tr, blocklens = n_trs) )  # Create baseline model (intercept + linear drift)   bl_model <- baseline_model(basis = \"bs\", degree = 3, sframe = fmrihrf::sampling_frame(TR = tr, blocklens = n_trs)) # Generate synthetic BOLD data # First half of voxels discriminate condition A vs B true_pattern <- c(rep(1, n_voxels / 2), rep(0, n_voxels / 2))  # Build TR-grid stick functions per condition stick_A <- rep(0, n_trs) stick_B <- rep(0, n_trs) idx_A <- pmin(n_trs, pmax(1L, floor(event_table$onset[event_table$condition == \"A\"] / tr) + 1L)) idx_B <- pmin(n_trs, pmax(1L, floor(event_table$onset[event_table$condition == \"B\"] / tr) + 1L)) stick_A[idx_A] <- 1 stick_B[idx_B] <- 1 signal <- stick_A - stick_B  # Convolve with HRF (SPMG2 basis) sampled at TR grid hrf_obj <- fmrihrf::getHRF(\"spmg2\") span <- attr(hrf_obj, \"span\") %||% 24 K <- max(1L, ceiling(span / tr)) tgrid <- seq(0, (K - 1L) * tr, by = tr) hrf_basis <- fmrihrf::evaluate(hrf_obj, tgrid) hrf_vec <- as.numeric(hrf_basis %*% c(1, 0)) signal_conv <- stats::convolve(signal, rev(hrf_vec), type = \"open\")[1:n_trs]  # Add to random noise Y_train <- matrix(rnorm(n_trs * n_voxels, sd = 1), n_trs, n_voxels) for (v in 1:n_voxels) {   Y_train[, v] <- Y_train[, v] + signal_conv * true_pattern[v] * 0.5 }  dim(Y_train) #> [1] 200  50"},{"path":"/articles/01-getting-started.html","id":"fitting","dir":"Articles","previous_headings":"","what":"Fitting the decoder","title":"Getting started with hrfdecoder","text":"Now fit decoder using fit_hrfdecoder(). key arguments : Y: fMRI data matrix (time Ã— voxels) event_model: Event design fmridesign baseline_model: Baseline/nuisance model lambda_W: Ridge penalty decoder weights (default: 0.1) max_iter: Maximum ALS iterations (default: 10) fitted object contains: W: Decoder weight vector (V Ã— 1) theta: HRF basis coefficients y_soft: Soft labels (continuous trial predictions) preproc_params: Preprocessing metadata (centering, scaling, AR parameters) convergence: Convergence diagnostics","code":"fit <- fit_hrfdecoder(   Y = Y_train,   ev_model = ev_model,   base_model = bl_model,   lambda_W = 0.1,   max_iter = 10,   verbose = FALSE )  # Inspect fit object class(fit) #> [1] \"hrfdecoder_fit\" names(fit) #>  [1] \"W\"           \"P\"           \"b\"           \"theta\"       \"hrf\"         #>  [6] \"conditions\"  \"background\"  \"converged\"   \"iterations\"  \"settings\"    #> [11] \"train\"       \"preproc\"     \"diagnostics\""},{"path":"/articles/01-getting-started.html","id":"weights","dir":"Articles","previous_headings":"","what":"Inspecting decoder weights","title":"Getting started with hrfdecoder","text":"Letâ€™s examine voxels contribute decoder. decoder correctly identifies signal-bearing voxels (true_pattern = 1).","code":"# Get top 10 voxels by absolute weight top_voxels <- order(abs(fit$W), decreasing = TRUE)[1:10] top_weights <- fit$W[top_voxels]  data.frame(   voxel = top_voxels,   weight = round(top_weights, 3),   true_pattern = true_pattern[top_voxels] ) #>    voxel weight true_pattern #> 1     22  0.028            1 #> 2      6  0.026            1 #> 3     15  0.025            1 #> 4     52 -0.023           NA #> 5     65 -0.023           NA #> 6     51 -0.022           NA #> 7      1  0.022            1 #> 8     69 -0.022           NA #> 9     62 -0.021           NA #> 10    81 -0.021           NA"},{"path":"/articles/01-getting-started.html","id":"prediction","dir":"Articles","previous_headings":"","what":"Making predictions","title":"Getting started with hrfdecoder","text":"predict() method supports two modes: â€œtrâ€ â€” TR-level predictions (one per time point) â€œtrialâ€ â€” Trial-level predictions (aggregated across event duration) Trial-level predictions aggregate TR-level signal using HRF-weighted averaging within event window.","code":"# Create test data (new noise, same signal) Y_test <- matrix(rnorm(n_trs * n_voxels, sd = 1), n_trs, n_voxels) for (v in 1:n_voxels) {   Y_test[, v] <- Y_test[, v] + signal_conv * true_pattern[v] * 0.5 }  # TR-level predictions pred_tr <- predict_hrfdecoder(fit, Y_test = Y_test, mode = \"tr\") length(pred_tr)  # One prediction per TR #> [1] 600 # Trial-level predictions (aggregated within event windows) pred_trial <- predict_hrfdecoder(fit, Y_test = Y_test, ev_model_test = ev_model, mode = \"trial\") length(pred_trial)  # One prediction per trial #> [1] 2"},{"path":"/articles/01-getting-started.html","id":"evaluation","dir":"Articles","previous_headings":"","what":"Evaluating performance","title":"Getting started with hrfdecoder","text":"two-class problem, can compute classification accuracy.","code":"# Convert event-level probabilities to class predictions true_labels <- ifelse(conditions == \"A\", 1, -1) pred_classes <- ifelse(pred_trial$probs[, 1] >= pred_trial$probs[, 2], 1, -1)  # Accuracy accuracy <- mean(pred_classes == true_labels) cat(\"Classification accuracy:\", round(accuracy * 100, 1), \"%\\n\") #> Classification accuracy: 100 %"},{"path":"/articles/01-getting-started.html","id":"visualization","dir":"Articles","previous_headings":"","what":"Visualizing predictions","title":"Getting started with hrfdecoder","text":"Trial-level predictions vs.Â true condition labels. Positive values indicate condition , negative values condition B.","code":"if (requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)    plot_df <- data.frame(     trial = 1:n_trials,     prediction = as.numeric(pred_trial$probs[, 1] - pred_trial$probs[, 2]),     true_label = true_labels,     condition = conditions   )    ggplot(plot_df, aes(x = trial, y = prediction, color = condition)) +     geom_point(size = 2.5) +     geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +     hrfdecode::scale_color_albers() +     labs(       title = \"Trial-level decoder predictions\",       subtitle = \"Weakly supervised decoder correctly separates conditions\",       x = \"Trial number\",       y = \"Soft label prediction\",       color = \"Condition\"     ) }"},{"path":"/articles/01-getting-started.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Getting started with hrfdecoder","text":"tutorial covered basics fitting predicting hrfdecode. advanced topics: AR Prewhitening â€” Handle temporal autocorrelation multi-run data rMVPA Integration â€” Run searchlight analysis cross-validation HRF Estimation â€” Understand joint HRF learning event aggregation Weakly Supervised Learning â€” Deep dive ALS algorithm","code":""},{"path":"/articles/01-getting-started.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session info","title":"Getting started with hrfdecoder","text":"","code":"sessioninfo::session_info(pkgs = \"hrfdecode\") #> â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  setting  value #>  version  R version 4.5.1 (2025-06-13) #>  os       macOS Sonoma 14.3 #>  system   aarch64, darwin20 #>  ui       X11 #>  language en #>  collate  en_US.UTF-8 #>  ctype    en_US.UTF-8 #>  tz       America/Toronto #>  date     2025-11-09 #>  pandoc   3.7.0.2 @ /opt/homebrew/bin/ (via rmarkdown) #>  quarto   1.7.32 @ /usr/local/bin/quarto #>  #> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  package        * version    date (UTC) lib source #>  askpass          1.2.1      2024-10-04 [2] CRAN (R 4.5.0) #>  assertthat       0.2.1      2019-03-21 [2] CRAN (R 4.5.0) #>  backports        1.5.0      2024-05-23 [2] CRAN (R 4.5.0) #>  base64enc        0.1-3      2015-07-28 [2] CRAN (R 4.5.0) #>  bdsmatrix        1.3-7      2024-03-02 [2] CRAN (R 4.5.0) #>  bigassertr       0.1.7      2025-06-27 [2] CRAN (R 4.5.0) #>  bigparallelr     0.3.2      2021-10-02 [2] CRAN (R 4.5.0) #>  bigstatsr        1.6.2      2025-07-29 [2] CRAN (R 4.5.0) #>  bit              4.6.0      2025-03-06 [2] CRAN (R 4.5.0) #>  bit64            4.6.0-1    2025-01-16 [2] CRAN (R 4.5.0) #>  bitops           1.0-9      2024-10-03 [2] CRAN (R 4.5.0) #>  broom            1.0.10     2025-09-13 [2] CRAN (R 4.5.0) #>  bslib            0.9.0      2025-01-30 [2] CRAN (R 4.5.0) #>  cachem           1.1.0      2024-05-16 [2] CRAN (R 4.5.0) #>  caTools          1.18.3     2024-09-04 [2] CRAN (R 4.5.0) #>  cli              3.6.5      2025-04-23 [2] CRAN (R 4.5.0) #>  clipr            0.8.0      2022-02-22 [2] CRAN (R 4.5.0) #>  codetools        0.2-20     2024-03-31 [3] CRAN (R 4.5.1) #>  colorplane       0.5.0      2025-11-02 [2] Github (bbuchsbaum/colorplane@1b7a26f) #>  corpcor          1.6.10     2021-09-16 [2] CRAN (R 4.5.0) #>  cowplot          1.2.0      2025-07-07 [2] CRAN (R 4.5.0) #>  cpp11            0.5.2      2025-03-03 [2] CRAN (R 4.5.0) #>  crayon           1.5.3      2024-06-20 [2] CRAN (R 4.5.0) #>  crosstalk        1.2.2      2025-08-26 [2] CRAN (R 4.5.0) #>  curl             7.0.0      2025-08-19 [2] CRAN (R 4.5.0) #>  data.table       1.17.8     2025-07-10 [2] CRAN (R 4.5.0) #>  dbscan           1.2.3      2025-08-20 [2] CRAN (R 4.5.0) #>  deflist          0.2.0      2023-04-27 [2] CRAN (R 4.5.0) #>  DEoptimR         1.1-4      2025-07-27 [2] CRAN (R 4.5.0) #>  digest           0.6.37     2024-08-19 [2] CRAN (R 4.5.0) #>  doParallel       1.0.17     2022-02-07 [2] CRAN (R 4.5.0) #>  dplyr            1.1.4      2023-11-17 [2] CRAN (R 4.5.0) #>  entropy          1.3.2      2025-04-07 [2] CRAN (R 4.5.0) #>  evaluate         1.0.5      2025-08-27 [2] CRAN (R 4.5.0) #>  farver           2.1.2      2024-05-13 [2] CRAN (R 4.5.0) #>  fastmap          1.2.0      2024-05-15 [2] CRAN (R 4.5.0) #>  fdrtool          1.2.18     2024-08-20 [2] CRAN (R 4.5.0) #>  ff               4.5.2      2025-01-13 [2] CRAN (R 4.5.0) #>  ffmanova         1.1.2      2023-10-18 [2] CRAN (R 4.5.0) #>  filenamer        0.3        2025-04-09 [2] CRAN (R 4.5.0) #>  flock            0.7        2016-11-12 [2] CRAN (R 4.5.0) #>  fmriAR           0.1.0      2025-10-18 [2] Github (bbuchsbaum/fmriAR@0b10352) #>  fmridesign     * 0.5.0      2025-11-09 [2] Github (bbuchsbaum/fmridesign@f1462eb) #>  fmrihrf          0.1.0.9000 2025-11-01 [2] Github (bbuchsbaum/fmrihrf@708058f) #>  FNN              1.1.4.1    2024-09-22 [2] CRAN (R 4.5.0) #>  fontawesome      0.5.3      2024-11-16 [2] CRAN (R 4.5.0) #>  foreach          1.5.2      2022-02-02 [2] CRAN (R 4.5.0) #>  formatR          1.14       2023-01-17 [2] CRAN (R 4.5.0) #>  fs               1.6.6      2025-04-12 [2] CRAN (R 4.5.0) #>  furrr            0.3.1      2022-08-15 [2] CRAN (R 4.5.0) #>  futile.logger    1.4.3      2016-07-10 [2] CRAN (R 4.5.0) #>  futile.options   1.0.1      2018-04-20 [2] CRAN (R 4.5.0) #>  future           1.67.0     2025-07-29 [2] CRAN (R 4.5.0) #>  future.apply     1.20.0     2025-06-06 [2] CRAN (R 4.5.0) #>  generics         0.1.4      2025-05-09 [2] CRAN (R 4.5.0) #>  ggplot2        * 4.0.0      2025-09-11 [2] CRAN (R 4.5.0) #>  gifti            0.8.0      2020-11-11 [2] CRAN (R 4.5.0) #>  glmnet           4.1-10     2025-07-17 [2] CRAN (R 4.5.0) #>  globals          0.18.0     2025-05-08 [2] CRAN (R 4.5.0) #>  glue             1.8.0      2024-09-30 [2] CRAN (R 4.5.0) #>  gplots           3.2.0      2024-10-05 [2] CRAN (R 4.5.0) #>  gtable           0.3.6      2024-10-25 [2] CRAN (R 4.5.0) #>  gtools           3.9.5      2023-11-20 [2] CRAN (R 4.5.0) #>  hardhat          1.4.2      2025-08-20 [2] CRAN (R 4.5.0) #>  highr            0.11       2024-05-26 [2] CRAN (R 4.5.0) #>  hms              1.1.4      2025-10-17 [2] CRAN (R 4.5.0) #>  hrfdecode      * 0.2.0      2025-11-09 [1] local #>  htmltools        0.5.8.1    2024-04-04 [2] CRAN (R 4.5.0) #>  htmlwidgets      1.6.4      2023-12-06 [2] CRAN (R 4.5.0) #>  httr             1.4.7      2023-08-15 [2] CRAN (R 4.5.0) #>  igraph           2.2.1      2025-10-27 [2] CRAN (R 4.5.0) #>  io               0.3.2      2019-12-17 [2] CRAN (R 4.5.0) #>  isoband          0.2.7      2022-12-20 [2] CRAN (R 4.5.0) #>  iterators        1.0.14     2022-02-05 [2] CRAN (R 4.5.0) #>  jquerylib        0.1.4      2021-04-26 [2] CRAN (R 4.5.0) #>  jsonlite         2.0.0      2025-03-27 [2] CRAN (R 4.5.0) #>  KernSmooth       2.23-26    2025-01-01 [3] CRAN (R 4.5.1) #>  knitr            1.50       2025-03-16 [2] CRAN (R 4.5.0) #>  labeling         0.4.3      2023-08-29 [2] CRAN (R 4.5.0) #>  lambda.r         1.2.4      2019-09-18 [2] CRAN (R 4.5.0) #>  later            1.4.4      2025-08-27 [2] CRAN (R 4.5.0) #>  lattice          0.22-7     2025-04-02 [3] CRAN (R 4.5.1) #>  lazyeval         0.2.2      2019-03-15 [2] CRAN (R 4.5.0) #>  lifecycle        1.0.4      2023-11-07 [2] CRAN (R 4.5.0) #>  listenv          0.10.0     2025-11-02 [2] CRAN (R 4.5.0) #>  magrittr         2.0.4      2025-09-12 [2] CRAN (R 4.5.0) #>  MASS             7.3-65     2025-02-28 [2] CRAN (R 4.5.0) #>  Matrix           1.7-3      2025-03-11 [3] CRAN (R 4.5.1) #>  matrixStats      1.5.0      2025-01-07 [2] CRAN (R 4.5.0) #>  memoise          2.0.1      2021-11-26 [2] CRAN (R 4.5.0) #>  mime             0.13       2025-03-17 [2] CRAN (R 4.5.0) #>  mmap             0.6-22     2023-12-08 [2] CRAN (R 4.5.0) #>  modelr           0.1.11     2023-03-22 [2] CRAN (R 4.5.0) #>  mvtnorm          1.3-3      2025-01-10 [2] CRAN (R 4.5.0) #>  neuroim2         0.8.3      2025-11-07 [2] Github (bbuchsbaum/neuroim2@77cd9c4) #>  neurosurf        0.1.0      2025-11-02 [2] Github (bbuchsbaum/neurosurf@5af7de2) #>  numDeriv         2016.8-1.1 2019-06-06 [2] CRAN (R 4.5.0) #>  openssl          2.3.4      2025-09-30 [2] CRAN (R 4.5.0) #>  otel             0.2.0      2025-08-29 [2] CRAN (R 4.5.0) #>  parallelly       1.45.1     2025-07-24 [2] CRAN (R 4.5.0) #>  pillar           1.11.1     2025-09-17 [2] CRAN (R 4.5.0) #>  pkgconfig        2.0.3      2019-09-22 [2] CRAN (R 4.5.0) #>  plotly           4.11.0     2025-06-19 [2] CRAN (R 4.5.0) #>  pls              2.8-5      2024-09-15 [2] CRAN (R 4.5.0) #>  plyr             1.8.9      2023-10-02 [2] CRAN (R 4.5.0) #>  pracma           2.4.6      2025-10-22 [2] CRAN (R 4.5.0) #>  prettyunits      1.2.0      2023-09-24 [2] CRAN (R 4.5.0) #>  progress         1.2.3      2023-12-06 [2] CRAN (R 4.5.0) #>  promises         1.5.0      2025-11-01 [2] CRAN (R 4.5.0) #>  proxy            0.4-27     2022-06-09 [2] CRAN (R 4.5.0) #>  ps               1.9.1      2025-04-12 [2] CRAN (R 4.5.0) #>  purrr            1.2.0      2025-11-04 [2] CRAN (R 4.5.0) #>  R.methodsS3      1.8.2      2022-06-13 [2] CRAN (R 4.5.0) #>  R.oo             1.27.1     2025-05-02 [2] CRAN (R 4.5.0) #>  R.utils          2.13.0     2025-02-24 [2] CRAN (R 4.5.0) #>  R6               2.6.1      2025-02-15 [2] CRAN (R 4.5.0) #>  rappdirs         0.3.3      2021-01-31 [2] CRAN (R 4.5.0) #>  RColorBrewer     1.1-3      2022-04-03 [2] CRAN (R 4.5.0) #>  Rcpp             1.1.0      2025-07-02 [2] CRAN (R 4.5.0) #>  RcppArmadillo    15.0.2-2   2025-09-19 [2] CRAN (R 4.5.0) #>  RcppEigen        0.3.4.0.2  2024-08-24 [2] CRAN (R 4.5.0) #>  RcppParallel     5.1.11-1   2025-08-27 [2] CRAN (R 4.5.0) #>  readr            2.1.5      2024-01-10 [2] CRAN (R 4.5.0) #>  Rfit             0.27.0     2024-05-25 [2] CRAN (R 4.5.0) #>  rgl              1.3.24     2025-06-25 [2] CRAN (R 4.5.0) #>  RhpcBLASctl      0.23-42    2023-02-11 [2] CRAN (R 4.5.0) #>  rlang            1.1.6      2025-04-11 [2] CRAN (R 4.5.0) #>  rmarkdown        2.30       2025-09-28 [2] CRAN (R 4.5.0) #>  rmio             0.4.0      2022-02-17 [2] CRAN (R 4.5.0) #>  rMVPA            0.1.2      2025-11-09 [2] local #>  RNifti           1.8.0      2025-02-22 [2] CRAN (R 4.5.0) #>  RNiftyReg        2.8.4      2024-09-30 [2] CRAN (R 4.5.0) #>  robustbase       0.99-6     2025-09-04 [2] CRAN (R 4.5.0) #>  rsample          1.3.1      2025-07-29 [2] CRAN (R 4.5.0) #>  RSpectra         0.16-2     2024-07-18 [2] CRAN (R 4.5.0) #>  Rvcg             0.25       2025-03-14 [2] CRAN (R 4.5.0) #>  S7               0.2.0      2024-11-07 [2] CRAN (R 4.5.0) #>  sass             0.4.10     2025-04-11 [2] CRAN (R 4.5.0) #>  scales           1.4.0      2025-04-24 [2] CRAN (R 4.5.0) #>  sda              1.3.9      2025-04-08 [2] CRAN (R 4.5.0) #>  shape            1.4.6.1    2024-02-23 [2] CRAN (R 4.5.0) #>  slider           0.3.2      2024-10-25 [2] CRAN (R 4.5.0) #>  sparsediscrim    0.3.0      2021-07-01 [2] CRAN (R 4.5.0) #>  sparsevctrs      0.3.4      2025-05-25 [2] CRAN (R 4.5.0) #>  stringi          1.8.7      2025-03-27 [2] CRAN (R 4.5.0) #>  stringr          1.6.0      2025-11-04 [2] CRAN (R 4.5.0) #>  survival         3.8-3      2024-12-17 [3] CRAN (R 4.5.1) #>  sys              3.4.3      2024-10-04 [2] CRAN (R 4.5.0) #>  tibble           3.3.0      2025-06-08 [2] CRAN (R 4.5.0) #>  tidyr            1.3.1      2024-01-24 [2] CRAN (R 4.5.0) #>  tidyselect       1.2.1      2024-03-11 [2] CRAN (R 4.5.0) #>  tinytex          0.57       2025-04-15 [2] CRAN (R 4.5.0) #>  tzdb             0.5.0      2025-03-15 [2] CRAN (R 4.5.0) #>  utf8             1.2.6      2025-06-08 [2] CRAN (R 4.5.0) #>  vctrs            0.6.5      2023-12-01 [2] CRAN (R 4.5.0) #>  viridisLite      0.4.2      2023-05-02 [2] CRAN (R 4.5.0) #>  vroom            1.6.6      2025-09-19 [2] CRAN (R 4.5.0) #>  warp             0.2.1      2023-11-02 [2] CRAN (R 4.5.0) #>  whitening        1.4.0      2022-06-07 [2] CRAN (R 4.5.0) #>  withr            3.0.2      2024-10-28 [2] CRAN (R 4.5.0) #>  xfun             0.54       2025-10-30 [2] CRAN (R 4.5.0) #>  xml2             1.4.1      2025-10-27 [2] CRAN (R 4.5.0) #>  yaml             2.3.10     2024-07-26 [2] CRAN (R 4.5.0) #>  yardstick        1.3.2      2025-01-22 [2] CRAN (R 4.5.0) #>  #>  [1] /private/var/folders/9h/nkjq6vss7mqdl4ck7q1hd8ph0000gp/T/RtmpfzOoi1/temp_libpathe6e4ebda2ae #>  [2] /Users/bbuchsbaum/Library/R/arm64/4.5/library #>  [3] /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library #>  * â”€â”€ Packages attached to the search path. #>  #> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"},{"path":"/articles/02-ar-prewhitening.html","id":"goal","dir":"Articles","previous_headings":"","what":"Goal","title":"AR prewhitening for temporal autocorrelation","text":"Learn apply autoregressive (AR) prewhitening correct temporal autocorrelation fMRI noise, improving decoder performance statistical efficiency.","code":""},{"path":"/articles/02-ar-prewhitening.html","id":"tldr","dir":"Articles","previous_headings":"","what":"TL;DR","title":"AR prewhitening for temporal autocorrelation","text":"","code":"library(hrfdecode)  # Enable AR(1) prewhitening fit <- fit_hrfdecoder(   Y = fmri_data,   event_model = ev_model,   baseline_model = bl_model,   ar_order = 1              # AR(1) prewhitening )  # Automatic order selection via BIC fit_auto <- fit_hrfdecoder(   Y = fmri_data,   event_model = ev_model,   baseline_model = bl_model,   ar_order = \"auto\",        # BIC-based selection   ar_method = \"yule-walker\" # Yule-Walker estimation )  # Predictions automatically apply learned AR parameters preds <- predict(fit_auto, newdata = test_data, mode = \"trial\")"},{"path":"/articles/02-ar-prewhitening.html","id":"why-prewhiten","dir":"Articles","previous_headings":"","what":"Why prewhiten?","title":"AR prewhitening for temporal autocorrelation","text":"fMRI noise exhibits strong temporal autocorrelation: consecutive time points correlated due physiological processes, scanner drift, hemodynamic smoothing. Ignoring autocorrelation: Inflates false positives statistical tests Reduces decoder efficiency treating correlated errors independent Biases parameter estimates regression models Prewhitening transforms data remove temporal dependencies, making subsequent modeling accurate.","code":""},{"path":"/articles/02-ar-prewhitening.html","id":"ar-models","dir":"Articles","previous_headings":"","what":"AR noise models","title":"AR prewhitening for temporal autocorrelation","text":"hrfdecode supports three AR model types: ar_order = 1 â†’ AR(1), typical fMRI ar_order = 2 â†’ AR(2), captures complex dynamics ar_order = c(1, 1) â†’ ARMA(1,1) flexible requires data ar_order = \"auto\" â†’ Let fmriAR choose optimal order Tests AR(1) AR(5) selects best fit Estimation methods: â€œyule-walkerâ€: Fast, stable (default AR(p)) â€œhannan-rissanenâ€: ARMA models â€œautoâ€: Delegates fmriAR::estimate_ar_order()","code":""},{"path":"/articles/02-ar-prewhitening.html","id":"ar1-basic","dir":"Articles","previous_headings":"","what":"Basic AR(1) prewhitening","title":"AR prewhitening for temporal autocorrelation","text":"Letâ€™s start simple AR(1) example. Now fit without AR(1) prewhitening compare. Examine learned AR parameters: estimated AR(1) coefficient close true value 0.6.","code":"library(hrfdecode) library(fmridesign) # Simulation with AR(1) noise n_trs <- 200 n_voxels <- 30 n_trials <- 40 tr <- 2  # Event table onsets <- seq(10, n_trs * tr - 20, length.out = n_trials) conditions <- rep(c(\"A\", \"B\"), each = n_trials / 2) event_table <- data.frame(onset = onsets, condition = conditions, duration = 1)  # Design models ev_model <- event_model(   onset ~ hrf(condition, basis = \"spmg1\"),   data = event_table,   block = ~ 1,   sampling_frame = fmrihrf::sampling_frame(TR = tr, blocklens = n_trs) ) bl_model <- baseline_model(basis = \"bs\", degree = 3,                            sframe = fmrihrf::sampling_frame(TR = tr, blocklens = n_trs))  # Simulate signal hrf_basis <- fmrihrf::evaluate(fmrihrf::getHRF(\"spmg2\"), seq(0, 24, by = tr)) hrf_vec <- as.numeric(hrf_basis %*% c(1, 0)) stick_A <- rep(0, n_trs); stick_B <- rep(0, n_trs) idx_A <- pmin(n_trs, pmax(1L, floor(event_table$onset[event_table$condition == \"A\"] / tr) + 1L)) idx_B <- pmin(n_trs, pmax(1L, floor(event_table$onset[event_table$condition == \"B\"] / tr) + 1L)) stick_A[idx_A] <- 1; stick_B[idx_B] <- 1 signal <- stick_A - stick_B signal_conv <- stats::convolve(signal, rev(hrf_vec), type = \"open\")[1:n_trs]  # Generate AR(1) noise with phi = 0.6 phi_true <- 0.6 Y_ar <- matrix(0, n_trs, n_voxels) for (v in 1:n_voxels) {   eps <- rnorm(n_trs)   for (t in 2:n_trs) {     eps[t] <- phi_true * eps[t - 1] + rnorm(1)   }   Y_ar[, v] <- eps + signal_conv * (v <= n_voxels / 2) * 0.5 } # No prewhitening fit_none <- fit_hrfdecoder(   Y = Y_ar,   ev_model = ev_model,   base_model = bl_model,   ar_order = NULL,  # No AR correction   verbose = FALSE )  # AR(1) prewhitening fit_ar1 <- fit_hrfdecoder(   Y = Y_ar,   ev_model = ev_model,   base_model = bl_model,   ar_order = 1,     # AR(1)   verbose = FALSE ) # AR coefficients (one per voxel if spatial pooling is off, or global) ar_params <- fit_ar1$preproc_params$ar_params cat(\"AR(1) coefficient (phi):\", round(mean(ar_params$coefficients[[1]]), 3), \"\\n\") #> AR(1) coefficient (phi): NA cat(\"True phi:\", phi_true, \"\\n\") #> True phi: 0.6"},{"path":"/articles/02-ar-prewhitening.html","id":"multi-run","dir":"Articles","previous_headings":"","what":"Multi-run data with run-specific AR","title":"AR prewhitening for temporal autocorrelation","text":"multi-run experiments, different runs may different AR structures. Use ar_pooling = \"run\" estimate separate AR models per run. Fit run-specific AR: Note: Run-specific AR pooling requires additional implementation. current version uses global AR pooling across voxels/runs.","code":"# Simulate 2 runs with different AR parameters n_runs <- 2 n_trs_per_run <- 100 n_trs_total <- n_runs * n_trs_per_run  # Create event table spanning both runs onsets_run <- seq(10, n_trs_per_run * tr - 20, length.out = n_trials / 2) event_table_multi <- data.frame(   onset = c(onsets_run, onsets_run + n_trs_per_run * tr),   condition = rep(c(\"A\", \"B\"), n_trials / 2),   duration = 1,   run = rep(1:2, each = n_trials / 2) )  # Design for multi-run ev_model_multi <- event_model(   onset ~ hrf(condition, basis = \"spmg1\"),   data = event_table_multi,   block = ~ run,   sampling_frame = fmrihrf::sampling_frame(TR = tr, blocklens = rep(n_trs_per_run, n_runs)) ) bl_model_multi <- baseline_model(   basis = \"bs\", degree = 3,   sframe = fmrihrf::sampling_frame(TR = tr, blocklens = rep(n_trs_per_run, n_runs)) )  # Simulate with different AR for each run phi_run1 <- 0.5 phi_run2 <- 0.7 Y_multi <- matrix(0, n_trs_total, n_voxels)  for (v in 1:n_voxels) {   # Run 1   eps1 <- rnorm(n_trs_per_run)   for (t in 2:n_trs_per_run) {     eps1[t] <- phi_run1 * eps1[t - 1] + rnorm(1)   }   # Run 2   eps2 <- rnorm(n_trs_per_run)   for (t in 2:n_trs_per_run) {     eps2[t] <- phi_run2 * eps2[t - 1] + rnorm(1)   }   Y_multi[, v] <- c(eps1, eps2) } # This would require ar_pooling parameter (not yet implemented in current version) # fit_run_ar <- fit_hrfdecoder( #   Y = Y_multi, #   event_model = ev_model_multi, #   baseline_model = bl_model_multi, #   ar_order = 1, #   ar_pooling = \"run\"  # Separate AR per run # )"},{"path":"/articles/02-ar-prewhitening.html","id":"auto-selection","dir":"Articles","previous_headings":"","what":"Automatic AR order selection","title":"AR prewhitening for temporal autocorrelation","text":"Instead manually specifying ar_order, use \"auto\" let BIC select optimal order. Automatic selection balances model fit (lower residual variance) complexity (number AR parameters).","code":"fit_auto <- fit_hrfdecoder(   Y = Y_ar,   ev_model = ev_model,   base_model = bl_model,   ar_order = \"auto\",   ar_method = \"ar\",   verbose = FALSE )  # Check selected order ar_params_auto <- fit_auto$preproc_params$ar_params cat(\"Automatically selected AR order:\", length(ar_params_auto$coefficients[[1]]), \"\\n\") #> Automatically selected AR order: 0"},{"path":"/articles/02-ar-prewhitening.html","id":"prediction-impact","dir":"Articles","previous_headings":"","what":"Impact on prediction","title":"AR prewhitening for temporal autocorrelation","text":"Predictions automatically apply learned AR transformation. AR prewhitening typically improves prediction accuracy properly accounting temporal structure.","code":"# Test data with same AR structure Y_test <- matrix(0, n_trs, n_voxels) for (v in 1:n_voxels) {   eps <- rnorm(n_trs)   for (t in 2:n_trs) {     eps[t] <- phi_true * eps[t - 1] + rnorm(1)   }   Y_test[, v] <- eps + signal_conv * (v <= n_voxels / 2) * 0.5 }  # Predictions with AR(1) model pred_ar1 <- predict_hrfdecoder(fit_ar1, Y_test = Y_test, ev_model_test = ev_model, mode = \"trial\")  # Predictions without AR pred_none <- predict_hrfdecoder(fit_none, Y_test = Y_test, ev_model_test = ev_model, mode = \"trial\")  # Compare accuracy true_labels <- ifelse(conditions == \"A\", 1, -1) acc_ar1 <- mean(sign(pred_ar1$probs[,1] - pred_ar1$probs[,2]) == true_labels) acc_none <- mean(sign(pred_none$probs[,1] - pred_none$probs[,2]) == true_labels)  cat(\"Accuracy with AR(1):\", round(acc_ar1 * 100, 1), \"%\\n\") #> Accuracy with AR(1): 95 % cat(\"Accuracy without AR:\", round(acc_none * 100, 1), \"%\\n\") #> Accuracy without AR: 97.5 %"},{"path":"/articles/02-ar-prewhitening.html","id":"visualization","dir":"Articles","previous_headings":"","what":"Visualizing AR effects","title":"AR prewhitening for temporal autocorrelation","text":"Comparison trial predictions without AR(1) prewhitening. AR correction reduces noise improves separation.","code":"if (requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)    plot_df <- data.frame(     trial = rep(1:n_trials, 2),     prediction = c(as.numeric(pred_ar1$probs[,1] - pred_ar1$probs[,2]),                    as.numeric(pred_none$probs[,1] - pred_none$probs[,2])),     method = rep(c(\"AR(1)\", \"No AR\"), each = n_trials),     condition = rep(conditions, 2)   )    ggplot(plot_df, aes(x = trial, y = prediction, color = condition)) +     geom_point(alpha = 0.7, size = 2) +     geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +     facet_wrap(~ method, ncol = 1) +     hrfdecode::scale_color_albers() +     labs(       title = \"Effect of AR prewhitening on predictions\",       subtitle = \"AR(1) correction improves signal-to-noise ratio\",       x = \"Trial number\",       y = \"Soft label prediction\",       color = \"Condition\"     ) }"},{"path":"/articles/02-ar-prewhitening.html","id":"when-to-use","dir":"Articles","previous_headings":"","what":"When to use AR prewhitening","title":"AR prewhitening for temporal autocorrelation","text":"Use AR prewhitening : Working multi-run fMRI data (common experiments) TR < 2s (faster sampling increases autocorrelation) Noise structure shows strong temporal dependence Statistical efficiency matters (e.g., limited data, weak signals) Skip AR prewhitening : Data already prewhitened (e.g., preprocessing pipelines) Single short run minimal temporal structure Computational speed critical (AR adds overhead)","code":""},{"path":"/articles/02-ar-prewhitening.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"AR prewhitening for temporal autocorrelation","text":"Getting Started â€” Basic decoder fitting workflow rMVPA Integration â€” Cross-validation searchlight analysis HRF Estimation â€” Joint HRF learning Weakly Supervised Learning â€” Algorithm internals","code":""},{"path":"/articles/02-ar-prewhitening.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session info","title":"AR prewhitening for temporal autocorrelation","text":"","code":"sessioninfo::session_info(pkgs = \"hrfdecode\") #> â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  setting  value #>  version  R version 4.5.1 (2025-06-13) #>  os       macOS Sonoma 14.3 #>  system   aarch64, darwin20 #>  ui       X11 #>  language en #>  collate  en_US.UTF-8 #>  ctype    en_US.UTF-8 #>  tz       America/Toronto #>  date     2025-11-09 #>  pandoc   3.7.0.2 @ /opt/homebrew/bin/ (via rmarkdown) #>  quarto   1.7.32 @ /usr/local/bin/quarto #>  #> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  package        * version    date (UTC) lib source #>  askpass          1.2.1      2024-10-04 [2] CRAN (R 4.5.0) #>  assertthat       0.2.1      2019-03-21 [2] CRAN (R 4.5.0) #>  backports        1.5.0      2024-05-23 [2] CRAN (R 4.5.0) #>  base64enc        0.1-3      2015-07-28 [2] CRAN (R 4.5.0) #>  bdsmatrix        1.3-7      2024-03-02 [2] CRAN (R 4.5.0) #>  bigassertr       0.1.7      2025-06-27 [2] CRAN (R 4.5.0) #>  bigparallelr     0.3.2      2021-10-02 [2] CRAN (R 4.5.0) #>  bigstatsr        1.6.2      2025-07-29 [2] CRAN (R 4.5.0) #>  bit              4.6.0      2025-03-06 [2] CRAN (R 4.5.0) #>  bit64            4.6.0-1    2025-01-16 [2] CRAN (R 4.5.0) #>  bitops           1.0-9      2024-10-03 [2] CRAN (R 4.5.0) #>  broom            1.0.10     2025-09-13 [2] CRAN (R 4.5.0) #>  bslib            0.9.0      2025-01-30 [2] CRAN (R 4.5.0) #>  cachem           1.1.0      2024-05-16 [2] CRAN (R 4.5.0) #>  caTools          1.18.3     2024-09-04 [2] CRAN (R 4.5.0) #>  cli              3.6.5      2025-04-23 [2] CRAN (R 4.5.0) #>  clipr            0.8.0      2022-02-22 [2] CRAN (R 4.5.0) #>  codetools        0.2-20     2024-03-31 [3] CRAN (R 4.5.1) #>  colorplane       0.5.0      2025-11-02 [2] Github (bbuchsbaum/colorplane@1b7a26f) #>  corpcor          1.6.10     2021-09-16 [2] CRAN (R 4.5.0) #>  cowplot          1.2.0      2025-07-07 [2] CRAN (R 4.5.0) #>  cpp11            0.5.2      2025-03-03 [2] CRAN (R 4.5.0) #>  crayon           1.5.3      2024-06-20 [2] CRAN (R 4.5.0) #>  crosstalk        1.2.2      2025-08-26 [2] CRAN (R 4.5.0) #>  curl             7.0.0      2025-08-19 [2] CRAN (R 4.5.0) #>  data.table       1.17.8     2025-07-10 [2] CRAN (R 4.5.0) #>  dbscan           1.2.3      2025-08-20 [2] CRAN (R 4.5.0) #>  deflist          0.2.0      2023-04-27 [2] CRAN (R 4.5.0) #>  DEoptimR         1.1-4      2025-07-27 [2] CRAN (R 4.5.0) #>  digest           0.6.37     2024-08-19 [2] CRAN (R 4.5.0) #>  doParallel       1.0.17     2022-02-07 [2] CRAN (R 4.5.0) #>  dplyr            1.1.4      2023-11-17 [2] CRAN (R 4.5.0) #>  entropy          1.3.2      2025-04-07 [2] CRAN (R 4.5.0) #>  evaluate         1.0.5      2025-08-27 [2] CRAN (R 4.5.0) #>  farver           2.1.2      2024-05-13 [2] CRAN (R 4.5.0) #>  fastmap          1.2.0      2024-05-15 [2] CRAN (R 4.5.0) #>  fdrtool          1.2.18     2024-08-20 [2] CRAN (R 4.5.0) #>  ff               4.5.2      2025-01-13 [2] CRAN (R 4.5.0) #>  ffmanova         1.1.2      2023-10-18 [2] CRAN (R 4.5.0) #>  filenamer        0.3        2025-04-09 [2] CRAN (R 4.5.0) #>  flock            0.7        2016-11-12 [2] CRAN (R 4.5.0) #>  fmriAR           0.1.0      2025-10-18 [2] Github (bbuchsbaum/fmriAR@0b10352) #>  fmridesign     * 0.5.0      2025-11-09 [2] Github (bbuchsbaum/fmridesign@f1462eb) #>  fmrihrf          0.1.0.9000 2025-11-01 [2] Github (bbuchsbaum/fmrihrf@708058f) #>  FNN              1.1.4.1    2024-09-22 [2] CRAN (R 4.5.0) #>  fontawesome      0.5.3      2024-11-16 [2] CRAN (R 4.5.0) #>  foreach          1.5.2      2022-02-02 [2] CRAN (R 4.5.0) #>  formatR          1.14       2023-01-17 [2] CRAN (R 4.5.0) #>  fs               1.6.6      2025-04-12 [2] CRAN (R 4.5.0) #>  furrr            0.3.1      2022-08-15 [2] CRAN (R 4.5.0) #>  futile.logger    1.4.3      2016-07-10 [2] CRAN (R 4.5.0) #>  futile.options   1.0.1      2018-04-20 [2] CRAN (R 4.5.0) #>  future           1.67.0     2025-07-29 [2] CRAN (R 4.5.0) #>  future.apply     1.20.0     2025-06-06 [2] CRAN (R 4.5.0) #>  generics         0.1.4      2025-05-09 [2] CRAN (R 4.5.0) #>  ggplot2        * 4.0.0      2025-09-11 [2] CRAN (R 4.5.0) #>  gifti            0.8.0      2020-11-11 [2] CRAN (R 4.5.0) #>  glmnet           4.1-10     2025-07-17 [2] CRAN (R 4.5.0) #>  globals          0.18.0     2025-05-08 [2] CRAN (R 4.5.0) #>  glue             1.8.0      2024-09-30 [2] CRAN (R 4.5.0) #>  gplots           3.2.0      2024-10-05 [2] CRAN (R 4.5.0) #>  gtable           0.3.6      2024-10-25 [2] CRAN (R 4.5.0) #>  gtools           3.9.5      2023-11-20 [2] CRAN (R 4.5.0) #>  hardhat          1.4.2      2025-08-20 [2] CRAN (R 4.5.0) #>  highr            0.11       2024-05-26 [2] CRAN (R 4.5.0) #>  hms              1.1.4      2025-10-17 [2] CRAN (R 4.5.0) #>  hrfdecode      * 0.2.0      2025-11-09 [1] local #>  htmltools        0.5.8.1    2024-04-04 [2] CRAN (R 4.5.0) #>  htmlwidgets      1.6.4      2023-12-06 [2] CRAN (R 4.5.0) #>  httr             1.4.7      2023-08-15 [2] CRAN (R 4.5.0) #>  igraph           2.2.1      2025-10-27 [2] CRAN (R 4.5.0) #>  io               0.3.2      2019-12-17 [2] CRAN (R 4.5.0) #>  isoband          0.2.7      2022-12-20 [2] CRAN (R 4.5.0) #>  iterators        1.0.14     2022-02-05 [2] CRAN (R 4.5.0) #>  jquerylib        0.1.4      2021-04-26 [2] CRAN (R 4.5.0) #>  jsonlite         2.0.0      2025-03-27 [2] CRAN (R 4.5.0) #>  KernSmooth       2.23-26    2025-01-01 [3] CRAN (R 4.5.1) #>  knitr            1.50       2025-03-16 [2] CRAN (R 4.5.0) #>  labeling         0.4.3      2023-08-29 [2] CRAN (R 4.5.0) #>  lambda.r         1.2.4      2019-09-18 [2] CRAN (R 4.5.0) #>  later            1.4.4      2025-08-27 [2] CRAN (R 4.5.0) #>  lattice          0.22-7     2025-04-02 [3] CRAN (R 4.5.1) #>  lazyeval         0.2.2      2019-03-15 [2] CRAN (R 4.5.0) #>  lifecycle        1.0.4      2023-11-07 [2] CRAN (R 4.5.0) #>  listenv          0.10.0     2025-11-02 [2] CRAN (R 4.5.0) #>  magrittr         2.0.4      2025-09-12 [2] CRAN (R 4.5.0) #>  MASS             7.3-65     2025-02-28 [2] CRAN (R 4.5.0) #>  Matrix           1.7-3      2025-03-11 [3] CRAN (R 4.5.1) #>  matrixStats      1.5.0      2025-01-07 [2] CRAN (R 4.5.0) #>  memoise          2.0.1      2021-11-26 [2] CRAN (R 4.5.0) #>  mime             0.13       2025-03-17 [2] CRAN (R 4.5.0) #>  mmap             0.6-22     2023-12-08 [2] CRAN (R 4.5.0) #>  modelr           0.1.11     2023-03-22 [2] CRAN (R 4.5.0) #>  mvtnorm          1.3-3      2025-01-10 [2] CRAN (R 4.5.0) #>  neuroim2         0.8.3      2025-11-07 [2] Github (bbuchsbaum/neuroim2@77cd9c4) #>  neurosurf        0.1.0      2025-11-02 [2] Github (bbuchsbaum/neurosurf@5af7de2) #>  numDeriv         2016.8-1.1 2019-06-06 [2] CRAN (R 4.5.0) #>  openssl          2.3.4      2025-09-30 [2] CRAN (R 4.5.0) #>  otel             0.2.0      2025-08-29 [2] CRAN (R 4.5.0) #>  parallelly       1.45.1     2025-07-24 [2] CRAN (R 4.5.0) #>  pillar           1.11.1     2025-09-17 [2] CRAN (R 4.5.0) #>  pkgconfig        2.0.3      2019-09-22 [2] CRAN (R 4.5.0) #>  plotly           4.11.0     2025-06-19 [2] CRAN (R 4.5.0) #>  pls              2.8-5      2024-09-15 [2] CRAN (R 4.5.0) #>  plyr             1.8.9      2023-10-02 [2] CRAN (R 4.5.0) #>  pracma           2.4.6      2025-10-22 [2] CRAN (R 4.5.0) #>  prettyunits      1.2.0      2023-09-24 [2] CRAN (R 4.5.0) #>  progress         1.2.3      2023-12-06 [2] CRAN (R 4.5.0) #>  promises         1.5.0      2025-11-01 [2] CRAN (R 4.5.0) #>  proxy            0.4-27     2022-06-09 [2] CRAN (R 4.5.0) #>  ps               1.9.1      2025-04-12 [2] CRAN (R 4.5.0) #>  purrr            1.2.0      2025-11-04 [2] CRAN (R 4.5.0) #>  R.methodsS3      1.8.2      2022-06-13 [2] CRAN (R 4.5.0) #>  R.oo             1.27.1     2025-05-02 [2] CRAN (R 4.5.0) #>  R.utils          2.13.0     2025-02-24 [2] CRAN (R 4.5.0) #>  R6               2.6.1      2025-02-15 [2] CRAN (R 4.5.0) #>  rappdirs         0.3.3      2021-01-31 [2] CRAN (R 4.5.0) #>  RColorBrewer     1.1-3      2022-04-03 [2] CRAN (R 4.5.0) #>  Rcpp             1.1.0      2025-07-02 [2] CRAN (R 4.5.0) #>  RcppArmadillo    15.0.2-2   2025-09-19 [2] CRAN (R 4.5.0) #>  RcppEigen        0.3.4.0.2  2024-08-24 [2] CRAN (R 4.5.0) #>  RcppParallel     5.1.11-1   2025-08-27 [2] CRAN (R 4.5.0) #>  readr            2.1.5      2024-01-10 [2] CRAN (R 4.5.0) #>  Rfit             0.27.0     2024-05-25 [2] CRAN (R 4.5.0) #>  rgl              1.3.24     2025-06-25 [2] CRAN (R 4.5.0) #>  RhpcBLASctl      0.23-42    2023-02-11 [2] CRAN (R 4.5.0) #>  rlang            1.1.6      2025-04-11 [2] CRAN (R 4.5.0) #>  rmarkdown        2.30       2025-09-28 [2] CRAN (R 4.5.0) #>  rmio             0.4.0      2022-02-17 [2] CRAN (R 4.5.0) #>  rMVPA            0.1.2      2025-11-09 [2] local #>  RNifti           1.8.0      2025-02-22 [2] CRAN (R 4.5.0) #>  RNiftyReg        2.8.4      2024-09-30 [2] CRAN (R 4.5.0) #>  robustbase       0.99-6     2025-09-04 [2] CRAN (R 4.5.0) #>  rsample          1.3.1      2025-07-29 [2] CRAN (R 4.5.0) #>  RSpectra         0.16-2     2024-07-18 [2] CRAN (R 4.5.0) #>  Rvcg             0.25       2025-03-14 [2] CRAN (R 4.5.0) #>  S7               0.2.0      2024-11-07 [2] CRAN (R 4.5.0) #>  sass             0.4.10     2025-04-11 [2] CRAN (R 4.5.0) #>  scales           1.4.0      2025-04-24 [2] CRAN (R 4.5.0) #>  sda              1.3.9      2025-04-08 [2] CRAN (R 4.5.0) #>  shape            1.4.6.1    2024-02-23 [2] CRAN (R 4.5.0) #>  slider           0.3.2      2024-10-25 [2] CRAN (R 4.5.0) #>  sparsediscrim    0.3.0      2021-07-01 [2] CRAN (R 4.5.0) #>  sparsevctrs      0.3.4      2025-05-25 [2] CRAN (R 4.5.0) #>  stringi          1.8.7      2025-03-27 [2] CRAN (R 4.5.0) #>  stringr          1.6.0      2025-11-04 [2] CRAN (R 4.5.0) #>  survival         3.8-3      2024-12-17 [3] CRAN (R 4.5.1) #>  sys              3.4.3      2024-10-04 [2] CRAN (R 4.5.0) #>  tibble           3.3.0      2025-06-08 [2] CRAN (R 4.5.0) #>  tidyr            1.3.1      2024-01-24 [2] CRAN (R 4.5.0) #>  tidyselect       1.2.1      2024-03-11 [2] CRAN (R 4.5.0) #>  tinytex          0.57       2025-04-15 [2] CRAN (R 4.5.0) #>  tzdb             0.5.0      2025-03-15 [2] CRAN (R 4.5.0) #>  utf8             1.2.6      2025-06-08 [2] CRAN (R 4.5.0) #>  vctrs            0.6.5      2023-12-01 [2] CRAN (R 4.5.0) #>  viridisLite      0.4.2      2023-05-02 [2] CRAN (R 4.5.0) #>  vroom            1.6.6      2025-09-19 [2] CRAN (R 4.5.0) #>  warp             0.2.1      2023-11-02 [2] CRAN (R 4.5.0) #>  whitening        1.4.0      2022-06-07 [2] CRAN (R 4.5.0) #>  withr            3.0.2      2024-10-28 [2] CRAN (R 4.5.0) #>  xfun             0.54       2025-10-30 [2] CRAN (R 4.5.0) #>  xml2             1.4.1      2025-10-27 [2] CRAN (R 4.5.0) #>  yaml             2.3.10     2024-07-26 [2] CRAN (R 4.5.0) #>  yardstick        1.3.2      2025-01-22 [2] CRAN (R 4.5.0) #>  #>  [1] /private/var/folders/9h/nkjq6vss7mqdl4ck7q1hd8ph0000gp/T/RtmpfzOoi1/temp_libpathe6e4ebda2ae #>  [2] /Users/bbuchsbaum/Library/R/arm64/4.5/library #>  [3] /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library #>  * â”€â”€ Packages attached to the search path. #>  #> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"},{"path":"/articles/03-rmvpa-integration.html","id":"goal","dir":"Articles","previous_headings":"","what":"Goal","title":"Integrating with rMVPA workflows","text":"Learn integrate hrfdecode rMVPA package whole-brain searchlight analysis cross-validation.","code":""},{"path":"/articles/03-rmvpa-integration.html","id":"tldr","dir":"Articles","previous_headings":"","what":"TL;DR","title":"Integrating with rMVPA workflows","text":"","code":"library(hrfdecode) library(rMVPA) library(fmridesign) library(fmrihrf)  # Build event_model and design using rMVPA helper sframe <- fmrihrf::sampling_frame(TR = 2, blocklens = c(200, 200)) ev_model <- fmridesign::event_model(   onset ~ fmridesign::hrf(condition, basis = \"spmg1\"),   data = events,   block = ~ run,   sampling_frame = sframe ) block_ids <- rep(1:2, each = 200) design <- rMVPA::hrfdecoder_design(event_model = ev_model,                                    events = events,                                    block_var = block_ids)  # Create hrfdecoder model specification model <- hrfdecoder_model(   design = design,   lambda_W = 0.1,   ar_order = 1 )  # Run searchlight with cross-validation result <- run_searchlight(   model_spec = model,   radius = 8,   method = \"randomized\" )"},{"path":"/articles/03-rmvpa-integration.html","id":"background","dir":"Articles","previous_headings":"","what":"Background","title":"Integrating with rMVPA workflows","text":"rMVPA package provides framework : Searchlight analysis â€” sliding-window decoding across brain ROI-based analysis â€” regional classification Cross-validation â€” train/test splits multiple folds Traditional rMVPA workflows require trial-averaged data explicit labels. hrfdecode integration enables continuous MVPA: decoding directly time series using weakly supervised learning.","code":""},{"path":[]},{"path":"/articles/03-rmvpa-integration.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Integrating with rMVPA workflows","text":"","code":"library(hrfdecode) library(fmridesign) # library(rMVPA)  # Load if available"},{"path":"/articles/03-rmvpa-integration.html","id":"continuous-design","dir":"Articles","previous_headings":"","what":"Creating a continuous design","title":"Integrating with rMVPA workflows","text":"continuous_mvpa_design() function wraps fmridesign::event_model() create rMVPA-compatible design object. Create continuous design: design contains: event_model: fmridesign event model HRF basis baseline_model: Nuisance/baseline regressor model block_var: Blocking structure cross-validation (e.g., runs)","code":"# Example event table (multi-run experiment) n_trials_per_run <- 20 tr <- 2 n_trs_per_run <- 200  event_table <- data.frame(   onset = c(     seq(10, (n_trs_per_run - 10) * tr, length.out = n_trials_per_run),     seq(10, (n_trs_per_run - 10) * tr, length.out = n_trials_per_run)   ),   condition = rep(c(\"face\", \"scene\"), n_trials_per_run),   duration = 1,   run = rep(1:2, each = n_trials_per_run) )  head(event_table, 3) #>      onset condition duration run #> 1 10.00000      face        1   1 #> 2 29.47368     scene        1   1 #> 3 48.94737      face        1   1 # Build sampling frame and event_model for two runs sframe <- fmrihrf::sampling_frame(TR = tr, blocklens = c(n_trs_per_run, n_trs_per_run)) ev_model <- fmridesign::event_model(   onset ~ fmridesign::hrf(condition, basis = \"spmg1\"),   data = event_table,   block = ~ run,   sampling_frame = sframe )  # Block/run ids per TR (length equals total TRs across runs) block_ids <- rep(1:2, each = n_trs_per_run)  # Create hrfdecoder design via rMVPA helper design <- rMVPA::hrfdecoder_design(event_model = ev_model,                                    events = event_table,                                    block_var = block_ids)  # Inspect design object class(design) #> [1] \"hrfdecoder_design\" \"mvpa_design\"       \"list\""},{"path":"/articles/03-rmvpa-integration.html","id":"hrfdecoder-model","dir":"Articles","previous_headings":"","what":"Creating an hrfdecoder model","title":"Integrating with rMVPA workflows","text":"Use hrfdecoder_model() create rMVPA-compatible model specification. model specification implements rMVPA interface: train_model(): Fit decoder training fold predict(): Apply decoder test fold (now using format_result()) merge_results(): Combine cross-validation results","code":"model <- hrfdecoder_model(   design = design,   lambda_W = 0.1,      # Ridge penalty on decoder weights   lambda_HRF = 0.01,   # Prior adherence to HRF basis   ar_order = 1,        # AR(1) prewhitening   max_iter = 10        # ALS iterations )  class(model) #> [1] \"hrfdecoder_model\""},{"path":"/articles/03-rmvpa-integration.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross-validation structure","title":"Integrating with rMVPA workflows","text":"continuous data, cross-validation happens across runs, trials. fold: Training: runs except one Testing: Held-run respects temporal dependencies within runs providing independent test sets.","code":"# Conceptual fold structure for 3 runs: # Fold 1: Train on runs 2,3 | Test on run 1 # Fold 2: Train on runs 1,3 | Test on run 2 # Fold 3: Train on runs 1,2 | Test on run 3"},{"path":"/articles/03-rmvpa-integration.html","id":"searchlight","dir":"Articles","previous_headings":"","what":"Running searchlight analysis","title":"Integrating with rMVPA workflows","text":"Note: following examples conceptual. Full searchlight integration requires rMVPA installation appropriate data structures.","code":"library(rMVPA)  # Prepare 4D fMRI dataset (X Ã— Y Ã— Z Ã— T) # Assume fmri_4d is a neuroim2::NeuroVec object # fmri_4d <- neuroim2::read_vec(\"func_data.nii.gz\")  # Run searchlight result <- run_searchlight(   model_spec = model,   dataset = fmri_4d,   radius = 8,          # 8mm searchlight sphere   method = \"randomized\",   niter = 4            # Iterations for randomized searchlight )  # Extract performance map perf_map <- result$performance  # Threshold at accuracy > 60% sig_map <- perf_map > 0.60"},{"path":"/articles/03-rmvpa-integration.html","id":"searchlight-workflow","dir":"Articles","previous_headings":"Running searchlight analysis","what":"Searchlight workflow","title":"Integrating with rMVPA workflows","text":"searchlight sphere: Extract voxel time series within radius Fit hrfdecoder using train_model() training folds Predict test fold using learned decoder, HRF, AR Aggregate predictions trial level Compute accuracy (metric) Assign performance center voxel result whole-brain performance map showing decoding succeeds.","code":""},{"path":"/articles/03-rmvpa-integration.html","id":"roi-analysis","dir":"Articles","previous_headings":"","what":"ROI-based analysis","title":"Integrating with rMVPA workflows","text":"hypothesis-driven analyses, use predefined ROIs instead searchlight.","code":"# Define ROI mask (e.g., fusiform face area) # roi_mask <- neuroim2::read_vol(\"ffa_mask.nii.gz\")  # Extract ROI time series # Y_roi <- extract_roi_timeseries(fmri_4d, roi_mask)  # Fit decoder on ROI fit_roi <- fit_hrfdecoder(   Y = Y_roi,   event_model = design$event_model,   baseline_model = design$baseline_model,   lambda_W = 0.1,   ar_order = 1 )  # Cross-validate manually # (or use rMVPA::crossval_* functions with hrfdecoder_model)"},{"path":"/articles/03-rmvpa-integration.html","id":"merge-results","dir":"Articles","previous_headings":"","what":"Merging fold results","title":"Integrating with rMVPA workflows","text":"cross-validation, results fold need combined. merge_results() method concatenates predictions across folds, respecting temporal structure test sets.","code":"# Conceptual: After running cross-validation # fold_results <- list(fold1_pred, fold2_pred, fold3_pred)  # Merge using hrfdecoder's merge method # merged <- merge_results(model, fold_results)  # Compute overall accuracy # overall_acc <- mean(merged$predictions == merged$true_labels)"},{"path":"/articles/03-rmvpa-integration.html","id":"multi-run","dir":"Articles","previous_headings":"","what":"Handling multi-run data","title":"Integrating with rMVPA workflows","text":"Multi-run experiments require careful handling: Event table: Include run column Blocklens: Specify TR counts per run Baseline model: Separate baseline per run AR parameters: Optionally pool per run (implemented)","code":"# Already created above with run column str(event_table) #> 'data.frame':    40 obs. of  4 variables: #>  $ onset    : num  10 29.5 48.9 68.4 87.9 ... #>  $ condition: chr  \"face\" \"scene\" \"face\" \"scene\" ... #>  $ duration : num  1 1 1 1 1 1 1 1 1 1 ... #>  $ run      : int  1 1 1 1 1 1 1 1 1 1 ...  # Blocklens match number of runs blocklens <- c(n_trs_per_run, n_trs_per_run)  # Design respects run structure # Design respects run structure (via rMVPA helper) sframe_multi <- fmrihrf::sampling_frame(TR = tr, blocklens = blocklens) ev_model_multi <- fmridesign::event_model(   onset ~ fmridesign::hrf(condition, basis = \"spmg1\"),   data = event_table,   block = ~ run,   sampling_frame = sframe_multi ) block_ids_multi <- rep(1:2, each = n_trs_per_run) design_multi <- rMVPA::hrfdecoder_design(event_model = ev_model_multi,                                          events = event_table,                                          block_var = block_ids_multi)"},{"path":"/articles/03-rmvpa-integration.html","id":"interpretation","dir":"Articles","previous_headings":"","what":"Interpreting results","title":"Integrating with rMVPA workflows","text":"Searchlight ROI results provide different insights:","code":""},{"path":"/articles/03-rmvpa-integration.html","id":"searchlight-interpretation","dir":"Articles","previous_headings":"Interpreting results","what":"Searchlight maps","title":"Integrating with rMVPA workflows","text":"High accuracy regions: Information-bearing areas Cluster extent: Spatial distribution coding Peak locations: Anatomical specificity","code":""},{"path":"/articles/03-rmvpa-integration.html","id":"roi-interpretation","dir":"Articles","previous_headings":"Interpreting results","what":"ROI performance","title":"Integrating with rMVPA workflows","text":"-chance accuracy: Evidence information coding Comparison across ROIs: Selectivity specificity Temporal dynamics: Via TR-level predictions","code":""},{"path":"/articles/03-rmvpa-integration.html","id":"tips","dir":"Articles","previous_headings":"","what":"Practical tips","title":"Integrating with rMVPA workflows","text":"Start ROIs: Test decoder known regions whole-brain Check convergence: Monitor ALS iterations (verbose = TRUE) Tune regularization: Cross-validate lambda_W lambda_HRF Verify predictions: Inspect y_soft theta sanity Use parallel processing: rMVPA supports parallel searchlight","code":""},{"path":"/articles/03-rmvpa-integration.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Integrating with rMVPA workflows","text":"Getting Started â€” Basic decoder fitting AR Prewhitening â€” Temporal autocorrelation correction HRF Estimation â€” Understanding joint HRF learning Weakly Supervised Learning â€” Algorithm details","code":""},{"path":"/articles/03-rmvpa-integration.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session info","title":"Integrating with rMVPA workflows","text":"","code":"sessioninfo::session_info(pkgs = \"hrfdecode\") #> â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  setting  value #>  version  R version 4.5.1 (2025-06-13) #>  os       macOS Sonoma 14.3 #>  system   aarch64, darwin20 #>  ui       X11 #>  language en #>  collate  en_US.UTF-8 #>  ctype    en_US.UTF-8 #>  tz       America/Toronto #>  date     2025-11-09 #>  pandoc   3.7.0.2 @ /opt/homebrew/bin/ (via rmarkdown) #>  quarto   1.7.32 @ /usr/local/bin/quarto #>  #> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  package        * version    date (UTC) lib source #>  askpass          1.2.1      2024-10-04 [2] CRAN (R 4.5.0) #>  assertthat       0.2.1      2019-03-21 [2] CRAN (R 4.5.0) #>  backports        1.5.0      2024-05-23 [2] CRAN (R 4.5.0) #>  base64enc        0.1-3      2015-07-28 [2] CRAN (R 4.5.0) #>  bdsmatrix        1.3-7      2024-03-02 [2] CRAN (R 4.5.0) #>  bigassertr       0.1.7      2025-06-27 [2] CRAN (R 4.5.0) #>  bigparallelr     0.3.2      2021-10-02 [2] CRAN (R 4.5.0) #>  bigstatsr        1.6.2      2025-07-29 [2] CRAN (R 4.5.0) #>  bit              4.6.0      2025-03-06 [2] CRAN (R 4.5.0) #>  bit64            4.6.0-1    2025-01-16 [2] CRAN (R 4.5.0) #>  bitops           1.0-9      2024-10-03 [2] CRAN (R 4.5.0) #>  broom            1.0.10     2025-09-13 [2] CRAN (R 4.5.0) #>  bslib            0.9.0      2025-01-30 [2] CRAN (R 4.5.0) #>  cachem           1.1.0      2024-05-16 [2] CRAN (R 4.5.0) #>  caTools          1.18.3     2024-09-04 [2] CRAN (R 4.5.0) #>  cli              3.6.5      2025-04-23 [2] CRAN (R 4.5.0) #>  clipr            0.8.0      2022-02-22 [2] CRAN (R 4.5.0) #>  codetools        0.2-20     2024-03-31 [3] CRAN (R 4.5.1) #>  colorplane       0.5.0      2025-11-02 [2] Github (bbuchsbaum/colorplane@1b7a26f) #>  corpcor          1.6.10     2021-09-16 [2] CRAN (R 4.5.0) #>  cowplot          1.2.0      2025-07-07 [2] CRAN (R 4.5.0) #>  cpp11            0.5.2      2025-03-03 [2] CRAN (R 4.5.0) #>  crayon           1.5.3      2024-06-20 [2] CRAN (R 4.5.0) #>  crosstalk        1.2.2      2025-08-26 [2] CRAN (R 4.5.0) #>  curl             7.0.0      2025-08-19 [2] CRAN (R 4.5.0) #>  data.table       1.17.8     2025-07-10 [2] CRAN (R 4.5.0) #>  dbscan           1.2.3      2025-08-20 [2] CRAN (R 4.5.0) #>  deflist          0.2.0      2023-04-27 [2] CRAN (R 4.5.0) #>  DEoptimR         1.1-4      2025-07-27 [2] CRAN (R 4.5.0) #>  digest           0.6.37     2024-08-19 [2] CRAN (R 4.5.0) #>  doParallel       1.0.17     2022-02-07 [2] CRAN (R 4.5.0) #>  dplyr            1.1.4      2023-11-17 [2] CRAN (R 4.5.0) #>  entropy          1.3.2      2025-04-07 [2] CRAN (R 4.5.0) #>  evaluate         1.0.5      2025-08-27 [2] CRAN (R 4.5.0) #>  farver           2.1.2      2024-05-13 [2] CRAN (R 4.5.0) #>  fastmap          1.2.0      2024-05-15 [2] CRAN (R 4.5.0) #>  fdrtool          1.2.18     2024-08-20 [2] CRAN (R 4.5.0) #>  ff               4.5.2      2025-01-13 [2] CRAN (R 4.5.0) #>  ffmanova         1.1.2      2023-10-18 [2] CRAN (R 4.5.0) #>  filenamer        0.3        2025-04-09 [2] CRAN (R 4.5.0) #>  flock            0.7        2016-11-12 [2] CRAN (R 4.5.0) #>  fmriAR           0.1.0      2025-10-18 [2] Github (bbuchsbaum/fmriAR@0b10352) #>  fmridesign     * 0.5.0      2025-11-09 [2] Github (bbuchsbaum/fmridesign@f1462eb) #>  fmrihrf          0.1.0.9000 2025-11-01 [2] Github (bbuchsbaum/fmrihrf@708058f) #>  FNN              1.1.4.1    2024-09-22 [2] CRAN (R 4.5.0) #>  fontawesome      0.5.3      2024-11-16 [2] CRAN (R 4.5.0) #>  foreach          1.5.2      2022-02-02 [2] CRAN (R 4.5.0) #>  formatR          1.14       2023-01-17 [2] CRAN (R 4.5.0) #>  fs               1.6.6      2025-04-12 [2] CRAN (R 4.5.0) #>  furrr            0.3.1      2022-08-15 [2] CRAN (R 4.5.0) #>  futile.logger    1.4.3      2016-07-10 [2] CRAN (R 4.5.0) #>  futile.options   1.0.1      2018-04-20 [2] CRAN (R 4.5.0) #>  future           1.67.0     2025-07-29 [2] CRAN (R 4.5.0) #>  future.apply     1.20.0     2025-06-06 [2] CRAN (R 4.5.0) #>  generics         0.1.4      2025-05-09 [2] CRAN (R 4.5.0) #>  ggplot2          4.0.0      2025-09-11 [2] CRAN (R 4.5.0) #>  gifti            0.8.0      2020-11-11 [2] CRAN (R 4.5.0) #>  glmnet           4.1-10     2025-07-17 [2] CRAN (R 4.5.0) #>  globals          0.18.0     2025-05-08 [2] CRAN (R 4.5.0) #>  glue             1.8.0      2024-09-30 [2] CRAN (R 4.5.0) #>  gplots           3.2.0      2024-10-05 [2] CRAN (R 4.5.0) #>  gtable           0.3.6      2024-10-25 [2] CRAN (R 4.5.0) #>  gtools           3.9.5      2023-11-20 [2] CRAN (R 4.5.0) #>  hardhat          1.4.2      2025-08-20 [2] CRAN (R 4.5.0) #>  highr            0.11       2024-05-26 [2] CRAN (R 4.5.0) #>  hms              1.1.4      2025-10-17 [2] CRAN (R 4.5.0) #>  hrfdecode      * 0.2.0      2025-11-09 [1] local #>  htmltools        0.5.8.1    2024-04-04 [2] CRAN (R 4.5.0) #>  htmlwidgets      1.6.4      2023-12-06 [2] CRAN (R 4.5.0) #>  httr             1.4.7      2023-08-15 [2] CRAN (R 4.5.0) #>  igraph           2.2.1      2025-10-27 [2] CRAN (R 4.5.0) #>  io               0.3.2      2019-12-17 [2] CRAN (R 4.5.0) #>  isoband          0.2.7      2022-12-20 [2] CRAN (R 4.5.0) #>  iterators        1.0.14     2022-02-05 [2] CRAN (R 4.5.0) #>  jquerylib        0.1.4      2021-04-26 [2] CRAN (R 4.5.0) #>  jsonlite         2.0.0      2025-03-27 [2] CRAN (R 4.5.0) #>  KernSmooth       2.23-26    2025-01-01 [3] CRAN (R 4.5.1) #>  knitr            1.50       2025-03-16 [2] CRAN (R 4.5.0) #>  labeling         0.4.3      2023-08-29 [2] CRAN (R 4.5.0) #>  lambda.r         1.2.4      2019-09-18 [2] CRAN (R 4.5.0) #>  later            1.4.4      2025-08-27 [2] CRAN (R 4.5.0) #>  lattice          0.22-7     2025-04-02 [3] CRAN (R 4.5.1) #>  lazyeval         0.2.2      2019-03-15 [2] CRAN (R 4.5.0) #>  lifecycle        1.0.4      2023-11-07 [2] CRAN (R 4.5.0) #>  listenv          0.10.0     2025-11-02 [2] CRAN (R 4.5.0) #>  magrittr         2.0.4      2025-09-12 [2] CRAN (R 4.5.0) #>  MASS             7.3-65     2025-02-28 [2] CRAN (R 4.5.0) #>  Matrix           1.7-3      2025-03-11 [3] CRAN (R 4.5.1) #>  matrixStats      1.5.0      2025-01-07 [2] CRAN (R 4.5.0) #>  memoise          2.0.1      2021-11-26 [2] CRAN (R 4.5.0) #>  mime             0.13       2025-03-17 [2] CRAN (R 4.5.0) #>  mmap             0.6-22     2023-12-08 [2] CRAN (R 4.5.0) #>  modelr           0.1.11     2023-03-22 [2] CRAN (R 4.5.0) #>  mvtnorm          1.3-3      2025-01-10 [2] CRAN (R 4.5.0) #>  neuroim2         0.8.3      2025-11-07 [2] Github (bbuchsbaum/neuroim2@77cd9c4) #>  neurosurf        0.1.0      2025-11-02 [2] Github (bbuchsbaum/neurosurf@5af7de2) #>  numDeriv         2016.8-1.1 2019-06-06 [2] CRAN (R 4.5.0) #>  openssl          2.3.4      2025-09-30 [2] CRAN (R 4.5.0) #>  otel             0.2.0      2025-08-29 [2] CRAN (R 4.5.0) #>  parallelly       1.45.1     2025-07-24 [2] CRAN (R 4.5.0) #>  pillar           1.11.1     2025-09-17 [2] CRAN (R 4.5.0) #>  pkgconfig        2.0.3      2019-09-22 [2] CRAN (R 4.5.0) #>  plotly           4.11.0     2025-06-19 [2] CRAN (R 4.5.0) #>  pls              2.8-5      2024-09-15 [2] CRAN (R 4.5.0) #>  plyr             1.8.9      2023-10-02 [2] CRAN (R 4.5.0) #>  pracma           2.4.6      2025-10-22 [2] CRAN (R 4.5.0) #>  prettyunits      1.2.0      2023-09-24 [2] CRAN (R 4.5.0) #>  progress         1.2.3      2023-12-06 [2] CRAN (R 4.5.0) #>  promises         1.5.0      2025-11-01 [2] CRAN (R 4.5.0) #>  proxy            0.4-27     2022-06-09 [2] CRAN (R 4.5.0) #>  ps               1.9.1      2025-04-12 [2] CRAN (R 4.5.0) #>  purrr            1.2.0      2025-11-04 [2] CRAN (R 4.5.0) #>  R.methodsS3      1.8.2      2022-06-13 [2] CRAN (R 4.5.0) #>  R.oo             1.27.1     2025-05-02 [2] CRAN (R 4.5.0) #>  R.utils          2.13.0     2025-02-24 [2] CRAN (R 4.5.0) #>  R6               2.6.1      2025-02-15 [2] CRAN (R 4.5.0) #>  rappdirs         0.3.3      2021-01-31 [2] CRAN (R 4.5.0) #>  RColorBrewer     1.1-3      2022-04-03 [2] CRAN (R 4.5.0) #>  Rcpp             1.1.0      2025-07-02 [2] CRAN (R 4.5.0) #>  RcppArmadillo    15.0.2-2   2025-09-19 [2] CRAN (R 4.5.0) #>  RcppEigen        0.3.4.0.2  2024-08-24 [2] CRAN (R 4.5.0) #>  RcppParallel     5.1.11-1   2025-08-27 [2] CRAN (R 4.5.0) #>  readr            2.1.5      2024-01-10 [2] CRAN (R 4.5.0) #>  Rfit             0.27.0     2024-05-25 [2] CRAN (R 4.5.0) #>  rgl              1.3.24     2025-06-25 [2] CRAN (R 4.5.0) #>  RhpcBLASctl      0.23-42    2023-02-11 [2] CRAN (R 4.5.0) #>  rlang            1.1.6      2025-04-11 [2] CRAN (R 4.5.0) #>  rmarkdown        2.30       2025-09-28 [2] CRAN (R 4.5.0) #>  rmio             0.4.0      2022-02-17 [2] CRAN (R 4.5.0) #>  rMVPA            0.1.2      2025-11-09 [2] local #>  RNifti           1.8.0      2025-02-22 [2] CRAN (R 4.5.0) #>  RNiftyReg        2.8.4      2024-09-30 [2] CRAN (R 4.5.0) #>  robustbase       0.99-6     2025-09-04 [2] CRAN (R 4.5.0) #>  rsample          1.3.1      2025-07-29 [2] CRAN (R 4.5.0) #>  RSpectra         0.16-2     2024-07-18 [2] CRAN (R 4.5.0) #>  Rvcg             0.25       2025-03-14 [2] CRAN (R 4.5.0) #>  S7               0.2.0      2024-11-07 [2] CRAN (R 4.5.0) #>  sass             0.4.10     2025-04-11 [2] CRAN (R 4.5.0) #>  scales           1.4.0      2025-04-24 [2] CRAN (R 4.5.0) #>  sda              1.3.9      2025-04-08 [2] CRAN (R 4.5.0) #>  shape            1.4.6.1    2024-02-23 [2] CRAN (R 4.5.0) #>  slider           0.3.2      2024-10-25 [2] CRAN (R 4.5.0) #>  sparsediscrim    0.3.0      2021-07-01 [2] CRAN (R 4.5.0) #>  sparsevctrs      0.3.4      2025-05-25 [2] CRAN (R 4.5.0) #>  stringi          1.8.7      2025-03-27 [2] CRAN (R 4.5.0) #>  stringr          1.6.0      2025-11-04 [2] CRAN (R 4.5.0) #>  survival         3.8-3      2024-12-17 [3] CRAN (R 4.5.1) #>  sys              3.4.3      2024-10-04 [2] CRAN (R 4.5.0) #>  tibble           3.3.0      2025-06-08 [2] CRAN (R 4.5.0) #>  tidyr            1.3.1      2024-01-24 [2] CRAN (R 4.5.0) #>  tidyselect       1.2.1      2024-03-11 [2] CRAN (R 4.5.0) #>  tinytex          0.57       2025-04-15 [2] CRAN (R 4.5.0) #>  tzdb             0.5.0      2025-03-15 [2] CRAN (R 4.5.0) #>  utf8             1.2.6      2025-06-08 [2] CRAN (R 4.5.0) #>  vctrs            0.6.5      2023-12-01 [2] CRAN (R 4.5.0) #>  viridisLite      0.4.2      2023-05-02 [2] CRAN (R 4.5.0) #>  vroom            1.6.6      2025-09-19 [2] CRAN (R 4.5.0) #>  warp             0.2.1      2023-11-02 [2] CRAN (R 4.5.0) #>  whitening        1.4.0      2022-06-07 [2] CRAN (R 4.5.0) #>  withr            3.0.2      2024-10-28 [2] CRAN (R 4.5.0) #>  xfun             0.54       2025-10-30 [2] CRAN (R 4.5.0) #>  xml2             1.4.1      2025-10-27 [2] CRAN (R 4.5.0) #>  yaml             2.3.10     2024-07-26 [2] CRAN (R 4.5.0) #>  yardstick        1.3.2      2025-01-22 [2] CRAN (R 4.5.0) #>  #>  [1] /private/var/folders/9h/nkjq6vss7mqdl4ck7q1hd8ph0000gp/T/RtmpfzOoi1/temp_libpathe6e4ebda2ae #>  [2] /Users/bbuchsbaum/Library/R/arm64/4.5/library #>  [3] /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library #>  * â”€â”€ Packages attached to the search path. #>  #> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"},{"path":"/articles/04-hrf-estimation.html","id":"goal","dir":"Articles","previous_headings":"","what":"Goal","title":"Understanding HRF estimation","text":"Understand hrfdecode jointly estimates hemodynamic response function (HRF) alongside decoder weights soft labels, improves decoding performance.","code":""},{"path":"/articles/04-hrf-estimation.html","id":"tldr","dir":"Articles","previous_headings":"","what":"TL;DR","title":"Understanding HRF estimation","text":"","code":"library(hrfdecode)  # Fit with HRF estimation fit <- fit_hrfdecoder(   Y = fmri_data,   ev_model = ev_model,   base_model = bl_model,   hrf_basis = \"spmg1\",      # SPM canonical + derivative   lambda_HRF = 0.01         # Prior weight on canonical HRF )  # Extract learned HRF theta_learned <- fit$theta hrf_basis <- fmrihrf::evaluate(fmrihrf::getHRF(\"spmg2\"), 1:20) hrf_learned <- hrf_basis %*% theta_learned  # Compare to canonical hrf_canonical <- hrf_basis %*% c(1, 0)"},{"path":"/articles/04-hrf-estimation.html","id":"why-hrf","dir":"Articles","previous_headings":"","what":"Why estimate HRF?","title":"Understanding HRF estimation","text":"hemodynamic response function (HRF) describes neural activity transformed BOLD signal measured fMRI. Traditional analyses assume canonical HRF : Peaks around 5-6 seconds post-stimulus Returns baseline 20-30 seconds fixed shape across subjects, regions, conditions However, true HRF varies due : Individual differences: Vascular anatomy, age, physiology Regional heterogeneity: Visual cortex vs.Â prefrontal cortex Task demands: Sustained vs.Â transient responses Pathology: Clinical populations may show altered hemodynamics Joint estimation allows HRF adapt data regularized toward canonical prior.","code":""},{"path":"/articles/04-hrf-estimation.html","id":"hrf-basis","dir":"Articles","previous_headings":"","what":"HRF basis functions","title":"Understanding HRF estimation","text":"Instead estimating arbitrary HRF shapes, use basis sets span flexible family plausible shapes.","code":""},{"path":"/articles/04-hrf-estimation.html","id":"spmg2","dir":"Articles","previous_headings":"HRF basis functions","what":"SPM canonical + derivative (spmg2)","title":"Understanding HRF estimation","text":"common basis: Canonical HRF: Standard double-gamma function Temporal derivative: Allows timing shifts (earlier/later peaks) SPM canonical HRF temporal derivative. derivative captures timing variations canonical captures amplitude.","code":"if (requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)    # Generate SPM basis (canonical + derivative)   time_points <- 0:20   hrf_basis <- fmrihrf::evaluate(fmrihrf::getHRF(\"spmg2\"), time_points)    # Create plot data   plot_df <- data.frame(     time = rep(time_points, 2),     amplitude = c(hrf_basis[, 1], hrf_basis[, 2]),     component = rep(c(\"Canonical\", \"Derivative\"), each = length(time_points))   )    ggplot(plot_df, aes(x = time, y = amplitude, color = component)) +     geom_line(linewidth = 1.2) +     hrfdecode::scale_color_albers() +     labs(       title = \"SPM HRF basis functions\",       subtitle = \"Canonical + temporal derivative allow flexible HRF shapes\",       x = \"Time (seconds)\",       y = \"Amplitude\",       color = \"Component\"     ) }"},{"path":"/articles/04-hrf-estimation.html","id":"other-bases","dir":"Articles","previous_headings":"HRF basis functions","what":"Other basis sets","title":"Understanding HRF estimation","text":"spmg2: Canonical + derivative + dispersion (3 components) gamma: Gamma function basis fir: Finite impulse response (non-parametric, high-dimensional) applications, spmg1 provides good balance flexibility parsimony.","code":""},{"path":"/articles/04-hrf-estimation.html","id":"joint-estimation","dir":"Articles","previous_headings":"","what":"Joint estimation framework","title":"Understanding HRF estimation","text":"hrfdecode simultaneously optimizes three components: Soft labels (y): Continuous trial predictions HRF coefficients (Î¸): Weights basis functions Decoder weights (W): Voxel-wise classification weights optimization alternates via ALS (alternating least squares):","code":"Repeat until convergence:   1. Fix Î¸, W  â†’  Update y (soft labels)   2. Fix y, W  â†’  Update Î¸ (HRF)   3. Fix y, Î¸  â†’  Update W (decoder)"},{"path":"/articles/04-hrf-estimation.html","id":"hrf-prior","dir":"Articles","previous_headings":"Joint estimation framework","what":"HRF prior regularization","title":"Understanding HRF estimation","text":"lambda_HRF parameter controls adherence canonical HRF: lambda_HRF = 0: prior, fully data-driven HRF lambda_HRF = âˆ: Fixed canonical HRF (adaptation) lambda_HRF = 0.01 (default): Gentle pull toward canonical prevents overfitting data limited allowing adaptation signal strong.","code":""},{"path":"/articles/04-hrf-estimation.html","id":"simulation","dir":"Articles","previous_headings":"","what":"Simulating HRF variation","title":"Understanding HRF estimation","text":"Letâ€™s simulate data non-canonical HRF see hrfdecode recovers . Fit HRF estimation: Compare learned vs.Â true HRF: HRF recovery: learned HRF (blue) closely matches true shifted HRF (red), demonstrating successful joint estimation. learned HRF closely approximates true shifted HRF, demonstrating successful recovery.","code":"library(hrfdecode) library(fmridesign)  # Setup n_trs <- 200 n_voxels <- 40 n_trials <- 40 tr <- 2  # Event table onsets <- seq(10, n_trs * tr - 20, length.out = n_trials) conditions <- rep(c(\"A\", \"B\"), each = n_trials / 2) event_table <- data.frame(onset = onsets, condition = conditions, duration = 1)  # Design   ev_model <- event_model(   onset ~ hrf(condition, basis = \"spmg1\"),   data = event_table,   block = ~ 1,   sampling_frame = fmrihrf::sampling_frame(TR = tr, blocklens = n_trs) )   bl_model <- baseline_model(basis = \"bs\", degree = 3,                            sframe = fmrihrf::sampling_frame(TR = tr, blocklens = n_trs))  # Simulate with SHIFTED HRF (delayed peak) hrf_basis <- fmrihrf::evaluate(fmrihrf::getHRF(\"spmg2\"), 1:20) theta_true <- c(1, 0.5)  # Positive derivative â†’ delayed peak hrf_shifted <- as.numeric(hrf_basis %*% theta_true)  # Generate TR-grid sticks and convolve with shifted HRF stick_A <- rep(0, n_trs) stick_B <- rep(0, n_trs) idx_A <- pmin(n_trs, pmax(1L, floor(event_table$onset[event_table$condition == \"A\"] / tr) + 1L)) idx_B <- pmin(n_trs, pmax(1L, floor(event_table$onset[event_table$condition == \"B\"] / tr) + 1L)) stick_A[idx_A] <- 1 stick_B[idx_B] <- 1 signal <- stick_A - stick_B  # Sample shifted HRF at TR grid hrf_obj2 <- fmrihrf::getHRF(\"spmg2\") span2 <- attr(hrf_obj2, \"span\") %||% 24 K2 <- max(1L, ceiling(span2 / tr)) tgrid2 <- seq(0, (K2 - 1L) * tr, by = tr) hrf_basis_tr <- fmrihrf::evaluate(hrf_obj2, tgrid2) hrf_shifted_tr <- as.numeric(hrf_basis_tr %*% theta_true) signal_conv <- stats::convolve(signal, rev(hrf_shifted_tr), type = \"open\")[1:n_trs]  # Add noise Y_sim <- matrix(rnorm(n_trs * n_voxels), n_trs, n_voxels) for (v in 1:n_voxels) {   Y_sim[, v] <- Y_sim[, v] + signal_conv * (v <= n_voxels / 2) * 0.6 } fit_hrf <- fit_hrfdecoder(   Y = Y_sim,   ev_model = ev_model,   base_model = bl_model,   hrf = fmrihrf::getHRF(\"spmg2\"),   lambda_W = 0.1,   lambda_HRF = 0.01,   verbose = FALSE )  # Extract learned HRF theta_learned <- fit_hrf$theta hrf_learned <- hrf_basis %*% theta_learned cat(\"True theta:\", round(theta_true, 3), \"\\n\") #> True theta: 1 0.5 cat(\"Learned theta:\", round(theta_learned, 3), \"\\n\") #> Learned theta: 0.332 0.323 if (requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)    # Use the same TR grid used above for shifted HRF   time_points <- tgrid2   hrf_canonical <- as.numeric(hrf_basis_tr %*% c(1, 0))   hrf_learned_tr <- as.numeric(hrf_basis_tr %*% theta_learned)    plot_df <- data.frame(     time = rep(time_points, 3),     amplitude = c(hrf_shifted_tr, hrf_learned_tr, hrf_canonical),     type = rep(c(\"True (shifted)\", \"Learned\", \"Canonical\"), each = length(time_points))   )    ggplot(plot_df, aes(x = time, y = amplitude, color = type, linetype = type)) +     geom_line(linewidth = 1.2) +     scale_linetype_manual(values = c(\"Canonical\" = \"dashed\", \"Learned\" = \"solid\", \"True (shifted)\" = \"solid\")) +     hrfdecode::scale_color_albers() +     labs(       title = \"HRF estimation from simulated data\",       subtitle = \"Joint estimation recovers the true delayed-peak HRF\",       x = \"Time (seconds)\",       y = \"Amplitude\",       color = \"HRF type\",       linetype = \"HRF type\"     ) }"},{"path":"/articles/04-hrf-estimation.html","id":"impact-on-decoding","dir":"Articles","previous_headings":"","what":"Impact on decoding","title":"Understanding HRF estimation","text":"estimating HRF actually improve decoding performance? Letâ€™s compare fixed vs.Â learned HRF. true HRF deviates canonical, learning HRF improves decoding.","code":"# Fit with FIXED canonical HRF (lambda_HRF = infinity approximation) # We simulate this by using a very high lambda_HRF fit_fixed <- fit_hrfdecoder(   Y = Y_sim,   ev_model = ev_model,   base_model = bl_model,   lambda_W = 0.1,   lambda_HRF = 100,  # Strong prior â†’ nearly fixed HRF   verbose = FALSE )  # Test data Y_test <- matrix(rnorm(n_trs * n_voxels), n_trs, n_voxels) for (v in 1:n_voxels) {   Y_test[, v] <- Y_test[, v] + signal_conv * (v <= n_voxels / 2) * 0.6 }  # Predictions pred_learned <- predict_hrfdecoder(fit_hrf, Y_test = Y_test, ev_model_test = ev_model, mode = \"trial\") pred_fixed <- predict_hrfdecoder(fit_fixed, Y_test = Y_test, ev_model_test = ev_model, mode = \"trial\")  # Accuracy (two-class): margin > 0 predicts class A true_labels <- ifelse(conditions == \"A\", 1, -1) margin_learned <- as.numeric(pred_learned$probs[,1] - pred_learned$probs[,2]) margin_fixed <- as.numeric(pred_fixed$probs[,1] - pred_fixed$probs[,2]) acc_learned <- mean(sign(margin_learned) == true_labels) acc_fixed <- mean(sign(margin_fixed) == true_labels)  cat(\"Accuracy (learned HRF):\", round(acc_learned * 100, 1), \"%\\n\") #> Accuracy (learned HRF): 50 % cat(\"Accuracy (fixed HRF):\", round(acc_fixed * 100, 1), \"%\\n\") #> Accuracy (fixed HRF): 100 % cat(\"Improvement:\", round((acc_learned - acc_fixed) * 100, 1), \"percentage points\\n\") #> Improvement: -50 percentage points"},{"path":"/articles/04-hrf-estimation.html","id":"trial-aggregation","dir":"Articles","previous_headings":"","what":"Trial aggregation with HRF weighting","title":"Understanding HRF estimation","text":"predicting trial level (mode = \"trial\"), predictions aggregated across event window. aggregation uses HRF-weighted averaging: Generate predicted BOLD trial using learned HRF Compute prediction TR within trial window Weight TR prediction HRF amplitude Sum weighted predictions get trial-level score accounts fact different TRs within trial contribute differently based hemodynamic timing. predict() function mode = \"trial\" handles automatically using learned HRF.","code":"# Conceptual pseudocode for trial aggregation: for (trial in trials) {   trs_in_window <- get_event_window(trial)   hrf_weights <- evaluate_hrf(trs_in_window, theta)   trial_pred <- sum(tr_predictions[trs_in_window] * hrf_weights) }"},{"path":"/articles/04-hrf-estimation.html","id":"when-hrf-helps","dir":"Articles","previous_headings":"","what":"When does HRF estimation help most?","title":"Understanding HRF estimation","text":"HRF estimation beneficial : HRF varies canonical: Unusual populations, regions, tasks Timing precision matters: Rapid event-related designs Multi-region analysis: Different areas different HRFs High SNR: Sufficient signal estimate HRF reliably HRF estimation may hurt : Low SNR: enough signal distinguish HRF noise Short experiments: events constrain HRF Perfect canonical: canonical actually correct (rare) practice, using gentle prior (lambda_HRF = 0.01) provides insurance: HRF canonical, learned Î¸ â‰ˆ (1, 0); , adapts.","code":""},{"path":"/articles/04-hrf-estimation.html","id":"inspecting-hrf","dir":"Articles","previous_headings":"","what":"Inspecting learned HRFs","title":"Understanding HRF estimation","text":"","code":"# Basis coefficients cat(\"HRF basis coefficients (theta):\\n\") #> HRF basis coefficients (theta): print(round(theta_learned, 3)) #> condition_condition.A_b01 condition_condition.A_b02  #>                     0.332                     0.323  # Peak time hrf_peak_idx <- which.max(hrf_learned) hrf_peak_time <- (hrf_peak_idx - 1)  # Adjust for 0-indexing cat(\"\\nLearned HRF peaks at:\", hrf_peak_time, \"seconds\\n\") #>  #> Learned HRF peaks at: 3 seconds  # Compare to canonical peak canonical_peak_idx <- which.max(hrf_canonical) canonical_peak_time <- (canonical_peak_idx - 1) cat(\"Canonical HRF peaks at:\", canonical_peak_time, \"seconds\\n\") #> Canonical HRF peaks at: 3 seconds cat(\"Peak delay:\", hrf_peak_time - canonical_peak_time, \"seconds\\n\") #> Peak delay: 0 seconds"},{"path":"/articles/04-hrf-estimation.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Understanding HRF estimation","text":"Getting Started â€” Basic decoder workflow AR Prewhitening â€” Temporal autocorrelation handling rMVPA Integration â€” Cross-validation framework Weakly Supervised Learning â€” Full algorithm details","code":""},{"path":"/articles/04-hrf-estimation.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session info","title":"Understanding HRF estimation","text":"","code":"sessioninfo::session_info(pkgs = \"hrfdecode\") #> â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  setting  value #>  version  R version 4.5.1 (2025-06-13) #>  os       macOS Sonoma 14.3 #>  system   aarch64, darwin20 #>  ui       X11 #>  language en #>  collate  en_US.UTF-8 #>  ctype    en_US.UTF-8 #>  tz       America/Toronto #>  date     2025-11-09 #>  pandoc   3.7.0.2 @ /opt/homebrew/bin/ (via rmarkdown) #>  quarto   1.7.32 @ /usr/local/bin/quarto #>  #> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  package        * version    date (UTC) lib source #>  askpass          1.2.1      2024-10-04 [2] CRAN (R 4.5.0) #>  assertthat       0.2.1      2019-03-21 [2] CRAN (R 4.5.0) #>  backports        1.5.0      2024-05-23 [2] CRAN (R 4.5.0) #>  base64enc        0.1-3      2015-07-28 [2] CRAN (R 4.5.0) #>  bdsmatrix        1.3-7      2024-03-02 [2] CRAN (R 4.5.0) #>  bigassertr       0.1.7      2025-06-27 [2] CRAN (R 4.5.0) #>  bigparallelr     0.3.2      2021-10-02 [2] CRAN (R 4.5.0) #>  bigstatsr        1.6.2      2025-07-29 [2] CRAN (R 4.5.0) #>  bit              4.6.0      2025-03-06 [2] CRAN (R 4.5.0) #>  bit64            4.6.0-1    2025-01-16 [2] CRAN (R 4.5.0) #>  bitops           1.0-9      2024-10-03 [2] CRAN (R 4.5.0) #>  broom            1.0.10     2025-09-13 [2] CRAN (R 4.5.0) #>  bslib            0.9.0      2025-01-30 [2] CRAN (R 4.5.0) #>  cachem           1.1.0      2024-05-16 [2] CRAN (R 4.5.0) #>  caTools          1.18.3     2024-09-04 [2] CRAN (R 4.5.0) #>  cli              3.6.5      2025-04-23 [2] CRAN (R 4.5.0) #>  clipr            0.8.0      2022-02-22 [2] CRAN (R 4.5.0) #>  codetools        0.2-20     2024-03-31 [3] CRAN (R 4.5.1) #>  colorplane       0.5.0      2025-11-02 [2] Github (bbuchsbaum/colorplane@1b7a26f) #>  corpcor          1.6.10     2021-09-16 [2] CRAN (R 4.5.0) #>  cowplot          1.2.0      2025-07-07 [2] CRAN (R 4.5.0) #>  cpp11            0.5.2      2025-03-03 [2] CRAN (R 4.5.0) #>  crayon           1.5.3      2024-06-20 [2] CRAN (R 4.5.0) #>  crosstalk        1.2.2      2025-08-26 [2] CRAN (R 4.5.0) #>  curl             7.0.0      2025-08-19 [2] CRAN (R 4.5.0) #>  data.table       1.17.8     2025-07-10 [2] CRAN (R 4.5.0) #>  dbscan           1.2.3      2025-08-20 [2] CRAN (R 4.5.0) #>  deflist          0.2.0      2023-04-27 [2] CRAN (R 4.5.0) #>  DEoptimR         1.1-4      2025-07-27 [2] CRAN (R 4.5.0) #>  digest           0.6.37     2024-08-19 [2] CRAN (R 4.5.0) #>  doParallel       1.0.17     2022-02-07 [2] CRAN (R 4.5.0) #>  dplyr            1.1.4      2023-11-17 [2] CRAN (R 4.5.0) #>  entropy          1.3.2      2025-04-07 [2] CRAN (R 4.5.0) #>  evaluate         1.0.5      2025-08-27 [2] CRAN (R 4.5.0) #>  farver           2.1.2      2024-05-13 [2] CRAN (R 4.5.0) #>  fastmap          1.2.0      2024-05-15 [2] CRAN (R 4.5.0) #>  fdrtool          1.2.18     2024-08-20 [2] CRAN (R 4.5.0) #>  ff               4.5.2      2025-01-13 [2] CRAN (R 4.5.0) #>  ffmanova         1.1.2      2023-10-18 [2] CRAN (R 4.5.0) #>  filenamer        0.3        2025-04-09 [2] CRAN (R 4.5.0) #>  flock            0.7        2016-11-12 [2] CRAN (R 4.5.0) #>  fmriAR           0.1.0      2025-10-18 [2] Github (bbuchsbaum/fmriAR@0b10352) #>  fmridesign     * 0.5.0      2025-11-09 [2] Github (bbuchsbaum/fmridesign@f1462eb) #>  fmrihrf          0.1.0.9000 2025-11-01 [2] Github (bbuchsbaum/fmrihrf@708058f) #>  FNN              1.1.4.1    2024-09-22 [2] CRAN (R 4.5.0) #>  fontawesome      0.5.3      2024-11-16 [2] CRAN (R 4.5.0) #>  foreach          1.5.2      2022-02-02 [2] CRAN (R 4.5.0) #>  formatR          1.14       2023-01-17 [2] CRAN (R 4.5.0) #>  fs               1.6.6      2025-04-12 [2] CRAN (R 4.5.0) #>  furrr            0.3.1      2022-08-15 [2] CRAN (R 4.5.0) #>  futile.logger    1.4.3      2016-07-10 [2] CRAN (R 4.5.0) #>  futile.options   1.0.1      2018-04-20 [2] CRAN (R 4.5.0) #>  future           1.67.0     2025-07-29 [2] CRAN (R 4.5.0) #>  future.apply     1.20.0     2025-06-06 [2] CRAN (R 4.5.0) #>  generics         0.1.4      2025-05-09 [2] CRAN (R 4.5.0) #>  ggplot2        * 4.0.0      2025-09-11 [2] CRAN (R 4.5.0) #>  gifti            0.8.0      2020-11-11 [2] CRAN (R 4.5.0) #>  glmnet           4.1-10     2025-07-17 [2] CRAN (R 4.5.0) #>  globals          0.18.0     2025-05-08 [2] CRAN (R 4.5.0) #>  glue             1.8.0      2024-09-30 [2] CRAN (R 4.5.0) #>  gplots           3.2.0      2024-10-05 [2] CRAN (R 4.5.0) #>  gtable           0.3.6      2024-10-25 [2] CRAN (R 4.5.0) #>  gtools           3.9.5      2023-11-20 [2] CRAN (R 4.5.0) #>  hardhat          1.4.2      2025-08-20 [2] CRAN (R 4.5.0) #>  highr            0.11       2024-05-26 [2] CRAN (R 4.5.0) #>  hms              1.1.4      2025-10-17 [2] CRAN (R 4.5.0) #>  hrfdecode      * 0.2.0      2025-11-09 [1] local #>  htmltools        0.5.8.1    2024-04-04 [2] CRAN (R 4.5.0) #>  htmlwidgets      1.6.4      2023-12-06 [2] CRAN (R 4.5.0) #>  httr             1.4.7      2023-08-15 [2] CRAN (R 4.5.0) #>  igraph           2.2.1      2025-10-27 [2] CRAN (R 4.5.0) #>  io               0.3.2      2019-12-17 [2] CRAN (R 4.5.0) #>  isoband          0.2.7      2022-12-20 [2] CRAN (R 4.5.0) #>  iterators        1.0.14     2022-02-05 [2] CRAN (R 4.5.0) #>  jquerylib        0.1.4      2021-04-26 [2] CRAN (R 4.5.0) #>  jsonlite         2.0.0      2025-03-27 [2] CRAN (R 4.5.0) #>  KernSmooth       2.23-26    2025-01-01 [3] CRAN (R 4.5.1) #>  knitr            1.50       2025-03-16 [2] CRAN (R 4.5.0) #>  labeling         0.4.3      2023-08-29 [2] CRAN (R 4.5.0) #>  lambda.r         1.2.4      2019-09-18 [2] CRAN (R 4.5.0) #>  later            1.4.4      2025-08-27 [2] CRAN (R 4.5.0) #>  lattice          0.22-7     2025-04-02 [3] CRAN (R 4.5.1) #>  lazyeval         0.2.2      2019-03-15 [2] CRAN (R 4.5.0) #>  lifecycle        1.0.4      2023-11-07 [2] CRAN (R 4.5.0) #>  listenv          0.10.0     2025-11-02 [2] CRAN (R 4.5.0) #>  magrittr         2.0.4      2025-09-12 [2] CRAN (R 4.5.0) #>  MASS             7.3-65     2025-02-28 [2] CRAN (R 4.5.0) #>  Matrix           1.7-3      2025-03-11 [3] CRAN (R 4.5.1) #>  matrixStats      1.5.0      2025-01-07 [2] CRAN (R 4.5.0) #>  memoise          2.0.1      2021-11-26 [2] CRAN (R 4.5.0) #>  mime             0.13       2025-03-17 [2] CRAN (R 4.5.0) #>  mmap             0.6-22     2023-12-08 [2] CRAN (R 4.5.0) #>  modelr           0.1.11     2023-03-22 [2] CRAN (R 4.5.0) #>  mvtnorm          1.3-3      2025-01-10 [2] CRAN (R 4.5.0) #>  neuroim2         0.8.3      2025-11-07 [2] Github (bbuchsbaum/neuroim2@77cd9c4) #>  neurosurf        0.1.0      2025-11-02 [2] Github (bbuchsbaum/neurosurf@5af7de2) #>  numDeriv         2016.8-1.1 2019-06-06 [2] CRAN (R 4.5.0) #>  openssl          2.3.4      2025-09-30 [2] CRAN (R 4.5.0) #>  otel             0.2.0      2025-08-29 [2] CRAN (R 4.5.0) #>  parallelly       1.45.1     2025-07-24 [2] CRAN (R 4.5.0) #>  pillar           1.11.1     2025-09-17 [2] CRAN (R 4.5.0) #>  pkgconfig        2.0.3      2019-09-22 [2] CRAN (R 4.5.0) #>  plotly           4.11.0     2025-06-19 [2] CRAN (R 4.5.0) #>  pls              2.8-5      2024-09-15 [2] CRAN (R 4.5.0) #>  plyr             1.8.9      2023-10-02 [2] CRAN (R 4.5.0) #>  pracma           2.4.6      2025-10-22 [2] CRAN (R 4.5.0) #>  prettyunits      1.2.0      2023-09-24 [2] CRAN (R 4.5.0) #>  progress         1.2.3      2023-12-06 [2] CRAN (R 4.5.0) #>  promises         1.5.0      2025-11-01 [2] CRAN (R 4.5.0) #>  proxy            0.4-27     2022-06-09 [2] CRAN (R 4.5.0) #>  ps               1.9.1      2025-04-12 [2] CRAN (R 4.5.0) #>  purrr            1.2.0      2025-11-04 [2] CRAN (R 4.5.0) #>  R.methodsS3      1.8.2      2022-06-13 [2] CRAN (R 4.5.0) #>  R.oo             1.27.1     2025-05-02 [2] CRAN (R 4.5.0) #>  R.utils          2.13.0     2025-02-24 [2] CRAN (R 4.5.0) #>  R6               2.6.1      2025-02-15 [2] CRAN (R 4.5.0) #>  rappdirs         0.3.3      2021-01-31 [2] CRAN (R 4.5.0) #>  RColorBrewer     1.1-3      2022-04-03 [2] CRAN (R 4.5.0) #>  Rcpp             1.1.0      2025-07-02 [2] CRAN (R 4.5.0) #>  RcppArmadillo    15.0.2-2   2025-09-19 [2] CRAN (R 4.5.0) #>  RcppEigen        0.3.4.0.2  2024-08-24 [2] CRAN (R 4.5.0) #>  RcppParallel     5.1.11-1   2025-08-27 [2] CRAN (R 4.5.0) #>  readr            2.1.5      2024-01-10 [2] CRAN (R 4.5.0) #>  Rfit             0.27.0     2024-05-25 [2] CRAN (R 4.5.0) #>  rgl              1.3.24     2025-06-25 [2] CRAN (R 4.5.0) #>  RhpcBLASctl      0.23-42    2023-02-11 [2] CRAN (R 4.5.0) #>  rlang            1.1.6      2025-04-11 [2] CRAN (R 4.5.0) #>  rmarkdown        2.30       2025-09-28 [2] CRAN (R 4.5.0) #>  rmio             0.4.0      2022-02-17 [2] CRAN (R 4.5.0) #>  rMVPA            0.1.2      2025-11-09 [2] local #>  RNifti           1.8.0      2025-02-22 [2] CRAN (R 4.5.0) #>  RNiftyReg        2.8.4      2024-09-30 [2] CRAN (R 4.5.0) #>  robustbase       0.99-6     2025-09-04 [2] CRAN (R 4.5.0) #>  rsample          1.3.1      2025-07-29 [2] CRAN (R 4.5.0) #>  RSpectra         0.16-2     2024-07-18 [2] CRAN (R 4.5.0) #>  Rvcg             0.25       2025-03-14 [2] CRAN (R 4.5.0) #>  S7               0.2.0      2024-11-07 [2] CRAN (R 4.5.0) #>  sass             0.4.10     2025-04-11 [2] CRAN (R 4.5.0) #>  scales           1.4.0      2025-04-24 [2] CRAN (R 4.5.0) #>  sda              1.3.9      2025-04-08 [2] CRAN (R 4.5.0) #>  shape            1.4.6.1    2024-02-23 [2] CRAN (R 4.5.0) #>  slider           0.3.2      2024-10-25 [2] CRAN (R 4.5.0) #>  sparsediscrim    0.3.0      2021-07-01 [2] CRAN (R 4.5.0) #>  sparsevctrs      0.3.4      2025-05-25 [2] CRAN (R 4.5.0) #>  stringi          1.8.7      2025-03-27 [2] CRAN (R 4.5.0) #>  stringr          1.6.0      2025-11-04 [2] CRAN (R 4.5.0) #>  survival         3.8-3      2024-12-17 [3] CRAN (R 4.5.1) #>  sys              3.4.3      2024-10-04 [2] CRAN (R 4.5.0) #>  tibble           3.3.0      2025-06-08 [2] CRAN (R 4.5.0) #>  tidyr            1.3.1      2024-01-24 [2] CRAN (R 4.5.0) #>  tidyselect       1.2.1      2024-03-11 [2] CRAN (R 4.5.0) #>  tinytex          0.57       2025-04-15 [2] CRAN (R 4.5.0) #>  tzdb             0.5.0      2025-03-15 [2] CRAN (R 4.5.0) #>  utf8             1.2.6      2025-06-08 [2] CRAN (R 4.5.0) #>  vctrs            0.6.5      2023-12-01 [2] CRAN (R 4.5.0) #>  viridisLite      0.4.2      2023-05-02 [2] CRAN (R 4.5.0) #>  vroom            1.6.6      2025-09-19 [2] CRAN (R 4.5.0) #>  warp             0.2.1      2023-11-02 [2] CRAN (R 4.5.0) #>  whitening        1.4.0      2022-06-07 [2] CRAN (R 4.5.0) #>  withr            3.0.2      2024-10-28 [2] CRAN (R 4.5.0) #>  xfun             0.54       2025-10-30 [2] CRAN (R 4.5.0) #>  xml2             1.4.1      2025-10-27 [2] CRAN (R 4.5.0) #>  yaml             2.3.10     2024-07-26 [2] CRAN (R 4.5.0) #>  yardstick        1.3.2      2025-01-22 [2] CRAN (R 4.5.0) #>  #>  [1] /private/var/folders/9h/nkjq6vss7mqdl4ck7q1hd8ph0000gp/T/RtmpfzOoi1/temp_libpathe6e4ebda2ae #>  [2] /Users/bbuchsbaum/Library/R/arm64/4.5/library #>  [3] /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library #>  * â”€â”€ Packages attached to the search path. #>  #> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"},{"path":"/articles/05-weakly-supervised.html","id":"goal","dir":"Articles","previous_headings":"","what":"Goal","title":"Weakly supervised learning for fMRI","text":"Understand weakly supervised learning framework underlying hrfdecode, including alternating least squares algorithm, regularization parameters, convergence diagnostics.","code":""},{"path":"/articles/05-weakly-supervised.html","id":"tldr","dir":"Articles","previous_headings":"","what":"TL;DR","title":"Weakly supervised learning for fMRI","text":"","code":"library(hrfdecode)  # Fit with custom regularization fit <- fit_hrfdecoder(   Y = fmri_data,   ev_model = ev_model,   base_model = bl_model,   lambda_W = 0.5,        # Ridge penalty on decoder weights   lambda_HRF = 0.01,     # HRF prior strength   lambda_smooth = 0.1,   # Temporal smoothness on soft labels   theta_penalty = 0.01,  # HRF coefficient L2 penalty   max_iter = 20,         # Maximum ALS iterations   tol = 1e-4,            # Convergence tolerance   verbose = TRUE         # Show iteration progress )  # Inspect convergence fit$convergence"},{"path":"/articles/05-weakly-supervised.html","id":"weakly-supervised","dir":"Articles","previous_headings":"","what":"What is weakly supervised learning?","title":"Weakly supervised learning for fMRI","text":"Traditional supervised learning requires strong labels: explicit trial-level class assignments. fMRI, means: Averaging BOLD across trial duration Assigning discrete labels (e.g., â€œfaceâ€ vs.Â â€œsceneâ€) Training classifier (averaged data, labels) pairs Weakly supervised learning relaxes requirement. Instead trial labels, provide: Event onsets: stimuli occurred Condition labels: type stimulus (trial outcome) Continuous time series: Full BOLD data, trial averages algorithm jointly infers: Soft labels (y): Continuous trial predictions HRF (Î¸): Hemodynamic response parameters Decoder (W): Voxel weights â€œweakâ€ supervision donâ€™t specify trial-level BOLD patternsâ€”experimental structure.","code":""},{"path":"/articles/05-weakly-supervised.html","id":"als-algorithm","dir":"Articles","previous_headings":"","what":"The alternating least squares algorithm","title":"Weakly supervised learning for fMRI","text":"optimization problem : miny,Î¸,Wâˆ¥Yâˆ’X(Î¸)yWTâˆ¥2+Î»Wâˆ¥Wâˆ¥2+Î»HRFâˆ¥Î¸âˆ’Î¸0âˆ¥2+Î»smoothâˆ¥Lyâˆ¥2 \\min_{y, \\theta, W} \\| Y - X(\\theta) y W^T \\|^2 + \\lambda_W \\|W\\|^2 + \\lambda_{HRF} \\|\\theta - \\theta_0\\|^2 + \\lambda_{smooth} \\|Ly\\|^2 : Y: fMRI data (T Ã— V) X(Î¸): Design matrix convolved HRF(Î¸) y: Soft labels (N Ã— 1) W: Decoder weights (V Ã— 1) L: Laplacian temporal smoothness non-convex (y, Î¸, W) jointly, convex variable others fixed. ALS exploits :","code":""},{"path":"/articles/05-weakly-supervised.html","id":"als-steps","dir":"Articles","previous_headings":"The alternating least squares algorithm","what":"ALS steps","title":"Weakly supervised learning for fMRI","text":"step regularized least squares problem closed-form solution.","code":"Initialize: yâ‚€, Î¸â‚€, Wâ‚€ For iteration t = 1, 2, ..., max_iter:    1. Update soft labels:      y_t = argmin_y || Y - X(Î¸_{t-1}) y W_{t-1}^T ||^2 + Î»_smooth ||Ly||^2    2. Update HRF:      Î¸_t = argmin_Î¸ || Y - X(Î¸) y_t W_{t-1}^T ||^2 + Î»_HRF ||Î¸ - Î¸â‚€||^2    3. Update decoder:      W_t = argmin_W || Y - X(Î¸_t) y_t W^T ||^2 + Î»_W ||W||^2    If || (y_t, Î¸_t, W_t) - (y_{t-1}, Î¸_{t-1}, W_{t-1}) || < tol:     Break (converged)"},{"path":"/articles/05-weakly-supervised.html","id":"regularization","dir":"Articles","previous_headings":"","what":"Regularization parameters","title":"Weakly supervised learning for fMRI","text":"Four main parameters control optimization:","code":""},{"path":"/articles/05-weakly-supervised.html","id":"lambda-w","dir":"Articles","previous_headings":"Regularization parameters","what":"lambda_W: Decoder ridge penalty","title":"Weakly supervised learning for fMRI","text":"Controls decoder complexity via L2 penalty weights. Low (0.01): Complex decoder, risk overfitting Medium (0.1): Default, good balance High (1.0): Simple decoder, risk underfitting increase: Many voxels, low SNR, small sample decrease: voxels, high SNR, large sample","code":""},{"path":"/articles/05-weakly-supervised.html","id":"lambda-hrf","dir":"Articles","previous_headings":"Regularization parameters","what":"lambda_HRF: HRF prior strength","title":"Weakly supervised learning for fMRI","text":"Pulls HRF toward canonical shape. Low (0.001): Data-driven HRF, high flexibility Medium (0.01): Default, gentle regularization High (1.0): Nearly fixed canonical HRF increase: Uncertain HRF, low SNR, standard populations decrease: Known HRF deviation, high SNR, atypical populations","code":""},{"path":"/articles/05-weakly-supervised.html","id":"lambda-smooth","dir":"Articles","previous_headings":"Regularization parameters","what":"lambda_smooth: Soft label smoothness","title":"Weakly supervised learning for fMRI","text":"Encourages temporal smoothness soft labels across trials. Low (0): Independent trial labels Medium (0.1): Gentle smoothing High (1.0): Strong smoothness (assumes slow label drift) increase: Blocked designs, gradual condition changes decrease: Event-related designs, rapid switching","code":""},{"path":"/articles/05-weakly-supervised.html","id":"theta-penalty","dir":"Articles","previous_headings":"Regularization parameters","what":"theta_penalty: HRF coefficient penalty","title":"Weakly supervised learning for fMRI","text":"Additional L2 penalty HRF basis coefficients (beyond prior). Default (0.01): Light regularization Can prevent extreme HRF shapes","code":""},{"path":[]},{"path":"/articles/05-weakly-supervised.html","id":"grid-search","dir":"Articles","previous_headings":"Practical parameter tuning","what":"Strategy 1: Grid search","title":"Weakly supervised learning for fMRI","text":"","code":"# Setup (assuming data and models are defined) library(hrfdecode)  # Parameter grid lambda_W_values <- c(0.01, 0.1, 0.5, 1.0) results <- list()  for (i in seq_along(lambda_W_values)) {   fit <- fit_hrfdecoder(     Y = Y_train,     ev_model = ev_model,     base_model = bl_model,     lambda_W = lambda_W_values[i],     verbose = FALSE   )    # Evaluate on validation set   preds <- predict(fit, newdata = Y_val, mode = \"trial\")   acc <- mean(sign(preds) == true_labels_val)    results[[i]] <- list(lambda_W = lambda_W_values[i], accuracy = acc) }  # Select best best_idx <- which.max(sapply(results, function(x) x$accuracy)) best_lambda_W <- results[[best_idx]]$lambda_W"},{"path":"/articles/05-weakly-supervised.html","id":"cross-validation","dir":"Articles","previous_headings":"Practical parameter tuning","what":"Strategy 2: Cross-validation","title":"Weakly supervised learning for fMRI","text":"Integrate rMVPA automatic parameter selection:","code":"library(rMVPA)  # Define parameter grid param_grid <- expand.grid(   lambda_W = c(0.1, 0.5, 1.0),   lambda_HRF = c(0.001, 0.01, 0.1) )  # Cross-validate each combination # (Conceptual; requires rMVPA infrastructure) # cv_results <- tune_parameters(hrfdecoder_model, param_grid, cv_folds = 5)"},{"path":"/articles/05-weakly-supervised.html","id":"heuristics","dir":"Articles","previous_headings":"Practical parameter tuning","what":"Strategy 3: Heuristics","title":"Weakly supervised learning for fMRI","text":"Rule--thumb starting points: lambda_W: 0.1 Ã— (number voxels / number trials) lambda_HRF: 0.01 (rarely needs tuning) lambda_smooth: 0 event-related, 0.1-0.5 block designs theta_penalty: 0.01 (fixed)","code":""},{"path":"/articles/05-weakly-supervised.html","id":"convergence","dir":"Articles","previous_headings":"","what":"Convergence diagnostics","title":"Weakly supervised learning for fMRI","text":"Monitor convergence ensure optimization succeeded. Fit convergence tracking:","code":"library(hrfdecode) library(fmridesign)  # Simple simulation n_trs <- 150 n_voxels <- 30 n_trials <- 30 tr <- 2  onsets <- seq(10, n_trs * tr - 20, length.out = n_trials) conditions <- rep(c(\"A\", \"B\"), each = n_trials / 2) event_table <- data.frame(onset = onsets, condition = conditions, duration = 1)    ev_model <- event_model(   onset ~ hrf(condition, basis = \"spmg1\"),   data = event_table,   block = ~ 1,   sampling_frame = fmrihrf::sampling_frame(TR = tr, blocklens = n_trs) )   bl_model <- baseline_model(basis = \"bs\", degree = 3,                            sframe = fmrihrf::sampling_frame(TR = tr, blocklens = n_trs))  # Simulate data hrf_basis <- fmrihrf::evaluate(fmrihrf::getHRF(\"spmg2\"), seq(0, 24, by = tr)) hrf_vec <- as.numeric(hrf_basis %*% c(1, 0)) stick_A <- rep(0, n_trs); stick_B <- rep(0, n_trs) idx_A <- pmin(n_trs, pmax(1L, floor(event_table$onset[event_table$condition == \"A\"] / tr) + 1L)) idx_B <- pmin(n_trs, pmax(1L, floor(event_table$onset[event_table$condition == \"B\"] / tr) + 1L)) stick_A[idx_A] <- 1; stick_B[idx_B] <- 1 signal <- stick_A - stick_B signal_conv <- stats::convolve(signal, rev(hrf_vec), type = \"open\")[1:n_trs]  Y_data <- matrix(rnorm(n_trs * n_voxels), n_trs, n_voxels) for (v in 1:n_voxels) {   Y_data[, v] <- Y_data[, v] + signal_conv * (v <= n_voxels / 2) * 0.5 } fit <- fit_hrfdecoder(   Y = Y_data,   ev_model = ev_model,   base_model = bl_model,   lambda_W = 0.1,   max_iter = 15,   tol = 1e-4,   verbose = FALSE )  # Inspect convergence info names(fit$convergence) #> NULL # Did it converge? if (!is.null(fit$convergence$converged)) {   cat(\"Converged:\", fit$convergence$converged, \"\\n\")   if (fit$convergence$converged) {     cat(\"Iterations:\", fit$convergence$iterations, \"\\n\")   } } else {   cat(\"Convergence info not available in this version\\n\") } #> Convergence info not available in this version"},{"path":"/articles/05-weakly-supervised.html","id":"warning-signs","dir":"Articles","previous_headings":"Convergence diagnostics","what":"Warning signs","title":"Weakly supervised learning for fMRI","text":"Max iterations reached: Increase max_iter check parameter values Oscillating objective: Reduce learning rate (exposed) increase regularization iterations: May indicate poor initialization trivial solution","code":""},{"path":"/articles/05-weakly-supervised.html","id":"soft-labels","dir":"Articles","previous_headings":"","what":"Understanding soft labels","title":"Weakly supervised learning for fMRI","text":"Soft labels represent algorithmâ€™s continuous predictions trial. Soft labels learned training. Continuous predictions capture graded trial-level responses rather hard class assignments. Soft labels can reveal: Graded responses: trials â€œtypicalâ€ others Learning effects: Early vs.Â late trials may differ Attention lapses: Outlier trials weak predictions","code":"# Extract soft labels by aggregating TR-level predictions to trials pred_train <- predict_hrfdecoder(fit, Y_test = Y_data, ev_model_test = ev_model, mode = \"trial\") # For two-class case, define a signed margin (A vs B) y_soft <- as.numeric(pred_train$probs[, 1] - pred_train$probs[, 2])  # Summary cat(\"Soft label range:\", round(range(y_soft), 3), \"\\n\") #> Soft label range: -0.22 0.189 cat(\"Mean absolute value:\", round(mean(abs(y_soft)), 3), \"\\n\") #> Mean absolute value: 0.143  # For binary classification, expect negative/positive split true_labels <- ifelse(conditions == \"A\", 1, -1) cat(\"\\nCorrelation with true labels:\", round(cor(y_soft, true_labels), 3), \"\\n\") #>  #> Correlation with true labels: 0.963 if (requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)    plot_df <- data.frame(     trial = 1:n_trials,     soft_label = y_soft,     condition = conditions,     true_label = true_labels   )    ggplot(plot_df, aes(x = trial, y = soft_label, color = condition)) +     geom_point(size = 2.5) +     geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +     hrfdecode::scale_color_albers() +     labs(       title = \"Learned soft labels\",       subtitle = \"Continuous trial predictions from weakly supervised learning\",       x = \"Trial number\",       y = \"Soft label value\",       color = \"Condition\"     ) }"},{"path":"/articles/05-weakly-supervised.html","id":"complexity","dir":"Articles","previous_headings":"","what":"Computational complexity","title":"Weakly supervised learning for fMRI","text":"ALS iteration involves: Soft label update: O(NÂ³) N trials (Laplacian solve) HRF update: O(KÂ³) K basis functions (typically K = 2-3) Decoder update: O(VÂ³) V voxels (ridge regression) Bottleneck: Decoder update V large (thousands voxels). Speedup strategies: Prewhitening: Reduces effective dimensionality Rank reduction: Automatic nuisance rank estimation Sparse solvers: Exploit structure design matrices Typical runtime: 1-5 seconds 100 trials Ã— 100 voxels modern hardware.","code":""},{"path":[]},{"path":"/articles/05-weakly-supervised.html","id":"initialization","dir":"Articles","previous_headings":"Advanced topics","what":"Custom initialization","title":"Weakly supervised learning for fMRI","text":"algorithm initializes: y: Random normal based condition means Î¸: Canonical HRF (1, 0, â€¦) W: Ridge regression initial y Custom initialization (currently exposed) improve convergence difficult cases.","code":""},{"path":"/articles/05-weakly-supervised.html","id":"optimizers","dir":"Articles","previous_headings":"Advanced topics","what":"Alternative optimizers","title":"Weakly supervised learning for fMRI","text":"ALS simple stable always fastest. Alternatives: Coordinate descent: Update parameter sequentially Gradient descent: First-order optimization ADMM: Alternating direction method multipliers Current implementation uses ALS reliability interpretability.","code":""},{"path":[]},{"path":"/articles/05-weakly-supervised.html","id":"poor-predictions","dir":"Articles","previous_headings":"Debugging common issues","what":"Issue: Poor predictions","title":"Weakly supervised learning for fMRI","text":"Symptoms: Random -chance accuracy Possible causes: -regularization: Try decreasing lambda_W Wrong HRF: Try flexible HRF (lambda_HRF = 0.001) Insufficient iterations: Increase max_iter Misspecified design: Check event model timing","code":""},{"path":"/articles/05-weakly-supervised.html","id":"slow-convergence","dir":"Articles","previous_headings":"Debugging common issues","what":"Issue: Slow convergence","title":"Weakly supervised learning for fMRI","text":"Symptoms: Reaches max_iter without converging Possible causes: Weak signal: Increase regularization Conflicting constraints: Check lambda values Poor scaling: Ensure data standardized","code":""},{"path":"/articles/05-weakly-supervised.html","id":"extreme-labels","dir":"Articles","previous_headings":"Debugging common issues","what":"Issue: Extreme soft labels","title":"Weakly supervised learning for fMRI","text":"Symptoms: y_soft values >> 1 magnitude Possible causes: -regularization: Increase lambda_smooth Overfitting: Increase lambda_W","code":""},{"path":"/articles/05-weakly-supervised.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Weakly supervised learning for fMRI","text":"Getting Started â€” Basic usage tutorial AR Prewhitening â€” Handle temporal autocorrelation rMVPA Integration â€” Cross-validation framework HRF Estimation â€” Joint HRF learning","code":""},{"path":"/articles/05-weakly-supervised.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session info","title":"Weakly supervised learning for fMRI","text":"","code":"sessioninfo::session_info(pkgs = \"hrfdecode\") #> â”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  setting  value #>  version  R version 4.5.1 (2025-06-13) #>  os       macOS Sonoma 14.3 #>  system   aarch64, darwin20 #>  ui       X11 #>  language en #>  collate  en_US.UTF-8 #>  ctype    en_US.UTF-8 #>  tz       America/Toronto #>  date     2025-11-09 #>  pandoc   3.7.0.2 @ /opt/homebrew/bin/ (via rmarkdown) #>  quarto   1.7.32 @ /usr/local/bin/quarto #>  #> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #>  package        * version    date (UTC) lib source #>  askpass          1.2.1      2024-10-04 [2] CRAN (R 4.5.0) #>  assertthat       0.2.1      2019-03-21 [2] CRAN (R 4.5.0) #>  backports        1.5.0      2024-05-23 [2] CRAN (R 4.5.0) #>  base64enc        0.1-3      2015-07-28 [2] CRAN (R 4.5.0) #>  bdsmatrix        1.3-7      2024-03-02 [2] CRAN (R 4.5.0) #>  bigassertr       0.1.7      2025-06-27 [2] CRAN (R 4.5.0) #>  bigparallelr     0.3.2      2021-10-02 [2] CRAN (R 4.5.0) #>  bigstatsr        1.6.2      2025-07-29 [2] CRAN (R 4.5.0) #>  bit              4.6.0      2025-03-06 [2] CRAN (R 4.5.0) #>  bit64            4.6.0-1    2025-01-16 [2] CRAN (R 4.5.0) #>  bitops           1.0-9      2024-10-03 [2] CRAN (R 4.5.0) #>  broom            1.0.10     2025-09-13 [2] CRAN (R 4.5.0) #>  bslib            0.9.0      2025-01-30 [2] CRAN (R 4.5.0) #>  cachem           1.1.0      2024-05-16 [2] CRAN (R 4.5.0) #>  caTools          1.18.3     2024-09-04 [2] CRAN (R 4.5.0) #>  cli              3.6.5      2025-04-23 [2] CRAN (R 4.5.0) #>  clipr            0.8.0      2022-02-22 [2] CRAN (R 4.5.0) #>  codetools        0.2-20     2024-03-31 [3] CRAN (R 4.5.1) #>  colorplane       0.5.0      2025-11-02 [2] Github (bbuchsbaum/colorplane@1b7a26f) #>  corpcor          1.6.10     2021-09-16 [2] CRAN (R 4.5.0) #>  cowplot          1.2.0      2025-07-07 [2] CRAN (R 4.5.0) #>  cpp11            0.5.2      2025-03-03 [2] CRAN (R 4.5.0) #>  crayon           1.5.3      2024-06-20 [2] CRAN (R 4.5.0) #>  crosstalk        1.2.2      2025-08-26 [2] CRAN (R 4.5.0) #>  curl             7.0.0      2025-08-19 [2] CRAN (R 4.5.0) #>  data.table       1.17.8     2025-07-10 [2] CRAN (R 4.5.0) #>  dbscan           1.2.3      2025-08-20 [2] CRAN (R 4.5.0) #>  deflist          0.2.0      2023-04-27 [2] CRAN (R 4.5.0) #>  DEoptimR         1.1-4      2025-07-27 [2] CRAN (R 4.5.0) #>  digest           0.6.37     2024-08-19 [2] CRAN (R 4.5.0) #>  doParallel       1.0.17     2022-02-07 [2] CRAN (R 4.5.0) #>  dplyr            1.1.4      2023-11-17 [2] CRAN (R 4.5.0) #>  entropy          1.3.2      2025-04-07 [2] CRAN (R 4.5.0) #>  evaluate         1.0.5      2025-08-27 [2] CRAN (R 4.5.0) #>  farver           2.1.2      2024-05-13 [2] CRAN (R 4.5.0) #>  fastmap          1.2.0      2024-05-15 [2] CRAN (R 4.5.0) #>  fdrtool          1.2.18     2024-08-20 [2] CRAN (R 4.5.0) #>  ff               4.5.2      2025-01-13 [2] CRAN (R 4.5.0) #>  ffmanova         1.1.2      2023-10-18 [2] CRAN (R 4.5.0) #>  filenamer        0.3        2025-04-09 [2] CRAN (R 4.5.0) #>  flock            0.7        2016-11-12 [2] CRAN (R 4.5.0) #>  fmriAR           0.1.0      2025-10-18 [2] Github (bbuchsbaum/fmriAR@0b10352) #>  fmridesign     * 0.5.0      2025-11-09 [2] Github (bbuchsbaum/fmridesign@f1462eb) #>  fmrihrf          0.1.0.9000 2025-11-01 [2] Github (bbuchsbaum/fmrihrf@708058f) #>  FNN              1.1.4.1    2024-09-22 [2] CRAN (R 4.5.0) #>  fontawesome      0.5.3      2024-11-16 [2] CRAN (R 4.5.0) #>  foreach          1.5.2      2022-02-02 [2] CRAN (R 4.5.0) #>  formatR          1.14       2023-01-17 [2] CRAN (R 4.5.0) #>  fs               1.6.6      2025-04-12 [2] CRAN (R 4.5.0) #>  furrr            0.3.1      2022-08-15 [2] CRAN (R 4.5.0) #>  futile.logger    1.4.3      2016-07-10 [2] CRAN (R 4.5.0) #>  futile.options   1.0.1      2018-04-20 [2] CRAN (R 4.5.0) #>  future           1.67.0     2025-07-29 [2] CRAN (R 4.5.0) #>  future.apply     1.20.0     2025-06-06 [2] CRAN (R 4.5.0) #>  generics         0.1.4      2025-05-09 [2] CRAN (R 4.5.0) #>  ggplot2        * 4.0.0      2025-09-11 [2] CRAN (R 4.5.0) #>  gifti            0.8.0      2020-11-11 [2] CRAN (R 4.5.0) #>  glmnet           4.1-10     2025-07-17 [2] CRAN (R 4.5.0) #>  globals          0.18.0     2025-05-08 [2] CRAN (R 4.5.0) #>  glue             1.8.0      2024-09-30 [2] CRAN (R 4.5.0) #>  gplots           3.2.0      2024-10-05 [2] CRAN (R 4.5.0) #>  gtable           0.3.6      2024-10-25 [2] CRAN (R 4.5.0) #>  gtools           3.9.5      2023-11-20 [2] CRAN (R 4.5.0) #>  hardhat          1.4.2      2025-08-20 [2] CRAN (R 4.5.0) #>  highr            0.11       2024-05-26 [2] CRAN (R 4.5.0) #>  hms              1.1.4      2025-10-17 [2] CRAN (R 4.5.0) #>  hrfdecode      * 0.2.0      2025-11-09 [1] local #>  htmltools        0.5.8.1    2024-04-04 [2] CRAN (R 4.5.0) #>  htmlwidgets      1.6.4      2023-12-06 [2] CRAN (R 4.5.0) #>  httr             1.4.7      2023-08-15 [2] CRAN (R 4.5.0) #>  igraph           2.2.1      2025-10-27 [2] CRAN (R 4.5.0) #>  io               0.3.2      2019-12-17 [2] CRAN (R 4.5.0) #>  isoband          0.2.7      2022-12-20 [2] CRAN (R 4.5.0) #>  iterators        1.0.14     2022-02-05 [2] CRAN (R 4.5.0) #>  jquerylib        0.1.4      2021-04-26 [2] CRAN (R 4.5.0) #>  jsonlite         2.0.0      2025-03-27 [2] CRAN (R 4.5.0) #>  KernSmooth       2.23-26    2025-01-01 [3] CRAN (R 4.5.1) #>  knitr            1.50       2025-03-16 [2] CRAN (R 4.5.0) #>  labeling         0.4.3      2023-08-29 [2] CRAN (R 4.5.0) #>  lambda.r         1.2.4      2019-09-18 [2] CRAN (R 4.5.0) #>  later            1.4.4      2025-08-27 [2] CRAN (R 4.5.0) #>  lattice          0.22-7     2025-04-02 [3] CRAN (R 4.5.1) #>  lazyeval         0.2.2      2019-03-15 [2] CRAN (R 4.5.0) #>  lifecycle        1.0.4      2023-11-07 [2] CRAN (R 4.5.0) #>  listenv          0.10.0     2025-11-02 [2] CRAN (R 4.5.0) #>  magrittr         2.0.4      2025-09-12 [2] CRAN (R 4.5.0) #>  MASS             7.3-65     2025-02-28 [2] CRAN (R 4.5.0) #>  Matrix           1.7-3      2025-03-11 [3] CRAN (R 4.5.1) #>  matrixStats      1.5.0      2025-01-07 [2] CRAN (R 4.5.0) #>  memoise          2.0.1      2021-11-26 [2] CRAN (R 4.5.0) #>  mime             0.13       2025-03-17 [2] CRAN (R 4.5.0) #>  mmap             0.6-22     2023-12-08 [2] CRAN (R 4.5.0) #>  modelr           0.1.11     2023-03-22 [2] CRAN (R 4.5.0) #>  mvtnorm          1.3-3      2025-01-10 [2] CRAN (R 4.5.0) #>  neuroim2         0.8.3      2025-11-07 [2] Github (bbuchsbaum/neuroim2@77cd9c4) #>  neurosurf        0.1.0      2025-11-02 [2] Github (bbuchsbaum/neurosurf@5af7de2) #>  numDeriv         2016.8-1.1 2019-06-06 [2] CRAN (R 4.5.0) #>  openssl          2.3.4      2025-09-30 [2] CRAN (R 4.5.0) #>  otel             0.2.0      2025-08-29 [2] CRAN (R 4.5.0) #>  parallelly       1.45.1     2025-07-24 [2] CRAN (R 4.5.0) #>  pillar           1.11.1     2025-09-17 [2] CRAN (R 4.5.0) #>  pkgconfig        2.0.3      2019-09-22 [2] CRAN (R 4.5.0) #>  plotly           4.11.0     2025-06-19 [2] CRAN (R 4.5.0) #>  pls              2.8-5      2024-09-15 [2] CRAN (R 4.5.0) #>  plyr             1.8.9      2023-10-02 [2] CRAN (R 4.5.0) #>  pracma           2.4.6      2025-10-22 [2] CRAN (R 4.5.0) #>  prettyunits      1.2.0      2023-09-24 [2] CRAN (R 4.5.0) #>  progress         1.2.3      2023-12-06 [2] CRAN (R 4.5.0) #>  promises         1.5.0      2025-11-01 [2] CRAN (R 4.5.0) #>  proxy            0.4-27     2022-06-09 [2] CRAN (R 4.5.0) #>  ps               1.9.1      2025-04-12 [2] CRAN (R 4.5.0) #>  purrr            1.2.0      2025-11-04 [2] CRAN (R 4.5.0) #>  R.methodsS3      1.8.2      2022-06-13 [2] CRAN (R 4.5.0) #>  R.oo             1.27.1     2025-05-02 [2] CRAN (R 4.5.0) #>  R.utils          2.13.0     2025-02-24 [2] CRAN (R 4.5.0) #>  R6               2.6.1      2025-02-15 [2] CRAN (R 4.5.0) #>  rappdirs         0.3.3      2021-01-31 [2] CRAN (R 4.5.0) #>  RColorBrewer     1.1-3      2022-04-03 [2] CRAN (R 4.5.0) #>  Rcpp             1.1.0      2025-07-02 [2] CRAN (R 4.5.0) #>  RcppArmadillo    15.0.2-2   2025-09-19 [2] CRAN (R 4.5.0) #>  RcppEigen        0.3.4.0.2  2024-08-24 [2] CRAN (R 4.5.0) #>  RcppParallel     5.1.11-1   2025-08-27 [2] CRAN (R 4.5.0) #>  readr            2.1.5      2024-01-10 [2] CRAN (R 4.5.0) #>  Rfit             0.27.0     2024-05-25 [2] CRAN (R 4.5.0) #>  rgl              1.3.24     2025-06-25 [2] CRAN (R 4.5.0) #>  RhpcBLASctl      0.23-42    2023-02-11 [2] CRAN (R 4.5.0) #>  rlang            1.1.6      2025-04-11 [2] CRAN (R 4.5.0) #>  rmarkdown        2.30       2025-09-28 [2] CRAN (R 4.5.0) #>  rmio             0.4.0      2022-02-17 [2] CRAN (R 4.5.0) #>  rMVPA            0.1.2      2025-11-09 [2] local #>  RNifti           1.8.0      2025-02-22 [2] CRAN (R 4.5.0) #>  RNiftyReg        2.8.4      2024-09-30 [2] CRAN (R 4.5.0) #>  robustbase       0.99-6     2025-09-04 [2] CRAN (R 4.5.0) #>  rsample          1.3.1      2025-07-29 [2] CRAN (R 4.5.0) #>  RSpectra         0.16-2     2024-07-18 [2] CRAN (R 4.5.0) #>  Rvcg             0.25       2025-03-14 [2] CRAN (R 4.5.0) #>  S7               0.2.0      2024-11-07 [2] CRAN (R 4.5.0) #>  sass             0.4.10     2025-04-11 [2] CRAN (R 4.5.0) #>  scales           1.4.0      2025-04-24 [2] CRAN (R 4.5.0) #>  sda              1.3.9      2025-04-08 [2] CRAN (R 4.5.0) #>  shape            1.4.6.1    2024-02-23 [2] CRAN (R 4.5.0) #>  slider           0.3.2      2024-10-25 [2] CRAN (R 4.5.0) #>  sparsediscrim    0.3.0      2021-07-01 [2] CRAN (R 4.5.0) #>  sparsevctrs      0.3.4      2025-05-25 [2] CRAN (R 4.5.0) #>  stringi          1.8.7      2025-03-27 [2] CRAN (R 4.5.0) #>  stringr          1.6.0      2025-11-04 [2] CRAN (R 4.5.0) #>  survival         3.8-3      2024-12-17 [3] CRAN (R 4.5.1) #>  sys              3.4.3      2024-10-04 [2] CRAN (R 4.5.0) #>  tibble           3.3.0      2025-06-08 [2] CRAN (R 4.5.0) #>  tidyr            1.3.1      2024-01-24 [2] CRAN (R 4.5.0) #>  tidyselect       1.2.1      2024-03-11 [2] CRAN (R 4.5.0) #>  tinytex          0.57       2025-04-15 [2] CRAN (R 4.5.0) #>  tzdb             0.5.0      2025-03-15 [2] CRAN (R 4.5.0) #>  utf8             1.2.6      2025-06-08 [2] CRAN (R 4.5.0) #>  vctrs            0.6.5      2023-12-01 [2] CRAN (R 4.5.0) #>  viridisLite      0.4.2      2023-05-02 [2] CRAN (R 4.5.0) #>  vroom            1.6.6      2025-09-19 [2] CRAN (R 4.5.0) #>  warp             0.2.1      2023-11-02 [2] CRAN (R 4.5.0) #>  whitening        1.4.0      2022-06-07 [2] CRAN (R 4.5.0) #>  withr            3.0.2      2024-10-28 [2] CRAN (R 4.5.0) #>  xfun             0.54       2025-10-30 [2] CRAN (R 4.5.0) #>  xml2             1.4.1      2025-10-27 [2] CRAN (R 4.5.0) #>  yaml             2.3.10     2024-07-26 [2] CRAN (R 4.5.0) #>  yardstick        1.3.2      2025-01-22 [2] CRAN (R 4.5.0) #>  #>  [1] /private/var/folders/9h/nkjq6vss7mqdl4ck7q1hd8ph0000gp/T/RtmpfzOoi1/temp_libpathe6e4ebda2ae #>  [2] /Users/bbuchsbaum/Library/R/arm64/4.5/library #>  [3] /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library #>  * â”€â”€ Packages attached to the search path. #>  #> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Name. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Name Y (2025). hrfdecode: HRF-Aware Weakly Supervised MVPA Decoder fMRI. R package version 0.2.0.","code":"@Manual{,   title = {hrfdecode: HRF-Aware Weakly Supervised MVPA Decoder for fMRI},   author = {Your Name},   year = {2025},   note = {R package version 0.2.0}, }"},{"path":[]},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"executive-summary","dir":"","previous_headings":"","what":"Executive Summary","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"report documents rMVPA plugin architecture specific focus hrfdecoder decoder model integrates. analysis covers (1) S3 model plugin system, (2) data flow searchlight/cross-validation pipelines, (3) existing preprocessing capabilities, (4) architectural patterns extending framework.","code":""},{"path":[]},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_11-s3-method-dispatch-system","dir":"","previous_headings":"1. Model Plugin Architecture","what":"1.1 S3 Method Dispatch System","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"rMVPA uses S3 classes extensibility. Core model operations dispatch : Generic Functions (defined /R/allgeneric.R): - train_model(obj, ...) â†’ trains model ROI/fold data - predict_model(object, fit, newdata, ...) â†’ generates predictions - format_result(obj, result, error_message, context, ...) â†’ formats fold-level outputs - merge_results(obj, result_set, indices, id, ...) â†’ aggregates fold results ROI-level performance - compute_performance(obj, result) â†’ extracts metrics result","code":""},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_12-model-spec-creation-create_model_spec-and-mvpa_model","dir":"","previous_headings":"1. Model Plugin Architecture","what":"1.2 Model Spec Creation: create_model_spec() and mvpa_model()","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"Function: /R/mvpa_model.R:196-213 /R/mvpa_model.R:260-337 Key Fields Model Spec:","code":"create_model_spec(name, dataset, design, return_predictions=FALSE,                    compute_performance=FALSE, tune_reps=FALSE, ...) # Returns: S3 object with class c(name, \"model_spec\", \"list\")  mvpa_model(model, dataset, design, model_type=c(\"classification\", \"regression\"),             crossval=NULL, feature_selector=NULL, tune_grid=NULL, ...) # Returns: S3 object with class c(\"mvpa_model\", \"model_spec\", \"list\") model_spec$dataset          # mvpa_dataset with train_data, test_data, mask model_spec$design           # mvpa_design with y_train, y_test, block_var model_spec$crossval         # cross_validation spec (blocked_cross_validation, etc.) model_spec$compute_performance  # logical: compute metrics after fold merging model_spec$return_predictions   # logical: return per-fold predictions model_spec$return_fits      # logical: keep fitted models model_spec$performance      # function: (result) -> named_metrics model_spec$has_test_set     # logical: external test set present"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_13-hrfdecoder-plugin-specialized-model-spec","dir":"","previous_headings":"1. Model Plugin Architecture","what":"1.3 hrfdecoder Plugin: Specialized Model Spec","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"File: /R/hrfdecoder_model.R:97-146 Key Differences Standard MVPA Models: - Operates TR-level (continuous) data, trial-level betas - y_train() returns dummy TR sequence (1:T) fold construction - Actual targets come design$event_model design$events (ignored fold machinery) - Fold assignment determined block_var (run IDs), y_train values","code":"hrfdecoder_model <- function(   dataset,           # mvpa_dataset with TR x V data   design,            # hrfdecoder_design (custom mvpa_design subclass)   lambda_W = 10,     # ridge penalty for decoder weights   lambda_HRF = 1,    # HRF conformity penalty   lambda_smooth = 5, # temporal smoothness penalty   basis = NULL,      # HRF basis (fmrihrf::spmg1, etc.)   window = c(4, 8),  # event aggregation window (seconds)   nonneg = TRUE,     # project soft labels to [0,1]   max_iter = 10,     # ALS iterations   tol = 1e-4,        # convergence tolerance   performance = NULL,   crossval = NULL,   return_predictions = TRUE,   return_fits = FALSE ) # Returns: S3 object with class c(\"hrfdecoder_model\", \"model_spec\", \"list\")"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_14-s3-method-implementation-pattern","dir":"","previous_headings":"1. Model Plugin Architecture","what":"1.4 S3 Method Implementation Pattern","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"Example hrfdecoder:","code":"# STEP 1: y_train extraction (for fold construction) y_train.hrfdecoder_model <- function(obj) {   seq_len(nobs(obj$dataset))  # Dummy sequence 1:T }  # STEP 2: Model training (ignored y parameter, uses design metadata) train_model.hrfdecoder_model <- function(obj, train_dat, y, sl_info, cv_spec, indices, ...) {   X <- as.matrix(train_dat)   evm <- obj$design$event_model   events <- obj$design$events   fit <- hrfdecoder::hrfdecoder_fit(X, event_model=evm, basis=obj$basis, ...)   structure(list(fit=fit, sl_info=sl_info, indices=indices),              class=\"hrfdecoder_fit_wrap\") }  # STEP 3: Format fold result (predict + aggregate to events) format_result.hrfdecoder_model <- function(obj, result, error_message=NULL, context, ...) {   Xtest <- as.matrix(context$test)   Ptest <- predict(result$fit, Xtest)  # TR-level soft labels   agg <- hrfdecoder::aggregate_events(Ptest, obj$design$events, window=obj$window)   # Return tibble with event-level probs and predictions }  # STEP 4: Merge fold results merge_results.hrfdecoder_model <- function(obj, result_set, indices, id, ...) {   # Concatenate fold results, build classification_result, compute performance }  # STEP 5: Compute performance compute_performance.hrfdecoder_model <- function(obj, result) {   obj$performance(result)  # Delegates to perf_fun }"},{"path":[]},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_21-roisearchlight-extraction--cross-validation-folds--trainingtesting","dir":"","previous_headings":"2. Data Flow: Searchlight and Cross-Validation","what":"2.1 ROI/Searchlight Extraction â†’ Cross-Validation Folds â†’ Training/Testing","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"Main Pipeline (/R/mvpa_iterate.R /R/allgeneric.R):","code":"process_roi(mod_spec, roi, rnum, center_global_id)   â”œâ”€â†’ Check for external test set (has_test_set)   â”‚    â””â”€â†’ external_crossval(mspec, roi, id, center_global_id)   â”œâ”€â†’ Check for internal cross-validation (has_crossval)   â”‚    â””â”€â†’ internal_crossval(mspec, roi, id, center_global_id)   â””â”€â†’ Fallback: no CV        â””â”€â†’ process_roi_default(mspec, roi, rnum, center_global_id)"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_22-internal-cross-validation-most-common","dir":"","previous_headings":"2. Data Flow: Searchlight and Cross-Validation","what":"2.2 Internal Cross-Validation (Most Common)","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"Function: /R/mvpa_iterate.R:224-296 Key Insight: Data extraction (step 1) happens fold creation. means: - Preprocessing MUST happen fold iteration avoid data leakage - Preprocessing can done per-ROI, step 2 - per-fold preprocessing without custom handling","code":"internal_crossval <- function(mspec, roi, id, center_global_id = NA) {   # 1. EXTRACT DATA: Get train_roi from searchlight/region   xtrain <- neuroim2::values(roi$train_roi)  # ROI matrix: obs x features   ind <- neuroim2::indices(roi$train_roi)     # Global voxel indices      # 2. GENERATE FOLDS: Create train/test splits based on crossval spec   samples <- crossval_samples(     mspec$crossval,     tibble::as_tibble(xtrain),     y_train(mspec)  # For hrfdecoder: dummy TR sequence   )   # Returns tibble with columns: ytrain, ytest, train, test, .id      # 3. ITERATE FOLDS: For each fold   ret <- samples %>% pmap(function(ytrain, ytest, train, test, .id) {     # 3a. TRAIN     result <- train_model(       mspec,        tibble::as_tibble(train),       ytrain,           # For hrfdecoder: dummy sequence       sl_info = list(center_local_id, center_global_id),       cv_spec = mspec$crossval,       indices = ind     )          # 3b. FORMAT (predict + aggregate)     format_result(       mspec, result,        error_message = NULL,       context = list(         test = test,      # Test ROI data         ytest = ytest,    # Test labels/dummy         ...       )     )   }) %>% bind_rows()      # 4. MERGE FOLDS: Aggregate across folds   merge_results(mspec, ret, indices=ind, id=id)   # Returns: tibble with result, indices, performance, id, error }"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_23-cross-validation-specification","dir":"","previous_headings":"2. Data Flow: Searchlight and Cross-Validation","what":"2.3 Cross-Validation Specification","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"File: /R/crossval.R:402-546 Implementation:","code":"blocked_cross_validation(block_var) # Standard: leave one block (run) out at a time # Returns: object with class \"blocked_cross_validation\" # Used by: crossval_samples() â†’ creates folds  sequential_blocked_cross_validation(block_var, nfolds=2, nreps=4) # Advanced: subdivides each block further  custom_cross_validation(sample_set) # User provides explicit train/test index pairs crossval_samples.blocked_cross_validation <- function(obj, data, y, ...) {   crossv_block(data, y, obj$block_var)   # Returns: tibble(ytrain, ytest, train, test, .id) for each fold }"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_24-data-sample-structure","dir":"","previous_headings":"2. Data Flow: Searchlight and Cross-Validation","what":"2.4 Data Sample Structure","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"extracted ROI data wrapped modelr::resample objects (lightweight index-based views): Unpacking model training:","code":"# Inside crossval_samples: list(   ytrain = y[train_indices],   ytest = y[test_indices],   train = modelr::resample(data, train_indices),  # Lazy view   test = modelr::resample(data, test_indices)      # Lazy view ) xtrain <- tibble::as_tibble(train, .name_repair=.name_repair) # Converts to matrix/tibble of actual values"},{"path":[]},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_31-dataset-level-normalization","dir":"","previous_headings":"3. Existing Preprocessing Capabilities","what":"3.1 Dataset-Level Normalization","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"File: /R/common.R:52-88 Applied: dataset initialization configuration pipeline (fold-aware).","code":"# Per-volume z-scoring (before fold creation) normalize_image_samples <- function(bvec, mask) {   # Scale each volume independently   vlist <- vols(bvec) %>% future_map(function(v) {     scale(v[mask>0])[,1]   })   SparseNeuroVec(do.call(cbind, vlist), space(bvec), mask=mask) }  # Per-block standardization (within-run z-scoring) standardize_vars <- function(bvec, mask, blockvar) {   # Split by block, z-score within each block   vlist <- vectors(bvec, subset=which(mask>0)) %>% future_map(function(v) {     unlist(map(split(v, blockvar), scale))   })   SparseNeuroVec(do.call(cbind, vlist), space(bvec), mask=mask) }"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_32-feature-selection","dir":"","previous_headings":"3. Existing Preprocessing Capabilities","what":"3.2 Feature Selection","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"File: /R/feature_selection.R /R/allgeneric.R:36-64 Timing: Called per-fold within internal_crossval fold split. Avoids feature selection bias.","code":"select_features <- function(obj, X, Y, ...) # Generic called within internal_crossval before training # Returns: logical vector of selected features  # Implemented in mvpa_model: select_features.mvpa_model <- function(obj, X, Y,...) {   if (!is.null(obj$feature_selector)) {     select_features(obj$feature_selector, X, Y,...)   } else {     rep(TRUE, length(X))  # No selection   } }"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_33-nuisance-regression--ar-handling","dir":"","previous_headings":"3. Existing Preprocessing Capabilities","what":"3.3 Nuisance Regression & AR Handling","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"Current Status: IMPLEMENTED rMVPA core. built-AR whitening, ARMA models, nuisance regression Normalization (normalize_samples flag config) limited z-scoring key integration point hrfdecoderâ€™s prewhitening","code":""},{"path":[]},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_41-model-specification-flow","dir":"","previous_headings":"4. Configuration and Hyperparameters","what":"4.1 Model Specification Flow","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"","code":"mvpa_model()                          # Specify model, dataset, design     â†“ create_model_spec()                   # Build S3 object     â†“ run_searchlight(model_spec, ...)      # Execute searchlight     â†“ process_roi(model_spec, roi, ...)     # Per-ROI entry point     â†“ internal_crossval(model_spec, ...)    # CV iteration     â†“ train_model(model_spec, ...)          # Delegate to S3 method"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_42-hyperparameter-specification","dir":"","previous_headings":"4. Configuration and Hyperparameters","what":"4.2 Hyperparameter Specification","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"standard models (e.g., sda_notune): - Passed via model object registry (MVPAModels) - fit(x, y, wts, param, lev, last, weights, classProbs, ...) signature hrfdecoder: - Passed explicit function arguments: lambda_W, lambda_HRF, lambda_smooth, etc. - Stored directly model spec - Accessed train_model.hrfdecoder_model() via obj$lambda_W, etc.","code":""},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_43-cross-validation-interaction-with-preprocessing","dir":"","previous_headings":"4. Configuration and Hyperparameters","what":"4.3 Cross-Validation Interaction with Preprocessing","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"Key Issue: CV folds created ROI extraction preprocessing timing matters: AR Prewhitening: 1. Must happen fold creation avoid cross-fold contamination 2. Must use training data per fold (inside train_model) 3. Decoder weights fit prewhitened training data 4. Test predictions made independently prewhitened test data","code":"ROI Extracted â†’ [PREPROCESSING WINDOW] â†’ Folds Created â†’ Train/Test per Fold"},{"path":[]},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_51-where-prewhitening-fits","dir":"","previous_headings":"5. Integration Points for hrfdecoder","what":"5.1 Where Prewhitening Fits","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"Option : Per-ROI, Fold Creation (Preferred AR) Option B: Per-Fold, Inside train_model (Fold-specific AR params)","code":"process_roi_with_prewhitening <- function(mspec, roi, rnum, ...) {   # Extract ROI data   roi_data <- neuroim2::values(roi$train_roi)      # PREWHITEN per ROI (not per fold)   roi_data_pw <- prewhiten_ar(roi_data, mspec$ar_order)      # Update roi with prewhitened data   roi$train_roi <- update_roi_data(roi$train_roi, roi_data_pw)      # Proceed with normal flow   internal_crossval(mspec, roi, rnum, ...) } train_model.hrfdecoder_model <- function(obj, train_dat, y, ...) {   X <- as.matrix(train_dat)      # Estimate AR model and prewhiten TRAINING data only   X_pw <- prewhiten_ar_estimate(X, obj$ar_order)      # Fit decoder on prewhitened data   fit <- hrfdecoder::hrfdecoder_fit(X_pw, event_model=obj$design$event_model, ...)      # Store AR parameters for test data   structure(list(fit=fit, ar_params=attr(X_pw, \"ar_params\")),              class=\"hrfdecoder_fit_wrap\") }  format_result.hrfdecoder_model <- function(obj, result, error_message=NULL, context, ...) {   Xtest <- as.matrix(context$test)      # Apply SAME AR whitening to test data   Xtest_pw <- apply_ar_whitening(Xtest, result$ar_params)      Ptest <- predict(result$fit, Xtest_pw)   # ... rest of aggregation }"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_52-design-extension-hrfdecoder_design","dir":"","previous_headings":"5. Integration Points for hrfdecoder","what":"5.2 Design Extension: hrfdecoder_design","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"File: /R/hrfdecoder_design.R:60-130 Key: Block-level CV ensures entire runs held (respects temporal structure).","code":"hrfdecoder_design <- function(event_model, events, block_var, split_by=NULL) {   # Creates mvpa_design subclass with additional fields      mvdes <- mvpa_design(     train_design = data.frame(y = seq_len(Tlen), block = block_var),     y_train = ~ y,           # Dummy: TR indices     block_var = ~ block,     # Run IDs - DETERMINES folds     split_by = split_by   )      # Attach metadata   mvdes$event_model <- event_model   # fmridesign object   mvdes$events <- events              # event-level labels      class(mvdes) <- c(\"hrfdecoder_design\", class(mvdes))   mvdes }"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_53-model-specific-preprocessing-hook","dir":"","previous_headings":"5. Integration Points for hrfdecoder","what":"5.3 Model-Specific Preprocessing Hook","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"New Generic Function (added): Called process_roi.default:","code":"preprocess_roi <- function(obj, roi, ...) {   UseMethod(\"preprocess_roi\") }  preprocess_roi.default <- function(obj, roi, ...) {   roi  # No preprocessing }  preprocess_roi.hrfdecoder_model <- function(obj, roi, ...) {   # Apply AR prewhitening   roi_data <- neuroim2::values(roi$train_roi)   roi_data_pw <- apply_prewhitening(roi_data, obj)      # Update roi   roi$train_roi <- replace_roi_data(roi$train_roi, roi_data_pw)   roi } process_roi.default <- function(mod_spec, roi, rnum, center_global_id = NA, ...) {   roi <- preprocess_roi(mod_spec, roi)  # NEW HOOK      xtrain <- neuroim2::values(roi$train_roi)   # ... rest of pipeline }"},{"path":[]},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_61-s3-method-resolution-order","dir":"","previous_headings":"6. Key Architectural Patterns","what":"6.1 S3 Method Resolution Order","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"hrfdecoder model operations: method receives full model_spec object, allowing access parameters.","code":"train_model(hrfdecoder_model, ...)   â†’ train_model.hrfdecoder_model()  format_result(hrfdecoder_model, ...)   â†’ format_result.hrfdecoder_model()  merge_results(hrfdecoder_model, ...)   â†’ merge_results.hrfdecoder_model()"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_62-preprocessing-timing-patterns","dir":"","previous_headings":"6. Key Architectural Patterns","what":"6.2 Preprocessing Timing Patterns","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"","code":"PATTERN 1: Dataset-level (immediate, no folds)   mvpa_dataset() â†’ normalize_image_samples() â†’ data ready  PATTERN 2: Per-ROI (before fold iteration)   process_roi() â†’ [preprocess_roi() NEW] â†’ internal_crossval()  PATTERN 3: Per-Fold (inside train_model)   internal_crossval() â†’ train_model() â†’ [local preprocessing] â†’ fit()  PATTERN 4: Test Data (must use training params)   format_result() â†’ [apply training preprocessing] â†’ predict()"},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_63-result-aggregation-hierarchy","dir":"","previous_headings":"6. Key Architectural Patterns","what":"6.3 Result Aggregation Hierarchy","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"","code":"Fold-Level Results (format_result):   class: tibble with columns [class, probs, y_true, test_ind, fit, error, error_message]  ROI-Level Results (merge_results):   class: tibble with columns [result, indices, performance, id, error, error_message]   - Combines all folds into single classification_result   - Computes metrics via compute_performance()  Searchlight Results (wrap_out, run_searchlight):   class: searchlight_result (list of NeuroVol/NeuroSurface maps)"},{"path":[]},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_8-integration-checklist-for-ar-prewhitening","dir":"","previous_headings":"","what":"8. Integration Checklist for AR Prewhitening","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"Implement preprocess_roi() generic function Add preprocess_roi.hrfdecoder_model() method Store AR parameters fit object test data application Ensure format_result.hrfdecoder_model() applies AR transform test data Validate prewhitening happens fold creation (data leakage) Document AR parameter control hrfdecoder_model() signature Test cross-validation ensure train/test independence Add ar_order parameter hrfdecoder_model() spec Consider model-agnostic interface preprocessing (nuisance regression, etc.)","code":""},{"path":"/rMVPA_ARCHITECTURE_REPORT.html","id":"id_9-recommended-preprocessing-architecture-for-hrfdecoder","dir":"","previous_headings":"","what":"9. Recommended Preprocessing Architecture for hrfdecoder","title":"rMVPA Package Architecture Report: Integration Points for hrfdecoder","text":"architecture ensures: 1. Train/test separation: foldâ€™s AR model estimated training data 2. Consistency: Test data transformed using training foldâ€™s AR parameters 3. Model transparency: AR fit stored result object inspection 4. Flexibility: Works without prewhitening 5. Extensibility: pattern works preprocessing (nuisance regression, etc.)","code":"#' Apply AR Prewhitening to ROI Data #' #' @param obj hrfdecoder_model spec #' @param roi ROI data (list with train_roi) #' @param ... Additional args #' @return roi with prewhitened train_roi preprocess_roi.hrfdecoder_model <- function(obj, roi, ...) {   if (is.null(obj$ar_order) || obj$ar_order == 0) {     return(roi)  # No whitening   }      roi_data <- neuroim2::values(roi$train_roi)      # Fit AR model per voxel   ar_fit <- fit_ar_model(roi_data, obj$ar_order)      # Prewhiten data   roi_data_pw <- apply_ar_whitening(roi_data, ar_fit)      # Update roi, preserving indices and metadata   roi$train_roi@data <- roi_data_pw   attr(roi$train_roi, \"ar_fit\") <- ar_fit      roi }  # Inside train_model: train_model.hrfdecoder_model <- function(obj, train_dat, y, sl_info, cv_spec, indices, ...) {   X <- as.matrix(train_dat)      # Check if ar_fit is pre-computed (from preprocess_roi)   ar_fit <- attr(train_dat, \"ar_fit\")   if (!is.null(ar_fit)) {     # Already prewhitened     X_pw <- X   } else if (!is.null(obj$ar_order) && obj$ar_order > 0) {     # Estimate per-fold     ar_fit <- fit_ar_model(X, obj$ar_order)     X_pw <- apply_ar_whitening(X, ar_fit)   } else {     X_pw <- X     ar_fit <- NULL   }      fit <- hrfdecoder::hrfdecoder_fit(X_pw, event_model=obj$design$event_model, ...)      structure(     list(fit=fit, ar_fit=ar_fit, sl_info=sl_info, indices=indices),     class=\"hrfdecoder_fit_wrap\"   ) }  # Inside format_result: format_result.hrfdecoder_model <- function(obj, result, error_message=NULL, context, ...) {   Xtest <- as.matrix(context$test)      # Apply same AR whitening to test data   if (!is.null(result$ar_fit)) {     Xtest_pw <- apply_ar_whitening(Xtest, result$ar_fit)   } else {     Xtest_pw <- Xtest   }      Ptest <- predict(result$fit, Xtest_pw)   # ... aggregation and result formatting }"},{"path":"/reference/aggregate_events.html","id":null,"dir":"Reference","previous_headings":"","what":"Aggregate TR-level soft labels to events â€” aggregate_events","title":"Aggregate TR-level soft labels to events â€” aggregate_events","text":"Aggregate TR-level soft labels events","code":""},{"path":"/reference/aggregate_events.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregate TR-level soft labels to events â€” aggregate_events","text":"","code":"aggregate_events(   P,   events,   TR,   conditions,   window = c(4, 8),   hrf = NULL,   normalize = FALSE )"},{"path":"/reference/aggregate_events.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregate TR-level soft labels to events â€” aggregate_events","text":"P matrix (T x K_event) events event data.frame (needs columns onset, condition) TR TR duration (seconds) conditions ordered condition labels window time window (s) onset hrf optional fmrihrf HRF object weighting","code":""},{"path":"/reference/aggregate_events.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Aggregate TR-level soft labels to events â€” aggregate_events","text":"list probs matrix y_true factor","code":""},{"path":"/reference/albers_okabe_ito.html","id":null,"dir":"Reference","previous_headings":"","what":"Okabe-Ito Color Palette for Albers Theme â€” albers_okabe_ito","title":"Okabe-Ito Color Palette for Albers Theme â€” albers_okabe_ito","text":"Returns colorblind-safe Okabe-Ito palette use plots.","code":""},{"path":"/reference/albers_okabe_ito.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Okabe-Ito Color Palette for Albers Theme â€” albers_okabe_ito","text":"","code":"albers_okabe_ito()"},{"path":"/reference/albers_okabe_ito.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Okabe-Ito Color Palette for Albers Theme â€” albers_okabe_ito","text":"character vector 8 hex colors","code":""},{"path":"/reference/build_condition_basis.html","id":null,"dir":"Reference","previous_headings":"","what":"Build per-condition basis matrices for decoding â€” build_condition_basis","title":"Build per-condition basis matrices for decoding â€” build_condition_basis","text":"Build per-condition basis matrices decoding","code":""},{"path":"/reference/build_condition_basis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build per-condition basis matrices for decoding â€” build_condition_basis","text":"","code":"build_condition_basis(ev_model, hrf = NULL)"},{"path":"/reference/build_condition_basis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build per-condition basis matrices for decoding â€” build_condition_basis","text":"ev_model fmridesign::event_model result hrf optional fmrihrf HRF object","code":""},{"path":"/reference/continuous_mvpa_design.html","id":null,"dir":"Reference","previous_headings":"","what":"Continuous-time MVPA design wrapper â€” continuous_mvpa_design","title":"Continuous-time MVPA design wrapper â€” continuous_mvpa_design","text":"Continuous-time MVPA design wrapper","code":""},{"path":"/reference/continuous_mvpa_design.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Continuous-time MVPA design wrapper â€” continuous_mvpa_design","text":"","code":"continuous_mvpa_design(   event_model,   block_var,   design_df_events = NULL,   split_by = NULL )"},{"path":"/reference/continuous_mvpa_design.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Continuous-time MVPA design wrapper â€” continuous_mvpa_design","text":"event_model fmridesign::event_model block_var run/block ids per TR design_df_events optional trial table (defaults events(event_model)) split_by optional formula passed mvpa_design","code":""},{"path":"/reference/dot-get_run_ids_from_test_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Get run IDs for test data (internal helper) â€” .get_run_ids_from_test_data","title":"Get run IDs for test data (internal helper) â€” .get_run_ids_from_test_data","text":"Get run IDs test data (internal helper)","code":""},{"path":"/reference/dot-get_run_ids_from_test_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get run IDs for test data (internal helper) â€” .get_run_ids_from_test_data","text":"","code":".get_run_ids_from_test_data(fit_obj, Y_test, ev_model_test)"},{"path":"/reference/estimate_hrf_theta.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate HRF basis coefficients from soft labels â€” estimate_hrf_theta","title":"Estimate HRF basis coefficients from soft labels â€” estimate_hrf_theta","text":"Estimate HRF basis coefficients soft labels","code":""},{"path":"/reference/estimate_hrf_theta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate HRF basis coefficients from soft labels â€” estimate_hrf_theta","text":"","code":"estimate_hrf_theta(X_list, Z_event, hrf_obj, penalty = 0.01)"},{"path":"/reference/estimate_rank_auto.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate nuisance rank from whitened residuals â€” estimate_rank_auto","title":"Estimate nuisance rank from whitened residuals â€” estimate_rank_auto","text":"Uses MarÄenkoâ€“Pastur bulk-edge threshold together parallel-analysis null (voxel-wise circular shifts) select low-rank nuisance dimension whitening optional baseline removal.","code":""},{"path":"/reference/estimate_rank_auto.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate nuisance rank from whitened residuals â€” estimate_rank_auto","text":"","code":"estimate_rank_auto(   Y,   X_list,   X_base = NULL,   runs = NULL,   control = list(),   alpha = 0.05,   B = 50L,   r_max = 50L,   block_shifts = TRUE )"},{"path":"/reference/estimate_rank_auto.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate nuisance rank from whitened residuals â€” estimate_rank_auto","text":"Y Numeric matrix (T x V) voxel time series. X_list List design blocks (T x K). design exists, pass single -zero column keep code path identical fitter. X_base Optional baseline design (T x B) subtracted whitening. runs Optional integer vector length T run identifiers. control Optional list entries `lambda_A_init_ridge` `lambda_base` initial ridge fits. alpha Family-wise level parallel-analysis quantile. B Number circular-shift null draws (set 0 disable). r_max Maximum number singular values probe. block_shifts Logical; TRUE, circular shifts done voxel-wise.","code":""},{"path":"/reference/estimate_rank_auto.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate nuisance rank from whitened residuals â€” estimate_rank_auto","text":"List fields `r`, `r_mp`, `r_pa`, `sigma2`, `lambda_plus`, `ev`.","code":""},{"path":"/reference/fit_hrfdecoder.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit HRF-aware weakly supervised decoder â€” fit_hrfdecoder","title":"Fit HRF-aware weakly supervised decoder â€” fit_hrfdecoder","text":"Fit HRF-aware weakly supervised decoder","code":""},{"path":"/reference/fit_hrfdecoder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit HRF-aware weakly supervised decoder â€” fit_hrfdecoder","text":"","code":"fit_hrfdecoder(   Y,   ev_model,   base_model = NULL,   hrf = NULL,   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5,   theta_penalty = 0.01,   max_iter = 20,   tol = 1e-04,   nonneg = TRUE,   background = TRUE,   standardize = TRUE,   ar_order = NULL,   ar_method = c(\"ar\", \"arma\"),   ar_pooling = c(\"run\", \"global\"),   verbose = 1 )"},{"path":"/reference/fit_hrfdecoder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit HRF-aware weakly supervised decoder â€” fit_hrfdecoder","text":"Y numeric matrix (T x V) fMRI data (time voxel) ev_model fmridesign::event_model describing events base_model optional fmridesign::baseline_model nuisance removal hrf optional fmrihrf basis object lambda_W ridge penalty decoder weights lambda_HRF adherence weight HRF prior lambda_smooth temporal smoothness weight theta_penalty ridge penalty HRF basis coefficients max_iter maximum ALS iterations tol convergence tolerance P updates nonneg enforce non-negative soft labels background include background column soft labels standardize z-score Y columns fitting ar_order AR order prewhitening (default: NULL AR prewhitening). Set 1 AR(1), 2 AR(2), etc. Use \"auto\" automatic BIC-based selection. ar_method AR estimation method: \"ar\" (Yule-Walker) \"arma\" (Hannan-Rissanen). Default: \"ar\". ar_pooling Spatial pooling AR parameters: \"global\" (one AR model voxels) \"run\" (separate AR model per run). Default: \"run\". verbose integer verbosity level","code":""},{"path":"/reference/fit_hrfdecoder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit HRF-aware weakly supervised decoder â€” fit_hrfdecoder","text":"object class `hrfdecoder_fit`","code":""},{"path":"/reference/hrfdecoder_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an hrfdecoder model spec for rMVPA â€” hrfdecoder_model","title":"Create an hrfdecoder model spec for rMVPA â€” hrfdecoder_model","text":"Create hrfdecoder model spec rMVPA","code":""},{"path":"/reference/hrfdecoder_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an hrfdecoder model spec for rMVPA â€” hrfdecoder_model","text":"","code":"hrfdecoder_model(   dataset,   design,   lambda_W = 10,   lambda_HRF = 1,   lambda_smooth = 5,   theta_penalty = 0.01,   basis = NULL,   window = c(4, 8),   nonneg = TRUE,   max_iter = 10,   tol = 1e-04,   ar_order = 1,   ar_method = c(\"ar\", \"arma\"),   ar_pooling = c(\"run\", \"global\"),   performance = NULL,   crossval = NULL,   return_predictions = TRUE,   return_fits = FALSE )"},{"path":"/reference/hrfdecoder_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an hrfdecoder model spec for rMVPA â€” hrfdecoder_model","text":"ar_order AR order prewhitening (default: 1 AR(1)). Set NULL 0 disable. ar_method AR estimation method: \"ar\" \"arma\". Default: \"ar\". ar_pooling Spatial pooling AR: \"global\" \"run\". Default: \"run\".","code":""},{"path":"/reference/predict_hrfdecoder.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with an hrfdecoder fit â€” predict_hrfdecoder","title":"Predict with an hrfdecoder fit â€” predict_hrfdecoder","text":"Predict hrfdecoder fit","code":""},{"path":"/reference/predict_hrfdecoder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with an hrfdecoder fit â€” predict_hrfdecoder","text":"","code":"predict_hrfdecoder(   object,   Y_test,   ev_model_test = NULL,   mode = c(\"tr\", \"trial\"),   window = c(4, 8),   weights = c(\"hrf\", \"flat\") )"},{"path":"/reference/predict_hrfdecoder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with an hrfdecoder fit â€” predict_hrfdecoder","text":"object hrfdecoder_fit Y_test numeric matrix (T x V) ev_model_test optional fmridesign::event_model trial-level outputs mode \"tr\" \"trial\" window time window (seconds) relative onset aggregation weights weighting scheme (\"hrf\" uses fitted HRF; \"flat\" uniform)","code":""},{"path":"/reference/prepare_decoder_inputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare decoder design inputs â€” prepare_decoder_inputs","title":"Prepare decoder design inputs â€” prepare_decoder_inputs","text":"Prepare decoder design inputs","code":""},{"path":"/reference/prepare_decoder_inputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare decoder design inputs â€” prepare_decoder_inputs","text":"","code":"prepare_decoder_inputs(ev_model, hrf = NULL, background = TRUE)"},{"path":"/reference/residualize_baseline.html","id":null,"dir":"Reference","previous_headings":"","what":"Residualize Y against a baseline model â€” residualize_baseline","title":"Residualize Y against a baseline model â€” residualize_baseline","text":"Residualize Y baseline model","code":""},{"path":"/reference/residualize_baseline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Residualize Y against a baseline model â€” residualize_baseline","text":"","code":"residualize_baseline(Y, base_model = NULL)"},{"path":"/reference/residualize_baseline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Residualize Y against a baseline model â€” residualize_baseline","text":"Y numeric matrix (T x V) base_model optional fmridesign::baseline_model","code":""},{"path":"/reference/scale_color_albers.html","id":null,"dir":"Reference","previous_headings":"","what":"Albers Color Scale for ggplot2 â€” scale_color_albers","title":"Albers Color Scale for ggplot2 â€” scale_color_albers","text":"Discrete continuous color scale matching Albers minimalist theme.","code":""},{"path":"/reference/scale_color_albers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Albers Color Scale for ggplot2 â€” scale_color_albers","text":"","code":"scale_color_albers(..., discrete = TRUE)"},{"path":"/reference/scale_color_albers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Albers Color Scale for ggplot2 â€” scale_color_albers","text":"... Additional arguments passed scale functions discrete Logical; TRUE uses Okabe-Ito palette, FALSE uses viridis","code":""},{"path":"/reference/scale_color_albers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Albers Color Scale for ggplot2 â€” scale_color_albers","text":"ggplot2 scale object","code":""},{"path":"/reference/scale_color_albers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Albers Color Scale for ggplot2 â€” scale_color_albers","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2) ggplot(mtcars, aes(wt, mpg, color = factor(cyl))) +   geom_point() +   scale_color_albers() } # }"},{"path":"/reference/scale_fill_albers.html","id":null,"dir":"Reference","previous_headings":"","what":"Albers Fill Scale for ggplot2 â€” scale_fill_albers","title":"Albers Fill Scale for ggplot2 â€” scale_fill_albers","text":"Discrete continuous fill scale matching Albers minimalist theme.","code":""},{"path":"/reference/scale_fill_albers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Albers Fill Scale for ggplot2 â€” scale_fill_albers","text":"","code":"scale_fill_albers(..., discrete = TRUE)"},{"path":"/reference/scale_fill_albers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Albers Fill Scale for ggplot2 â€” scale_fill_albers","text":"... Additional arguments passed scale functions discrete Logical; TRUE uses Okabe-Ito palette, FALSE uses viridis","code":""},{"path":"/reference/scale_fill_albers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Albers Fill Scale for ggplot2 â€” scale_fill_albers","text":"ggplot2 scale object","code":""},{"path":"/reference/scale_fill_albers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Albers Fill Scale for ggplot2 â€” scale_fill_albers","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2) ggplot(mtcars, aes(x = factor(cyl), fill = factor(cyl))) +   geom_bar() +   scale_fill_albers() } # }"},{"path":"/reference/theme_albers.html","id":null,"dir":"Reference","previous_headings":"","what":"Albers Minimalist ggplot2 Theme â€” theme_albers","title":"Albers Minimalist ggplot2 Theme â€” theme_albers","text":"clean, minimal ggplot2 theme subtle grid lines, top legend placement, accessible typography. Designed match Albers visual system used package vignettes.","code":""},{"path":"/reference/theme_albers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Albers Minimalist ggplot2 Theme â€” theme_albers","text":"","code":"theme_albers(base_size = 11, base_family = \"system-ui\")"},{"path":"/reference/theme_albers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Albers Minimalist ggplot2 Theme â€” theme_albers","text":"base_size Base font size points (default: 11) base_family Base font family (default: \"system-ui\")","code":""},{"path":"/reference/theme_albers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Albers Minimalist ggplot2 Theme â€” theme_albers","text":"ggplot2 theme object","code":""},{"path":"/reference/theme_albers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Albers Minimalist ggplot2 Theme â€” theme_albers","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2) theme_set(theme_albers())  ggplot(mtcars, aes(wt, mpg, color = factor(cyl))) +   geom_point(size = 2.2) +   scale_color_albers() +   labs(     title = \"Fuel efficiency vs. weight\",     subtitle = \"Example with Albers theme\",     x = \"Weight (1000 lbs)\",     y = \"MPG\"   ) } # }"}]
